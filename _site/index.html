<!DOCTYPE html>
<html lang="en-us">

<head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <link rel="stylesheet" href="/assets/css/normalize.css"/>
  <link rel="stylesheet" href="/assets/css/bulma.css"/>
  <link rel="stylesheet" href="/assets/css/custom.css"/>
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">
  <!-- <link href="https://fonts.googleapis.com/css?family=Open+Sans|Roboto" rel="stylesheet"> -->

  <!-- Icons -->
  <link rel="shortcut icon" href="/assets/images/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">

  <title>
    
      Well, Hello There! &middot; Columba M71's Blog
    
  </title>

</head>


<body>

<section class="hero is-dark">
  <div class="hero-body">
    <div class="container">
        <h1 class="title">
          <span class="typewrite" data-period="2000" data-type='[ "hello", "namaste", "assalam o alaikum",
          "sat srī akāl", "khammaghani", "hola", "bonjour", "nomoskaar", "guten tag", "kumusta", "salut", "nômoshkar"]'>
            <span class="wrap"></span>
          </span>
        </h1>
      <h4 class="subtitle" id="quote">
      </h4>
    </div>
  </div>
</section>

<div class="main-container">
  <div class="tile is-ancestor is-vertical">


    <nav class="nav has-shadow">

      <div class="nav-left">

        <a href="/" class="nav-item">
          <span class="icon">
            <i class="fa fa-home" aria-hidden="true" title="Homepage"></i>
          </span>
        </a>

        <a href="https://github.com/hnu2013wwj" class="nav-item">
          <span class="icon">
            <i class="fa fa-github" aria-hidden="true" title="Github"></i>
          </span>
        </a>

        <div class="nav-item" id="searchFieldNav">
          <div class="field has-addons">
            <p class="control">
              <input class="input is-small" type="text" placeholder="Find an article" id="search-text">
            </p>
            <p class="control">
              <a class="button is-dark is-small" onclick="searchHandler();">
                Search
              </a>
            </p>
          </div>
        </div>

      </div>

      <div class="nav-right nav-menu" id='nav-menu'>
        <a href="/archive" class="nav-item">Archive</a>
        <a href="/tags" class="nav-item">Tags</a>
      </div>

      <span class="nav-toggle" id="nav-toggle">
          <span></span>
          <span></span>
          <span></span>
    </nav>

    <div class="tile is-parent">
      <div class="tile is-8 is-child main">

        <div class="box">
    <h1 class="post-title">Well, Hello There!</h1>
    <hr/>
    <div class="content"><span class="post-text"><ul class="posts">
    
    <li class="post">
        <small>20 Sep 2018</small>
        <small><a style="color:grey" href="/2018/09/Homa-A-Receiver-Driven-Low-Latency-Transport-Protocol-Using-Network-Priorities.html#disqus_thread"></a></small>
        <br>
        <a class="nice-title" href="/2018/09/Homa-A-Receiver-Driven-Low-Latency-Transport-Protocol-Using-Network-Priorities.html">A Receiver-Driven Low-Latency Transport Protocol</a>
        <p class="nice-text">Homa – A Receiver-Driven Low-Latency Transport Protocol Using Network Priorities 0x00 引言 最近几年为数据中心设计的新的传输协议不少，这篇时SIGCOMM上最新的一篇(截止写这篇论文时)，总而言之，这篇论文做到了: In simulations, Homa’s latency is roughly equal to pFabric and significantly better than pHost, PIAS, and NDP for almost all message sizes and workloads. Homa can also sustain higher network loads than pFabric, pHost, or PIAS. -----... <a href="/2018/09/Homa-A-Receiver-Driven-Low-Latency-Transport-Protocol-Using-Network-Priorities.html">Read more!</a></p>
    </li>
    
    <li class="post">
        <small>18 Sep 2018</small>
        <small><a style="color:grey" href="/2018/09/Re-architecting-datacenter-networks-and-stacks-for-low-latency-and-high-performance.html#disqus_thread"></a></small>
        <br>
        <a class="nice-title" href="/2018/09/Re-architecting-datacenter-networks-and-stacks-for-low-latency-and-high-performance.html">NDP transport protocol</a>
        <p class="nice-text">Re-architecting datacenter networks and stacks for low latency and high performance

参考


  Re-architecting datacenter networks and stacks for low latency and high performance，SIGCOMM 2017.


 <a href="/2018/09/Re-architecting-datacenter-networks-and-stacks-for-low-latency-and-high-performance.html">Read more!</a></p>
    </li>
    
    <li class="post">
        <small>01 Sep 2018</small>
        <small><a style="color:grey" href="/2018/09/ISA-Wars-Understanding-the-Relevance-of-ISA-being-RISC-or-CISC-to-Performance,-Power,-and-Energy-on-Modern-Architecture.html#disqus_thread"></a></small>
        <br>
        <a class="nice-title" href="/2018/09/ISA-Wars-Understanding-the-Relevance-of-ISA-being-RISC-or-CISC-to-Performance,-Power,-and-Energy-on-Modern-Architecture.html">ISA Wars</a>
        <p class="nice-text">ISA Wars: Understanding the Relevance of ISA being RISC or CISC to Performance, Power, and Energy on Modern Architectures(指令集战争：理解现代RISC或CISC处理器架构与性能、能耗和能效的相关性 0x00 引言 RISC和CISC的比较来源已久，早期的比较大多都集中在性能方面。一般情况下，会认为CISC处理器的性能更加好，而能耗和能效更加低，RISC反之，当然这种说法不一定准确。而现在，CPU架构发展已经相对成熟，单核性能每年增长的幅度很小，CPU多核化，智能手机等移动设备的CPU越来越受关注。那么在CPU发展变化之后，RISC和CISC的指令集类型对处理器的各个关键指标又有什么样的影响呢？这里我们来看一看TPCS 2015上的一篇文章[1]。这篇paper主要做的就是: We present an exhaustive and rigorous analysis using workloads that span smart-phone, desktop, and server applications. In our study, we are primarily interested in whether and, if... <a href="/2018/09/ISA-Wars-Understanding-the-Relevance-of-ISA-being-RISC-or-CISC-to-Performance,-Power,-and-Energy-on-Modern-Architecture.html">Read more!</a></p>
    </li>
    
    <li class="post">
        <small>01 Aug 2018</small>
        <small><a style="color:grey" href="/2018/08/Read-Log-Update.html#disqus_thread"></a></small>
        <br>
        <a class="nice-title" href="/2018/08/Read-Log-Update.html">Read-Log-Update</a>
        <p class="nice-text">Read-Log-Update – A Lightweight Synchronization Mechanism for Concurrent Programming

0x00 引言

参考


  Read-Log-Update: A Lightweight Synchronization Mechanism for Concurrent Programming, SOSP 2015.

 <a href="/2018/08/Read-Log-Update.html">Read more!</a></p>
    </li>
    
    <li class="post">
        <small>20 Jul 2018</small>
        <small><a style="color:grey" href="/2018/07/HOT-Height-Optimized-Trie-Index.html#disqus_thread"></a></small>
        <br>
        <a class="nice-title" href="/2018/07/HOT-Height-Optimized-Trie-Index.html">A Height Optimized Trie Index for Main-Memory Database Systems</a>
        <p class="nice-text">HOT: A Height Optimized Trie Index for Main-Memory Database Systems

0x00 引言

Height Optimized Trie (HOT) 是一种新的为Main-Memory Database设计的新的数据结构。

参考


  HOT: A Height Optimized Trie Index for Main-Memory Database Systems，SIGMOD 2018；

 <a href="/2018/07/HOT-Height-Optimized-Trie-Index.html">Read more!</a></p>
    </li>
    
    <li class="post">
        <small>15 Jul 2018</small>
        <small><a style="color:grey" href="/2018/07/Contention-Aware-Lock-Scheduling-for-Transactional-Databases.html#disqus_thread"></a></small>
        <br>
        <a class="nice-title" href="/2018/07/Contention-Aware-Lock-Scheduling-for-Transactional-Databases.html">Contention-Aware Lock Scheduling</a>
        <p class="nice-text">Contention-Aware Lock Scheduling for Transactional Databases

0x00 引言

这是我目前读过的最喜欢的一篇论文之一。首先，论文写的非常通俗易懂，即使你之前对Lock Schedule没有什么了解，通过这篇论文也能很清楚的知道Contention-Aware Lock Scheduling的原理。另外，这个算法不仅仅是一个理论上的，只存在于实验室中，这个算法已经被MySQL 8.0采用了。简直赞的不行，如此快速的在真正的实际的被广泛使用软件中得到应用，也侧面印证了这个研究very excellent。

0x01 背景与动机

参考


  Contention-Aware Lock Scheduling for Transactional Databases ，VLDB 2018.

 <a href="/2018/07/Contention-Aware-Lock-Scheduling-for-Transactional-Databases.html">Read more!</a></p>
    </li>
    
    <li class="post">
        <small>05 Jun 2018</small>
        <small><a style="color:grey" href="/2018/06/TicToc-Time-Traveling-Optimistic-Concurrency-Control.html#disqus_thread"></a></small>
        <br>
        <a class="nice-title" href="/2018/06/TicToc-Time-Traveling-Optimistic-Concurrency-Control.html">Time Traveling Optimistic Concurrency Control</a>
        <p class="nice-text">TicToc: Time Traveling Optimistic Concurrency Control 0x00 引言 这篇文章是关于内存数据库的Optimistic Concurrency Control优化的一篇paper(对OCC没有了解的可以先看看论文[2])。Timestamp ordering (T/O) concurrency control 是数据库并发控制一种非常重要的方法，一般而言，系统会以事务为粒度分配timestamp，而这种方式存在一个缺点： A common way to implement the allocator is through an atomic add instruction that increments a global counter for each new transaction. This approach, however, is only able to generate less than 5 million... <a href="/2018/06/TicToc-Time-Traveling-Optimistic-Concurrency-Control.html">Read more!</a></p>
    </li>
    
    <li class="post">
        <small>12 Oct 2017</small>
        <small><a style="color:grey" href="/2017/10/Consistency-Tradeoffs-in-Modern-Distributed-Database-System-Design.html#disqus_thread"></a></small>
        <br>
        <a class="nice-title" href="/2017/10/Consistency-Tradeoffs-in-Modern-Distributed-Database-System-Design.html">Consistency Tradeoffs in Modern Distributed Database System Design</a>
        <p class="nice-text">Consistency Tradeoffs in Modern Distributed Database System Design CAP理论深刻地影响了分布式数据库的设计，另外一个方面，一致性和延迟之间的权衡也对现在的数据库产生了直接的影响。未来融合这个两者，一个新的理论PACELC[2]被提出。 DDBSs正在正在变得越来越”大”, 2个主要的原因促使了这种变化，第1个是现在的应用需要越来越多的数据和越来越高的事务处理能力，第2个是因为增长的跨国企业的需要在全球范围内开展业务，需要将数据中心建立在靠近客户的地方。在过去10年开发的DDBSs都具有高可用性或(和)全球范围内范围的能力，比如SimpleDB/DynamoDB，Cassandra，Voldmort，HBase/BigTable,MongoDB，VoltDB/H-Store以及Megastore等。 DDBSs是一个复杂的系统，开发这样一个系统是非常困难的。DDBS的设计是一门权衡的艺术，其中CAP理论是很重要的一个部分。虽然CAP很重要，但是论文[1]认为CAP被滥用了，仅仅只考虑C A P之间的权衡是不够的。实际上，CAP讨论的是故障复发生之后的一些限制。正常情况下，更为常见的权衡是一致性和延时之间的权衡，它也深刻地影响了DDBS的设计。PACELC被提出统一这些权衡，PACELC的含义为: P then AC else LC，意为在网络分区发生的时候系统需要在可用性和一致性之间做出权衡，而没有网络分区的情况下，系统需要在一致性和延时之间做出权衡。 CAP是关于故障的 由于CAP理论中三个最多只能取其二，只有以下几种类型的系统是可能的: CA，没有网络分区；CP，不能保证高可用；AP，不能保证一致性。现在很多的DDBS(默认情况下)都不保证C，比如SimpleDB/Dynamo, Cassandra等。 早起的DDBS的研究的关注点在一致性，很自然的认为CAP是影响数据库架构的主要因素。DDBS是必须要能容忍出现网络分区的，这样的话，DDBS这能在A和C之间做出选择。对于可靠性有高要求的应用就只能舍弃C。 看上去这里是没有问题的。但是实际上还是有一些瑕疵，这个观点不仅仅是一致性和可用性之间的权衡，而且还包括了网络分区和网络分区的存在这个件事情。也就是，这里只是简单地认为网络分区的存在得让我们在一致性和可用性之间做出选择。但是网络分区可能性是多种原因决定的: 系统在WAN上运行？或者知识一个局部的集群？使用了什么样的硬件？等等。通常网络分区是很少见的，发生的频率低于系统中的其它类型的故障。在没有网络分区的情况下，CAP允许系统完整的实现ACID的前提下也实现高可用性。CAP理论不是这些在正常的情况下减弱一致性的理由(比如SimpleDB/Dynamo, Cassandra等)。 一致性和延时之间的权衡 理解现代一些DDBS的设计要先了解它们面向的使用场景，现在的很多DDBS(比如SimpleDB/Dynamo, Cassandra等)面向的使用场景是在线活动，对交互的延时比较敏感，延时多上100ms可能就导致用户的流失。 不幸的事，在一致性、可用性和延时之间存在基本的权衡，即使没有网络分区，这种权衡也会存在，这种权衡与CAP无关。及时如此，这张权衡也是这些系统设计的关键因素。 高可用性机遇就意味着复制数据。为了实现尽可能高的可用性，DDBS就得在WLN复制数据，防治一个数据中心因为某些原因被整个损坏(比如地震、飓风)等。 当出现数据复制时，就会有一致性和延时之间的权衡。只有3种方法来实现数据复制: 1. 系统同时发送更新给所有副本；2. 发送给特殊的master节点；3. 随机发送给一个节点。无论哪一种方法，都存在一致性和延时之间的权衡。 同时发送给所有副本 如果更新不经过预处理合作其它的协议，因为每个副本得到的更新的顺序可能是不同的，这样就会导致明显的缺失一致性。而如果经过预处理合作通过其它协议来保证副本应用更新的顺序，这样的话，这些操作就是延时的来源。如果使用的是使用某种协议的方法，那么协议本身就是一种延时的来源。 在使用预处理方法的情况下，主要有两个延时的来源，第一个是附加的预处理会增加延时，第二，预处理器由几台机器或者一台机器组成，多台机器的情况下又需要某种协议来保证更新的顺序，一台机器的情况下任何位置的更新都得像这个单一的节点请求更新。 数据更新发送到一个商定的位置 这个商定的位置可以称为master。Master节点所有的请求然后更新数据。由于只由master来处理请求，所以更新执行的顺序是可以保证的。Master执行玩操作后，会复制到其它的节点。 这个复制有3种不同的选项: 1. 同步复制: Master节点会一直等待知道所有的副本都已经复制完成。这样一致性是有保障的，但是延时不可避免地会增大。 2. 异步复制: 系统会认为在复制完成之前就认为更新已经完成。通常会保证更新的数据以及被持久化保存，但不保证更新已经同步到所有的节点，这种情况下，一致性和延时的权衡取决于系统如何读取: i.... <a href="/2017/10/Consistency-Tradeoffs-in-Modern-Distributed-Database-System-Design.html">Read more!</a></p>
    </li>
    
    <li class="post">
        <small>06 Oct 2017</small>
        <small><a style="color:grey" href="/2017/10/Adaptive-Radix-Tree.html#disqus_thread"></a></small>
        <br>
        <a class="nice-title" href="/2017/10/Adaptive-Radix-Tree.html">Adaptive Radix Tree</a>
        <p class="nice-text">The Adaptive Radix Tree: ARTful Indexing for Main-Memory Databases 0x00 引言 ​ Adaptive Radix Tree是最近出现的为Main-Memory Database设计的的支持范围数据结构里面个人认为最优美的一种了，不如Masstree，Bwtree那么复杂，另外，相比于传统的一些结构如T-tree，也更好的适应了现代的多核处理器。这篇时关于Adaptive Radix Tree(ART)的基本结构的，另外有一篇时关于ART的Concurrency Control的，之后会加上。 ​ ART的主要思路时使用不同大小的Node，来减少内存使用。同时加上一些额外的如高度上的优化。 0x01 基本结构 前面提到，ART的内部Node有不同的大小。一般而言，离root比较远的Node里面保护的数据项时比较小的。一般的Radix Tree使用完整的Node的话，会浪费很多的内存，而ART就解决了这个问题： ART可以做到 With 32 bit keys, for example, a radix tree using s = 1 has 32 levels, while a span of 8 results in only... <a href="/2017/10/Adaptive-Radix-Tree.html">Read more!</a></p>
    </li>
    
    <li class="post">
        <small>23 Sep 2017</small>
        <small><a style="color:grey" href="/2017/09/Reducing-the-Storage-Overhead-of-Main-Memory-OLTP-Databases.html#disqus_thread"></a></small>
        <br>
        <a class="nice-title" href="/2017/09/Reducing-the-Storage-Overhead-of-Main-Memory-OLTP-Databases.html">Reducing the Storage Overhead of Main-Memory OLTP Databases</a>
        <p class="nice-text">Reducing the Storage Overhead of Main-Memory OLTP Databases with Hybrid Indexes

参考


  Reducing the Storage Overhead of Main-Memory OLTP Databases with Hybrid Indexes , SIGMOD2016.

 <a href="/2017/09/Reducing-the-Storage-Overhead-of-Main-Memory-OLTP-Databases.html">Read more!</a></p>
    </li>
    
    <li class="post">
        <small>14 Sep 2017</small>
        <small><a style="color:grey" href="/2017/09/PacificA-Replication-in-Log-Based-Distributed-Storage-Systems.html#disqus_thread"></a></small>
        <br>
        <a class="nice-title" href="/2017/09/PacificA-Replication-in-Log-Based-Distributed-Storage-Systems.html">PacificA -- Replication in Log-Based Distributed Storage Systems</a>
        <p class="nice-text">PacificA: Replication in Log-Based Distributed Storage Systems PacificA是微软推出的一个通用的复制框架。论文[1]中强调PacificA是一个简单、实用和强一致性的。也许是基于Paxos的方法被太多的吐槽了关于复杂，难以理解难以实现，Raft、PacificA等算法都特别强调自己简单易于理解也相对容易实现。 选择一个合适的复制算法就算选择一个正确的架构设计，简单和模块化是设计的几个要点。PacificA的设计有以下的特点: 1. PacificA将配置管理和数据复制分离，由基于Paxos的模块来负责管理配置，主副本(primary/backup)策略来复制数据； 2. 去中心化的错误检测和触发配置，监控流量遵循数据复制的策略； 3. PacificA是一个通用的抽象的模型，易于验证，可以有不同的策略实现。 系统有以下的假设: 系统运行在一个分布式的环境之中，一些server组成集群，这些server随时可能失败，这里假设的情况是失败之后就停止运行(fail-stop failures)，信息在传输的过程中可能被延时任意长的时间达到，也可能丢失，可能会发生网络分区。时钟也是不可靠的。PacificA可以在一个n+1个副本的系统中最多容忍n个副本故障。 主副本数据复制 系统将客户端的请求分为两种: queries请求只请求数据而不会更新数据，而updates请求会更新数据。所有的请求都会发送个primary，primary会处理全部的请求，而只会在updates请求的时候才会让副本参与进来。这样的主副本的策略好处就是简单易于实现。 如果更新操作的结果是确定的，且所有的服务器都以系统的顺序处理了相同的请求集合，那么就可以实现强一致性。为了实现这个目标，primary(主副本)会赋予每一个请求一个唯一的单调递增的序号给所有的updates请求，每个次副本都的安装这个序号表示的顺序进行处理。这里可以表示为每个副本都维持了一个包含了它收到的所有请求的prepared list和一个对于这个list的一个commited point。这个list会根据序号排序，commited point之前的prepred list部分就是commited list。 正常情况情况下的查询和更新协议 正常情况下，primary对于接受到的queries的请求，直接就查询commited list之中的结果如何回复客户端即可。 对于updates请求，primary会先给这个请求赋予下一个的编号(编号可以初始化为0)，然后将这个请求带上编号和当前配置版本号(prepare message)发送给所有的次副本。 在收到prepare message后，次副本会将这个请求插入prepared list(注意要按照编号的顺序)，在次副本确认这个请求被安置妥当之后，就发送给primary一个ack以告知自己已经将这个请求安置妥当。当primary在收到了所有的次副本的ack之后，primary就会提交这个请求(The request is committed when the primary receives acknowledgments from all replicas)。然后，primary会移动它的committed point，然后向所有的次副本发送消息通知它们已经处理完毕，在收到了primary确认成功的消息之后，次副本也就可以移动它的committed point的了。 在这种处理方式之中，会有以下的结论： Commit... <a href="/2017/09/PacificA-Replication-in-Log-Based-Distributed-Storage-Systems.html">Read more!</a></p>
    </li>
    
    <li class="post">
        <small>12 Sep 2017</small>
        <small><a style="color:grey" href="/2017/09/A-Quorum-Based-Commit-Protocol.html#disqus_thread"></a></small>
        <br>
        <a class="nice-title" href="/2017/09/A-Quorum-Based-Commit-Protocol.html">A Quorum-Based Commit Protocol</a>
        <p class="nice-text">A Quorum-Based Commit Protocol 在distributed computing中，quorum是指分布式执行时必须获得的最小的票数[2]，常被用于分布式的commit protocol和replica control。这里我们来看看论文[1]描述的一种quorum-based commit protocol。 介绍 这个Quorum-Based Commit Protocol(下文简称protocol or 协议)使用quorum的方式来解决错误发生之后的冲突。当有一个错误发生之后，一个事务只能在被一个quorum commit的情况下commit，这个quorum称为commit quorum，表示为V(C)，同理，一个事务abort只有在一个quorum abort的情况下abort，这个quorum称为abort quorum，表示为V(A)。 这个protocol有以下的一些的特点： 1. 这是一个集中式(centralized)的协议(也就是说存在中心节点)； 2. 当所有的错误(failure)被解决之后，协议最终会终止； 3. 这是一个阻塞的协议，只有当错误被修复之后才能继续运行。 此外，协议可以从多种的失败中快速恢复，但是主要关注的错误的网络分区，主要是以下两种类型: 节点失败和消息丢失，这个两类情况都可以被视为是网络分区错误。 协议 在介绍这个协议之前，先来看一看经典的2PC协议[3]，2PC可以用下图简单地表示: +---------+ | | | q | | | ++---------+ | | +----------+ | | +----------+ | | | +-&gt;... <a href="/2017/09/A-Quorum-Based-Commit-Protocol.html">Read more!</a></p>
    </li>
    
    <li class="post">
        <small>11 Sep 2017</small>
        <small><a style="color:grey" href="/2017/09/In-Search-of-an-Understandable-Consensus-Algorithm.html#disqus_thread"></a></small>
        <br>
        <a class="nice-title" href="/2017/09/In-Search-of-an-Understandable-Consensus-Algorithm.html">In Search of an Understandable Consensus Algorithm</a>
        <p class="nice-text">In Search of an Understandable Consensus Algorithm Raft算法[2]是一致性新晋的热门选手。Raft在一些方面和Viewstamped Replication[4]相似。Raft也有一它自己的特点[1,3]: 1. 强领导者: Raft中领导者的地位更高，日志条目指由leader发送给其它部分。这个方式更加简单且易于理解； 2. 领导选举: Raft在领导选举中使用了一随机的计数器，这在解决领导选举的冲突时会更加有效； 3. 成为变更: Raft在成员变更时使用了一种新的joint consensus方法，这使得及时在成员变更时Raft也能工作。 Raft存在以下概念: 1. Leader: 通常情况下，系统中只有一个leader。 2. Follower: 通常情况下，出leader之外的都是follower。 3. Candidate: 领导人选举时的一种身份状态。 4. Term: 一个成员担任领导人的这段时间，算法中会用一个递增的数字(任期号)表示。 Raft主要分为3个部分: 1. 领导选举； 2. 日志复制； 3. 安全性。 领导选举 到Follower发现自己和Leader之间的心跳出现问题时，它就会启动一次新的选举。Foller首先要递增它当前的任期号，然后转变自己为Candidate状态，然后并行地向其它成员发送投票请求，知道以下情况发生然后停止: 1. 它赢得了选举; 2. 其它成员成为Leader； 3. 一段时间过后没有选举出领导人。 当一个Candidate获得半数以上的选票时，它就赢得了选举。每个领导者只会对一个任期号的第一个投票请求投票。在赢得选举之中，就会向其它成员发送消息来确定自己的地位。在等待投票的时候，它就可能收到其它成员的投票的请求，如果这个请求中的任期号比自己当前的任期号，那么它就会投这一票并回到Follower的状态，否则，拒绝请求。 这里可能发生的一种情况时几个候选人争夺Leader位置导致无法选举出Leader，Raft使用的解决方案时选举超时的时间从一个时间区间内随机选择(比如150ms ～... <a href="/2017/09/In-Search-of-an-Understandable-Consensus-Algorithm.html">Read more!</a></p>
    </li>
    
    <li class="post">
        <small>10 Sep 2017</small>
        <small><a style="color:grey" href="/2017/09/Paxos-Made-Simple.html#disqus_thread"></a></small>
        <br>
        <a class="nice-title" href="/2017/09/Paxos-Made-Simple.html">Paxos Made Simple</a>
        <p class="nice-text">Paxos Made Simple Paxos是分布式系统中的一个非常重要的算法，用于解决容错的分布式系统中对一个值达成一致的算法。论文[1]以一种很好的理解的方式描述了这个算法(这里讨论的是基础的paxos算法，不是其它的变种)。 算法假设有以下的一些条件: 1. 进程(or 角色)一任意的速度执行，每个都可能出错、停止运行，也可能错误之后恢复过来。所有进程可能在选定这个值值后都失败重启，它们可能在重启之后不能确定之前被选定的值，除非其中的一些进程保存下了这些信息。 2. 信息在传输的过程中可能被延时任意长的时间达到，也可能丢失、重复，但是信息的被人不会被修改。 总的来说，这篇paper讲的还是很清晰的。 提出问题 考虑在一个可能发生一些错误的一个分布式系统中，多个进程针可以提出自己的value，最终这个进程将会对这个值达成一致。当然在没有值没有被提出的时候，也就没有值会最终选定。当只有一个值被提出的时候，算法也要保证最终被选定的值就是这个唯一提出的值，多个不同的值被提出来之后，最终只会有一个值被选定。如果一个值一旦被选定，这些进程能知道这个值。 总结如下，为了保证safety的要求，算法要求: 1. 只有一个值被提出之后才能被选定； 2. 只有一个值最终被选定； 3. 一个进程认为被选定的值必须是真的最终被选定的值。 在这个算法之中，有以下的概念: Proposal: 这里可以理解为代表了一个值； Proposer: 提出Proposal； Acceptor: 决定是否接受Proposal； Learner: 接受被选定的值。 选定一个值 由于算法要求只有一个进程提出一个值的情况下这个值也会被选定，所以算法必须满足以下的要求: P1: 一个Proposer必须接受它收到的第一个Proposal 这又导致另外一个问题，由于系统中可能存在多个的Acceptor，这种做法可能导致不同的Acceptor接受了不同的值，所以这里最初一个规定: 规定: 一个Proposal只有在一半以上的Acceptor接受的情况下才能被选定 为了实现这个规定，就要求可以选定不同的Proposal(因为不这样的话就可能无法到达半数以上的Acceptor接受)。在这里，为了区分这些Proposal，我们赋予每个Proposal递增的一个编号。为了保证选定了不同的Proposal也能得到最终准确的结果，这里要求被选定的不同的Proposal的值是相等的。所以有如下的要求: P2: 如果一个Proposal被选定了，每个被选定的有更高的编号的Proposal的值必须与此相同 为了选定一个Proposal，必须要求有一个Acceptor接受，也就是说: P2a: 如果一个Proposal被选定了，那么每一个被Acceptor接受的Proposal必须与此相同 这个算法中，通信是不可靠的，进程也可能失败后又重启，也就可能存在下面这种情况: 一个Acceptor c之前没有收到过之前的Proposal，又有一个”新”(可能从失败之后恢复了过来)的Proposer向其发送了一个有更高编号的带有不同值的Proposal，由于要求P1，c必须接受这个Proposal，这就会导致维持P1和P2a直接的矛盾。为了解决这个问题，提出了以下的要求: P2b: 如果一个Proposal被选定了，那么之后的Proposer提出的编号更高的Proposal的值也必须与此相同。 从P2b可以推导出P2a，P2a有可以推导出P2(具体证明略，可以参考[1])。 那么如果保证P2b呢，这里采用的方法就是P2c: P2c:... <a href="/2017/09/Paxos-Made-Simple.html">Read more!</a></p>
    </li>
    
    <li class="post">
        <small>03 Sep 2017</small>
        <small><a style="color:grey" href="/2017/09/Distributed-%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%92%E6%96%A5.html#disqus_thread"></a></small>
        <br>
        <a class="nice-title" href="/2017/09/Distributed-%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%92%E6%96%A5.html">Distributed — 分布式互斥</a>
        <p class="nice-text">Distributed — 分布式互斥 分布式系统中，Concurrency是很重要的一个部分。任务在不同的进程中执行时，不可避免的会遇到访问相同的资源，这种情况下，就需要保证并发访问情况下的准确性的解决方案。 通常情况下，分布式互斥两种基本的策略[1]： 1. 基于令牌的算法(token-based)； 2. 基于许可的算法(permission-based)。 在基于令牌的算法中，所有的进程共享一个token，只有拥有token的进程才可以进入临界区，当临界区执行完之后，token被释放。token的唯一性保证了互斥，死锁也可以简单的被避免，不过一个问题时处理token丢失比较麻烦。 对于基于许可的算法，一个进程要到达所有进程的一个子集的允许后才能进入临界入区。 一个互斥的算法应该满足下面几个基本的要求[2]： 1. 安全性：如何情况下只能有一个进程能够进入临界区； 2. 活性：不存在死锁和饥饿，每个进程会在有限的时间了得到临界区的机会； 3. 公平性：每个进程得到执行临界区的机会时公平的(一般是指执行临界区的顺序是按照它们请求执行临界区的逻辑时间的先后顺序)。 除了基本的要求之外，对于一个分布式互斥的算法性能也是很重要的: 1. 进去临界区需要发送消息的数量； 2. 同步延时，从一个节点离开临界区到下一个节点进入需要的时间； 3. 响应时间，一个请求消息发出到请求的临界区执行结束的时间； 4. 系统吞吐量，系统执行临界区请求的速度，等于同步延时+平均临界区执行时间的倒数； 中央服务器算法 基于中央服务器的算法是一个简单有效的方法。最基本的思路是使用一个进程作为协调人。如果一个进程要执行临界区，它得向协调人请求，协调人在没有在其它的进程使用临界区的情况下就会授予改进程使用临界区的权限，否则将会拒绝请求，在进程执行完临界区之后，将会发送信息告知协调人释放临界区的锁。 这个算法的基本思路很简单，也很实用，现在实际使用的很多相关的系统也是使用了这个模型，比如Google Chubby[3]、Zookper[4]。但是要具体实现还要解决很多的问题，比如进程在临界区内Crash，协调人如何保证可靠性等等，具体可以参考[3，4]。 令牌环的算法 基于令牌的算法以及令牌环的思路在网络中的一些协议也很常见。在这个算法中，所有的进程被组织位一个逻辑上的环。 环上的进程被丛0开始依次编号，初始化时，token个授予编号为0的进程，token在环中由k进程传递到k+1进程(最后一个传回0号进程)。如果一个进程需要执行临界区，需要得到token被传递给它时，将token保留到它执行完临界区，如果进程不对token感兴趣，只需要简单地将token传递给下一个进程即可。 因为任何时间，最多有一个进程拥有token，使用互斥时包保证的。改算法也不会导致死锁。该算法也存在不少的问题，第一个就时token丢失的问题，token在传递的过程中可能被丢失，一个在一个持有token的进程在执行临界区时Crash也会造成token丢失。另外一个问题时环中的进程Crash会阻碍token的传递。 基与组播和逻辑时钟的算法 Lamport算法 Lamport发明的这种分布式互斥的算法时一种时间同步机制[2]，它要求通信时FIFO的。该算法具体表现为 当一个节点S(i)想进入临界区时，它广播REQUEST(ts(i),i)的消息给其它的所有节点，并将改请求放置到request queue(i)队列； 当其它的节点S(j)说到这个消息时，它将S(i)的请求放置到request-queue(j)的队列，并恢复一个带时间戳的REPLY消息。 当下面两个条件满足时，S(i)进入临界区: 01: S(i)从其它的所有站点收到一个时间戳大于（ts(i),i)的消息； 02: 节点S(i)的请求位于request-queue(i)的队首。 当S(i)执行完临界区后，从自己的请求队列删除自己的请求，并广播一个带时间戳的释放消息给其它节点。 其它节点收到后释放消息，从自己的request-queue删除S(i)的请求。 Ricard-Agrawala 算法... <a href="/2017/09/Distributed-%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%92%E6%96%A5.html">Read more!</a></p>
    </li>
    
    <li class="post">
        <small>01 Sep 2017</small>
        <small><a style="color:grey" href="/2017/09/Distributed-%E5%85%A8%E5%B1%80%E7%8A%B6%E6%80%81%E4%B8%8E%E5%BF%AB%E7%85%A7.html#disqus_thread"></a></small>
        <br>
        <a class="nice-title" href="/2017/09/Distributed-%E5%85%A8%E5%B1%80%E7%8A%B6%E6%80%81%E4%B8%8E%E5%BF%AB%E7%85%A7.html">Distributed — 全局状态与快照</a>
        <p class="nice-text">Distributed — 全局状态与快照 在没有共享存储器和全局时钟的分布式系统之中有效的纪录系统的全局状态很重要但也不是一件简单的事情。系统全局状态在死锁检测，故障恢复等方面都有很重要的作用。 分布式系统的全局状态是进程和通道本地状态的集合，使用符号GS表示[1]。 分布式系统中的一致性快照需要处理一下的两个问题: 1. 如何判别纪录在快照中的消息和没有在快照中的消息，要求: 0x01. 在纪录快照之前的一个进程发送的消息一定都会被记录在全局快照之中； 0x02. 在纪录快照之后的一个进程发送的消息一定都不会被记录在全局快照之中。 2. 如何确定快照的瞬间。 这里只讲了一些很基础的东西。 FIFO通道: Chandy - Lamport 算法 Chandy - Lamport 算法算法假设通信的通道是FIFO的和可靠的，它使用了一个称为标记的控制信息。算法主要由两个部分组成: 标记发送规则和标记接收规则。消息一个进程在记录完它自己的快照之后，向其它所有的进程发送一个标记，这个被称为标记发送规则。因为通信通道是FIFO的，使用标记将在快照之前的消息标示出来，这样可以满足上面的第1点。 一个进程在接受到来自一个通道C标记之后，如果它还没有记录自身的状态，则记录接受到标记的通道C的状态为空，并执行上面描述的的标记发送规则。如果已经接受到C的标记，则记录C的状态记录为在该通道上记录了本地状态之后且在接受标记之前的消息的集合。 算法的过程如下[1，2]: 1. 进程p的标记发送规则: 0x01 进程p记录本地状态； 0x02 对其它进程发送标记。 2. 进程p的标记接收规则: 进程p在通道C接收到标记后，如何p没有记录自身的状态，则记录C的状态为空并执行标记发送规则， 否则，记录C的状态为消息集。 算法在进程p接收到来自所有输入通道的标记之后终止。 Variants 以Chandy - Lamport算法为基础，再次之上有多种算法的变种。 Spezialetti - Kearns 算法主要在快照收集的并发启动和被记录快照的有效发送。 在一些系统之中，会定期的收集系统的全局快照，Venkatesan快照增量算法优化了这种情况，Venkatesan快照增量算法将上一次获取的快照和记录最后一次快照以来的增量快照结合在一起，形成目前的系统快照。 应用 Flink中的创建快照的算法[4]主要收到了Chandy -... <a href="/2017/09/Distributed-%E5%85%A8%E5%B1%80%E7%8A%B6%E6%80%81%E4%B8%8E%E5%BF%AB%E7%85%A7.html">Read more!</a></p>
    </li>
    
    <li class="post">
        <small>25 Aug 2017</small>
        <small><a style="color:grey" href="/2017/08/Facebook-Gorilla.html#disqus_thread"></a></small>
        <br>
        <a class="nice-title" href="/2017/08/Facebook-Gorilla.html">Facebook Gorilla</a>
        <p class="nice-text">Facebook Gorilla Gorilla是Facebook公司内部的时序数据库产品。Facebook在世纪的使用过程中发现现有的产品不能满足对Facebook超大数据量的处理要求，开发了Gorilla这一样一个产品，通过应用多种压缩方法、将数据放在内存之中，Gorilla获得了73x的延时提升、14x的吞吐量提升 介绍&amp;背景 2013年Facebook就开始使用一套基于HBase的时序数据库，但是随着Facebook的发展，这套系统以及不能满足未来的负载，90%的查询已经长达几秒。一个对几千个时间序列的查询要消耗几十秒的时间来执行，而在稀疏的更大数据集上查询通常会超时。HBase被设置为写入优化，现在在其中已经保存了2PB的数据，查询的效率不高，又不太好完全更换系统。所以，Gorilla讲注意力转移到给现有的系统在一个in-memory的cache。这里数据的特点就是新数据一般是热点数据，所以选择奖近段时间内的数据cache，就能很好地提高性能。 Memcache是Facebook大规模使用的一个缓存系统，但是将memcache应用在这里，追加新数据到已经存在的时间序列中要消耗一个read／write周期，给memcache造成很大的压力，所以需要一种更好的解决方案。注：这里没怎么看懂，原文是： We also considered a separate Memcache [20] based write-through cache but rejected it as appending new data to an existing time series would require a read/write cycle, causing extremely high traffic to the memcache server. We needed a more efficient solution. 对于Gorilla，主要有以下的要求： 1. 2billion的unique的时间序列id，以string表示；... <a href="/2017/08/Facebook-Gorilla.html">Read more!</a></p>
    </li>
    
    <li class="post">
        <small>19 Aug 2017</small>
        <small><a style="color:grey" href="/2017/08/A-Critique-of-ANSI-SQL-Isolation-Levels.html#disqus_thread"></a></small>
        <br>
        <a class="nice-title" href="/2017/08/A-Critique-of-ANSI-SQL-Isolation-Levels.html">A Critique of ANSI SQL Isolation Levels</a>
        <p class="nice-text">A Critique of ANSI SQL Isolation Levels 在ANSI SQL-92 [MS, ANSI]（之后简称SQL-92）根据Phenomena（这个类似专有名词，不翻译了，中文意思是‘现象’）定义了SQL的隔离级别：Dirty Reads, Non-Repeatable Reads, and Phantoms。《A Critique of ANSI SQL Isolation Levels》[1]这篇paper阐述了有些Phenomena是无法用SQL-92中定义的一些隔离级别正确表征的，包括了各个基本上锁的实现。该论文讨论了SQL-92中的Phenomena中的定义模糊的地方。除此之外，还介绍了更好表征隔离级别的Phenomena，比如Snapshot Isolation。 介绍 不同的隔离级别定义了不同的在并发、吞吐量之间的取舍。较高的级别更容易正确的处理数据，而吞吐量比较低的隔离级别更低，较低的隔离级别更容易获得更高的吞吐量，却可能导致读取无效的数据和状态。 SQL-92定义了4个隔离级别： (1) READ UNCOMMITTED, (2) READ COMMITTED, (3) REPEATABLE READ, (4) SERIALIZABLE. 它使用了serializability的定义，加上3种禁止的操作子序列来定义这些隔离级别，这些子序列被称为Phenomena，有以下3种：Dirty Read, Non-repeatable Read, and Phantom（一般被翻译为脏读，不可重复读，幻读，不过个人认为不翻译直接理解更好）。规范只是说明phenomena是可能导致异常（anomalous)的动作序列。 ANSI的隔离级别与lock schedulers的行为相关。一些lock scheduler允许改变lock的范围和持续的时间（一般是为优化了性能），这就导致了不符合严格的两阶段锁定。由[GLPT]引入了以下3种方式定义的Degrees of Consistency（一致性程度）： 1. 锁(locking)；... <a href="/2017/08/A-Critique-of-ANSI-SQL-Isolation-Levels.html">Read more!</a></p>
    </li>
    
    <li class="post">
        <small>16 Aug 2017</small>
        <small><a style="color:grey" href="/2017/08/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E4%B8%AD%E7%9A%84%E6%97%B6%E9%97%B4-hybrid-logical-clock.html#disqus_thread"></a></small>
        <br>
        <a class="nice-title" href="/2017/08/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E4%B8%AD%E7%9A%84%E6%97%B6%E9%97%B4-hybrid-logical-clock.html">Hybrid Logical Clock</a>
        <p class="nice-text">分布式系统中的时间 — hybrid logical clock 发布系统中的时间有很重要的作用，比如Spanner的TrueTime[2]。但是TrueTime实现的难度太大，就算开源社区实现了，也不太好被一般的用户使用。所以CockRoachDB使用了Hybrid Logical Clock(HLC)，HLC是一种Logical Clock的实现，HPC将Logical Clock和物理时钟联系起来，与物理时间之间的误差在一个固定的值之内，这个值由NTP决定。 介绍 分布式系统中有几个关于时间的概念： 1. Logical clock (LC)：逻辑时间是有Lamport提出的，LC独立于物理上的时间存在。 2. Physical Time (PT)： 3. TrueTime (TT)：TT出现的时间比较近，在Google的Spanner中被使用。目前看来，这个最有B格。 4. HybridTime (HT)： 背景 对于由一系列可能随时间变化的节点组成的分布式系统，每个节点可以执行3中操作：1.发送动作，2. 接收动作和3.本地动作。时间戳算法为每个时间分配时间戳。如果使用LC算法来分配时间戳，则给时间e分配的时间记为 lc.e。 HLC HLC的设计目标是提供像LC一样的单向因果检测，同时保持时钟的值总是接近物理的时间（这里是NTP的时间）。一个HLC的表述如下： 给定一个分布式系统，为每一个事件分配一个时间戳，有： 1. e hb f ⇒ l.e &lt; l.f，e事件happen before f，则有l.e &lt; l.f;  2. Space requirement for l.e... <a href="/2017/08/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E4%B8%AD%E7%9A%84%E6%97%B6%E9%97%B4-hybrid-logical-clock.html">Read more!</a></p>
    </li>
    
    <li class="post">
        <small>15 Aug 2017</small>
        <small><a style="color:grey" href="/2017/08/Distributed-%E6%97%B6%E9%97%B40x01.html#disqus_thread"></a></small>
        <br>
        <a class="nice-title" href="/2017/08/Distributed-%E6%97%B6%E9%97%B40x01.html">Distributed — 时间0x01</a>
        <p class="nice-text">Distributed — 时间0x01 之前说了分布式系统中一些关于时间的基本概念和一些其他的东西，这里我们来讨论分布式系统中物理时间。 首先有一下的几点： 1. 相对论中，由于存在多个参考系，因此时间测量可能是不准确的。当然，地球上的分布式系统中，相对型的影响是可以忽略的。 2. 即使这样，受目前技术能力的限制，我们还是不能准确记录不同点上的事件发生的时间，以此表明事件发生的顺序。 我们将两个时钟读数之间的瞬间不同被称为始终漂移（clock skew）。时钟漂移以单个时钟读数和完美的参考时钟之间的漂移来度量。此外，参考时钟度量的每个单位时间内，和完美的参考时钟之间的漂移量称为漂移率。目前，原子钟时漂移率最小的时钟（Spanner中就使用了原子钟）。 原子时钟的输出被用作实际时间的标准，称为国际原子时间，而秒、年等我们使用的时间单位来源于天文时间，与原子时间并不一致， 通用协调时间（UTC）是国际计时标准，它基于原子时间的，但是偶尔需要增加闰秒或极偶尔的情况下要删除闰秒（这个闰秒导致了很多的软件故障）[1]。 同步物理时钟 为了知道分布式系统P的进程中事件发生的具体时间，有必要用权威的外部时间源同步进程的时钟Ci – 外部同步（external synchronization）。 如果时钟Ci与其他时钟同步到一个已知的精度，那么我们就能通过本地时钟度量在不同计算机上发生的两个事件的间隔 – 内部同步（internal synchronization）。 这里要定义一下时钟的正确性。正如我们所知，完全准确的时钟时几乎不可能的，所以： 时钟正确性（correctness）通常定义为，如果一个硬件时钟H的漂移率在一个已知的范围ρ&gt;0内，那么该时钟是正确的。 1. 这表明度量实际时间t和t’的时间间隔的误差是有界的 (1- ρ)(t’-t) ≤ H(t’)-H(t) ≤ (1+ ρ)(t’-t) 2. 该条件禁止了硬件时钟值的跳跃，有时，软件也时钟也要求遵循该条件。但是用一个较弱的单调性条件就足够了。 3. 单调性是指一个时钟C前进的条件 ``` t’&gt;t ==&gt; C(t’) &gt; C(t) ``` 现在来看看时钟同步中的一些情况: 一个进程在消息m中将本地时钟的时间发送给另一个进程: 最简单的做法就是接收进程可以将它的时钟设成t+T(trans)，但是T(trans)是不确定的。 在同步的系统中： 设消息传输时间的不确定性为u，那么有u=(max-min)； 如果接收放将时钟设置为t+min，那么时钟偏移至多为u；... <a href="/2017/08/Distributed-%E6%97%B6%E9%97%B40x01.html">Read more!</a></p>
    </li>
    
    <li class="post">
        <small>01 Aug 2017</small>
        <small><a style="color:grey" href="/2017/08/Viewstamped-Replication.html#disqus_thread"></a></small>
        <br>
        <a class="nice-title" href="/2017/08/Viewstamped-Replication.html">Viewstamped Replication</a>
        <p class="nice-text">Viewstamped Replication Viewstamped Replication(VR)是一个适用于故障-停止的异步系统中的一个关于复制的算法，发布于80年代[2]。 在论文[1]中有一段这样的话: VR was originally developed in the 1980s, at about the same time as Paxos [5, 6], but without knowledge of that work. It differs from Paxos in that it is a replication protocol rather than a consensus protocol: it uses consensus, with a protocol very... <a href="/2017/08/Viewstamped-Replication.html">Read more!</a></p>
    </li>
    
    <li class="post">
        <small>12 Jul 2017</small>
        <small><a style="color:grey" href="/2017/07/Finding-a-needle-in-Haystack.html#disqus_thread"></a></small>
        <br>
        <a class="nice-title" href="/2017/07/Finding-a-needle-in-Haystack.html">Finding a needle in Haystack</a>
        <p class="nice-text">Finding a needle in Haystack: Facebook’s photo storage Haystack是对象存储的经典设计，它是Facebook的照片应用而专门优化定制的对象存储系统。Facebook在峰值时需提供每秒查询超过1 million图片的能力。 设计上的关键点： 传统的基于NAS or NFS的存储图片的方式，因为元数据查询而导致了过多的磁盘操作。在Haystack的设计中，追求尽量减少每个图片的元数据，让Haystack能在内存中执行所有的元数据查询。在传统的POSIX的文件系统中，读取一个图片要经过读取目录信息，读取文件信息，最后才能读取文件信息，要经过多次的磁盘操作。访问文件的元数据成为了吞吐量的瓶颈。 每个数据只会写入一次、读操作频繁、从不修改、很少删除； 设计目标： 1. 高吞吐量和低延迟。 2. 容错。 3. 高性价比。 4. 简单。 架构 Haystack架构包含3个核心组件：Haytack Store、Haystack Directory和Haystack Cache。 1. Store是持久化存储系统，并负责管理图片的文件系统元数据。Store将数据存储在物理的卷上，此外，不同机器上的多个物理卷对应一个逻辑卷。一个图片存储到一个逻辑卷时，图片被写入到所有对应的物理卷。在物理卷的基础之上加上一个逻辑卷的方法可以解决一个物理卷损坏时导致的数据丢失和服务不可用。 2. Directory维护了逻辑到物理卷的映射以及其的元数据，比如某个图片保存在哪个逻辑卷已经某个逻辑卷的空闲空间的信息。 3. Cache的功能类似系统内部的CDN，主要处理热门的图片。Cache会尽可能的cache图片数据。Cache的存在也让系统在没有CDN的情况下也能很好的工作。 基本操作 访问一个图片：当用户使用Directory为每个图片来构建的一个URL来访问一个图片。这个URL包含几段信息，每一段内容对应了到从浏览器访问CDN(或者Cache)直至最终在一台Store机器上检索到图片的过程。一个典型的URL如下： http://&lt;CDN&gt;/&lt;Cache&gt;/&lt;Machine id&gt;/&lt;Logical volume, Photo&gt; 第一个部分指明了从哪个CDN查询此图片。到CDN后它使用最后部分的URL，包含了逻辑卷和图片ID等信息，即可查找缓存的图片。如果CDN未命中缓存，则从URL中删除相关信息，然后访问Cache。Cache的查找过程与之类似，如果还没命中，则去掉相关信息，请求被发至指定的Store机器，即。访问的过程也可以不经过CDN，而直接访问Cache。 Cache向Store请求一个图片时，需要提供逻辑卷id、key、alternate key，和cookie。Cookie的设计主要是为了安全，cookie是个数字，嵌在URL中。当一张新图片被上传，Directory为其随机分配一个cookie值，并作为应用元数据之一存储在Directory。此cookie可以保证所有发往Cache或CDN的请求都是经过Directory“批准”的，而Store，Cache就能放心处理数据。 当Store机器接收到Cache机器发来的图片查询请求，由于元数据会被保存在内存之中，可以快速查找。如果图片没有被删除，Store在卷文件（一个卷文件保存了大量的图片）中seek到相应的offset，从磁盘上读取整个图片的数据（这里被称为needle），然后检查cookie和数据完整性，若都合法则将图片数据返回到Cache机器。 上传一个图片：用户上传一个图片时，图片首先被发送到web服务器。web服务器随后从Directory中请求一个可写逻辑卷，然后web服务器为图片分配一个唯一的ID，web服务器必须向store辑卷id、key、alternate key、cookie和真实数据等信息。Store接受到之后，store创建一个新的needle，添加到卷文件的末尾，更新保存在内存中的元数据和映射。 更新一个图片：Haystack的设计是不考虑修改图片的。Store append-only的工作方式也不能很好的支持修改性的操作，又haystack并不允许覆盖needle，所以图片的修改都是直接通过添加一个新needle，其拥有相同的key和alternate key来完成。而如果更新之后的needle被写入到与老的needle不同的逻辑卷，需要Directory更新它的应用元数据，未来的请求都路由到新逻辑卷，如果被写入到同一个逻辑卷，则也会被store保存到同一个物理卷，根据offset的不同就可以判断文件的新旧。... <a href="/2017/07/Finding-a-needle-in-Haystack.html">Read more!</a></p>
    </li>
    
    <li class="post">
        <small>22 Jun 2017</small>
        <small><a style="color:grey" href="/2017/06/%E6%AF%95%E4%B8%9A%E4%BA%86.html#disqus_thread"></a></small>
        <br>
        <a class="nice-title" href="/2017/06/%E6%AF%95%E4%B8%9A%E4%BA%86.html">毕业了</a>
        <p class="nice-text">毕业了

不悔梦归处，只恨太匆匆



 <a href="/2017/06/%E6%AF%95%E4%B8%9A%E4%BA%86.html">Read more!</a></p>
    </li>
    
    <li class="post">
        <small>17 Apr 2017</small>
        <small><a style="color:grey" href="/2017/04/Lorem-Ipsum.html#disqus_thread"></a></small>
        <br>
        <a class="nice-title" href="/2017/04/Lorem-Ipsum.html">Lorem Ipsum</a>
        <p class="nice-text">Lorem ipsum dolor sit amet, ut paulo aperiam signiferumque quo. Quaeque pertinacia mnesarchum te vel. Ei tollit habemus delectus mel, pro zril cetero adipiscing an. Eius aliquip legimus ad mea, vix te quod eripuit, munere doctus oporteat qui te. Dolore fierent nam ea. Eos ex tale commune moderatius, ei mel... <a href="/2017/04/Lorem-Ipsum.html">Read more!</a></p>
    </li>
    
</ul>

<script type="text/javascript">
    /* * * CONFIGURATION VARIABLES * * */
    var disqus_shortname = 'nagekar';

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function () {
        var s = document.createElement('script'); s.async = true;
        s.type = 'text/javascript';
        s.src = '//' + disqus_shortname + '.disqus.com/count.js';
        (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
    }());
</script></span></div>
</div>


      </div>

      <div class="tile is-4 is-child">
        <div class="tile is-parent is-vertical sidebar">

          <div class="tile is-child widget">

            <div class="card">
              <header class="card-header">
                <p class="card-header-title nice-title">
                  Recent Posts
                </p>
              </header>
              <div class="card-content">
                <div class="content nice-text">
                  <ul>
                  
                  <li><a href="/2018/09/Homa-A-Receiver-Driven-Low-Latency-Transport-Protocol-Using-Network-Priorities.html">
                    A Receiver-Driven Low-Latency Transport Protocol
                  </a></li>
                  
                  <li><a href="/2018/09/Re-architecting-datacenter-networks-and-stacks-for-low-latency-and-high-performance.html">
                    NDP transport protocol
                  </a></li>
                  
                  <li><a href="/2018/09/ISA-Wars-Understanding-the-Relevance-of-ISA-being-RISC-or-CISC-to-Performance,-Power,-and-Energy-on-Modern-Architecture.html">
                    ISA Wars
                  </a></li>
                  
                  </ul>
                </div>
              </div>
            </div>
          </div>

          <div class="tile is-child widget">

            <div class="card">
              <header class="card-header">
                <p class="card-header-title nice-title">
                  Sponsored
                </p>
              </header>
              <div class="card-content">
                <div class="content nice-text">
                  <img src="/assets/images/Dust_Bunny.jpg"/>
                </div>
              </div>
            </div>

          </div>

          <div class="tile is-child widget">
            <div class="card">
              <header class="card-header">
                <p class="card-header-title nice-title">
                  Recommended Websites
                </p>
              </header>
              <!-- <div class="card-content">
                <div class="content nice-text">
                  <ul>
                    <li>
                      <a href="https://xkcd.com">xkcd</a>
                    </li>
                    <li>
                      <a href="http://commitstrip.com">Commit Strip</a>
                    </li>
                    <li>
                      <a href="http://www.smbc-comics.com">SMBC Comics</a>
                    </li>
                    <li>
                      <a href="https://blog.codinghorror.com">Coding Horror</a>
                    </li>
                    <li>
                      <a href="http://waitbutwhy.com">Wait Buy Why</a>
                    </li>
                    <li>
                      <a href="https://simplysoch.wordpress.com/">Simply Soch</a>
                    </li>
                  </ul>
                </div> -->
              </div>
            </div>
          </div>

        </div>

      </div>

    </div>

  </div>
</div>

<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <p>
        Some rights reserved.
      </p>
      <p>
        Made with <i class="fa fa-heart"></i> and <a href="https://github.com/jgthms/bulma">Bulma</a>, <a href="https://jekyllrb.com/">Jekyll</a>. Hosted on <a href="https://github.com/">Github</a>
      </p>
      <p>
        <a class="icon" href="https://github.com/abhn">
          <i class="fa fa-github" title="Github"></i>
        </a>
        <a class="icon" href="https://in.linkedin.com/in/AbhishekNagekar">
          <i class="fa fa-linkedin" title="Linkedin"></i>
        </a>
        <a class="icon" href="mailto:abhishek@nagekar.com">
          <i class="fa fa-envelope" title="Email"></i>
        </a>
      </p>
    </div>
  </div>
</footer>

<!-- js -->
<script src="/assets/js/custom.js"></script>
<script src="/assets/js/typewriter.js"></script>
</body>
</html>
