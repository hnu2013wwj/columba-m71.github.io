<?xml version="1.0" encoding="UTF-8" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
        <title>Columba M71's Blog</title>
        <description>Columba M71's Blog - Columba M71</description>
        <link>https://www.nagekar.com</link>
        <atom:link href="https://www.nagekar.com/rss.xml" rel="self" type="application/rss+xml" />
        <lastBuildDate>Fri, 12 Oct 2018 23:20:44 +0800</lastBuildDate>
        <pubDate>Fri, 12 Oct 2018 23:20:44 +0800</pubDate>
        <ttl>60</ttl>


        <item>
                <title>LegoOS -- A Disseminated, Distributed OS for Hardware Resource Disaggregation</title>
                <description>&lt;h2 id=&quot;legoos-a-disseminated-distributed-os-for-hardware-resource-disaggregation&quot;&gt;LegoOS: A Disseminated, Distributed OS for Hardware Resource Disaggregation&lt;/h2&gt;

&lt;h3 id=&quot;0x00-引言&quot;&gt;0x00 引言&lt;/h3&gt;

&lt;p&gt;这篇文章是OSDI 2018的best paper之一。这篇paper针对目前这样一个情况，提出了一种新的OS架构：目前硬件资源在数据中心这样的地方很多时候分布在不同的机器之后，而传统的一个系统由固定的几个硬件组成，主要的如内存，处理器，硬盘等。不同的应用的特性不同，对资源的使用也不同，就可能出现在一台机器上的一些资源已经很紧张了，另外的一些资源却没有怎么使用。&lt;/p&gt;

&lt;p&gt;针对这种情况，这里提出了一种叫做splitkernel 的架构，在splitkernel的模式下，设计并实现了一个LegoOS的操作系统，这个系统的主要特点就是资源分布在不同的地方，每一个地方运行不同的系统组件，各个组件之间由高速网络连接(这里使用的就是RDMA)，这些组件的组合就形成了一个新的系统。Lego这个名字取的很生动形象。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Following the splitkernel model, we built LegoOS, the first OS designed for hardware resource disaggregation. LegoOS is a distributed OS that appears to applications as a set of virtual servers (called vNodes). A vNode can run on multiple processor, memory, and storage components and one component can host resources for multiple vNodes. LegoOS cleanly separates OS functionalities into three types of monitors, process monitor, memory monitor, and storage monitor. LegoOS monitors share no or minimal states and use a customized RDMA-based network stack to communicate with each other.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;.&lt;/p&gt;

&lt;h3 id=&quot;0x01-the-splitkernel-os-architecture&quot;&gt;0x01 The Splitkernel OS Architecture&lt;/h3&gt;

&lt;p&gt;splitkernel的基本架构与传统的Monolithic Kernel和Multi-Kernel架构的对比(Multi-Kernel也是一种新的内核架构，主要思想是在不同的硬件部分运行不同的内核，这些内核可以针对不同的硬件定制，但是这些还是在一个机器上面的，关于Multi-Kernel可参考[2])&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/legoos-arch.png&quot; alt=&quot;legoos-arch&quot; /&gt;&lt;/p&gt;

&lt;p&gt;对于splitkernel由4个设计的核心原则:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Split OS functionalities，分离内核功能，将传统内核的功能大散为monitors，每一个monitors管理自己的资源，为硬件资源提供保护、虚拟化等的功能；&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Run monitors at hardware components，不同的硬件上运行不同的硬件程序。&lt;/p&gt;

    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;This design makes it easy to integrate heterogeneous hardware in datacenters — to deploy a new hardware device, its developers only need to build the device, implement a monitor to manage it, and attach the device to the network. Similarly, it is easy to reconfigure, restart, and remove hardware components.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Message passing across non-coherent components，模块之间通过消息传递来交换，这些消息不考虑coherent，coherent是个很复杂的东西，会给系统带来很高的成本。&lt;/p&gt;

    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;A splitkernel still retains the coherence guarantee that hardware already provides within a component (e.g., cache coherence across cores in a CPU), and applications running on top of a splitkernel can use message passing to implement their desired level of co- herence for their data across components.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Global resource management and failure handling，全局的资源和错误管理。在splitkernel中，一个资源组件可以被多个应用使用，自然故障就会影响到这些所有的应用。为了保证系统的可拓展性，splitkernel只会粗粒度地管理资源，更加细粒度的管理则交给monitors自己。对于容错，使用的就是常见的添加冗余的方式。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;.&lt;/p&gt;

&lt;h3 id=&quot;0x02-legoos-design&quot;&gt;0x02 LegoOS Design&lt;/h3&gt;

&lt;p&gt;splitkernel是一种kernel的架构，具体的实现这里实现了一个LegoOS。LegoOS中的组件这里只讨论了处理器，内存和存储，分别叫做 pComponent, mComponent, 和 sComponent。&lt;/p&gt;

&lt;p&gt;LegoOS暴露出一组vNodes(virtual nodes)给用户。在用户看了，一个vNode就像一个虚拟机，每一个vNode由一个唯一的id、一个唯一的IP地址和一个存储设备挂载点。每一个vNode之间的资源是隔离的。一个vNode可以同时使用多个pComponent, mComponent, 和 sComponent，与此同时，一个硬件组件又可以被多个vNode使用(简直就像乐高积木一样好玩)。具体的硬件资源信息对用户是透明的，他们不知道具体的硬件资源的使用。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;With splitkernel’s design principle of components not being coherent, LegoOS does not support writable shared memory across processors. LegoOS assumes that threads within the same process access shared memory and threads belonging to different processes do not share writable memory, and LegoOS makes scheduling decision based on this assumption.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;组件之间通过网络连接&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/legoos-component.png&quot; alt=&quot;legoos-component&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;process-management&quot;&gt;Process Management&lt;/h4&gt;

&lt;p&gt;LegoOS 的 process monitor运行在pComponent上面的内核空间，管理着pComponent的硬件资源。然后在pComponent的用户空间内运行应用程序。LegoOS 为内核的后台线程专门提供了少量CPU核心, 其余的CPU核心用于运行应用程序线程。pComponent的私有的memory被视为virtual cache，这里叫做ExCache。&lt;/p&gt;

&lt;p&gt;当一个新进程被启动时, LegoOS使用一个全局的策略为其选择一个pComponent，在此之后，LegoOS之后使用应用需要的最少的CPU核心去运行它。一个应用线程分配到核心后， 正常情况下，LegoOS会让这个线程一直运行，没有调度和抢占之类的东西，只有在这个pComponent运行的线程超过了它的CPU核心数量的时候才会有调度之类的行为。由于CPU资源和其它资源是分开的，这样分配CPU资源的时候就只需要CPU资源的因素了。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;LegoOS improves the overall processor utilization in a disaggregated cluster, since it can freely schedule processes on any pComponents without considering memory allocation. Thus, we do not push for perfect core utilization when scheduling indi- vidual threads and instead aim to minimize scheduling and context switch performance overheads.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;此外，LegoOS支持Linux的API。但是Linux的API是有状态的，LegoOS则是追求无状态的设计原则。为了实现这些目标，这里的做法是在pComponent的monitor上添加了一层，由于保持状态，达到实现支持Linux API的目的。&lt;/p&gt;

&lt;p&gt;.&lt;/p&gt;

&lt;h4 id=&quot;memory-management&quot;&gt;Memory Management&lt;/h4&gt;

&lt;p&gt;在LegoOS中，mComponent被分为三种类型：匿名内存 (i.e., heaps, stacks),内存映射文件, 以及存储的buffer。LegoOS的memory monitor同时管理虚拟和物理的地址空间，负责内存的读写操作，以及他们的分配、回收和映射等操作。&lt;/p&gt;

&lt;p&gt;在LegoOS的mComponent上面，是不运行用户进程的，它只运行kernel的代码，提供内存相关的服务。LegoOS将一个进程的地址空间跨越多个mComponent，以便于实现更加好的性能和更好的内存利用率。此外，在一个进程被新创建的时候，LegoOS使用一个全局的内存资源管理器来给这个进程分配一个home mComponent(有一点是它老家的感觉).&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;At the higher level, we split each virtual memory address space into coarse-grained, fix-sized virtual regions, or vRegions (e.g., of 1 GB). Each vRegion that contains allocated virtual memory addresses (an active vRegion) is owned by an mComponent. The owner of a vRegion handles all memory accesses and virtual memory requests within the vRegion.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/legoos-memory.png&quot; alt=&quot;legoos-memory&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;storage-management&quot;&gt;Storage Management&lt;/h4&gt;

&lt;p&gt;同之前的设计一样，LegoOS也追求sComponent的无状态的设计，使用它的每一个IO请求都包含了需要的所有消息，包括完整的路径，绝对的文件偏移等。Lego支持类似Posix的层级式的文件存储组织方式，但这个只是对外的提供的接口抽象。在内部的实现上来说，Lego的storage monitor 将完整的路径视为一个文件的名字，并将一个vNode的文件在一个sComponent的一个内部的目录下面。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;To locate a file, LegoOS storage moni- tor maintains a simple hash table with the full paths of files (and directories) as keys. From our observation, most datacenter applications only have a few hundred files or less. Thus, a simple hash table for a whole vNode is sufficient to achieve good lookup performance.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;对于存储的buffer，LegoOS将其保存在mComponents而不是sComponents(上面提到过mComponent有三种类型，一种就是存储buffer)，因为主要的内存资源在mComponent。&lt;/p&gt;

&lt;h4 id=&quot;global-resource-management&quot;&gt;Global Resource Management&lt;/h4&gt;

&lt;p&gt;前面提到了monitors负责细粒度的资源分配，粗粒度的资源分配是有全局的资源管理器实现的。LegoOS将管理CPU，内存和磁盘的全局管理器分配叫做GPM, GMM,和 GSM。这些资源管理器可以运行在一个普通的Linux机器上面，它们只维护大致的一个信息，同过在每次做分配资源决定的时候后者周期性的询问monitors来更新信息。&lt;/p&gt;

&lt;p&gt;这里举例来说明，process monitors 只有在创建新的进程的时候才会询问GPM，然后GPM选择一个可以满足其要求的pComponent来运行这个进程，而分配线程到的时候，process monitors直接自己处理就行。同理，分配内存也是一样的，只有在要分配新的vRegion的时候才询问GMM。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;LegoOS decouples the allocation of different resources and can freely allocate each type of resource from a pool of components. Doing so largely improves resource packing compared to a monolithic server cluster that packs all type of resources a job requires within one physical machine. 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;.&lt;/p&gt;

&lt;p&gt;关于错误处理和实现的信息在论文中有更多的内容。&lt;/p&gt;

&lt;p&gt;.&lt;/p&gt;

&lt;h3 id=&quot;0x03-evaluation&quot;&gt;0x03 Evaluation&lt;/h3&gt;

&lt;p&gt;具体数据参看论文&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/legoos-benchmark.png&quot; alt=&quot;legoos-benchmark&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;参考&quot;&gt;参考&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;LegoOS: A Disseminated, Distributed OS for Hardware Resource Disaggregation, OSDI ’18.&lt;/li&gt;
  &lt;li&gt;The Multikernel: A new OS architecture for scalable multicore systems, SOSP 2009.&lt;/li&gt;
&lt;/ol&gt;
</description>
                <link>https://www.nagekar.com/2018/10/LegoOS.html</link>
                <guid>https://www.nagekar.com/2018/10/LegoOS</guid>
                <pubDate>Wed, 10 Oct 2018 00:00:00 +0800</pubDate>
        </item>

        <item>
                <title>Write-Optimized and High-Performance Hashing Index Scheme for Persistent Memory</title>
                <description>&lt;h2 id=&quot;write-optimized-and-high-performance-hashing-index-scheme-for-persistent-memory&quot;&gt;Write-Optimized and High-Performance Hashing Index Scheme for Persistent Memory&lt;/h2&gt;

&lt;h3 id=&quot;0x00-引言&quot;&gt;0x00 引言&lt;/h3&gt;

&lt;p&gt;这篇OSDI 2018会议(就是今天开的, 2018-10-08)上的一篇关于Persistent Memory上hash index设计的文章[1]，是Path Hashing[2]的后续，也是华科在OSDI上发表的第一篇文章？？？这篇论文讨论了Path Hashing没有解决的问题，其中一个就是resize如何处理。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;To cost-efficiently resize this hash table, level hashing leverages an in- place resizing scheme that only needs to rehash 1/3 of buckets instead of the entire table, thus significantly reducing the number of rehashed buckets and improving the resizing performance. Experimental results demon- strate that level hashing achieves 1.4×−3.0× speedup for insertions, 1.2×−2.1× speedup for updates, and over 4.3× speedup for resizing, while maintaining high search and deletion performance, compared with state- of-the-art hashing schemes.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;.&lt;/p&gt;

&lt;p&gt;这里关于NVM之类的特点都不提了，可参考相关资料。这里只关注Lelvel Hasing的设计以及如何解决现在的问题。&lt;/p&gt;

&lt;blockquote&gt;

&lt;/blockquote&gt;

&lt;h3 id=&quot;0x01-基本思路&quot;&gt;0x01 基本思路&lt;/h3&gt;

&lt;p&gt;基本结构:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/level-hashing-arch.png&quot; alt=&quot;level-hashing-arch&quot; /&gt;&lt;/p&gt;

&lt;h5 id=&quot;multiple-slots-per-bucket&quot;&gt;Multiple Slots per Bucket&lt;/h5&gt;

&lt;p&gt;每一个Hash table由2层组成，Top Level和Bottom Level，其中Top Level的大小是Bottom Level的2倍。每一个bucket里面有4个slot，这个做法和常见的一个cuckoo hash的设计差不多。&lt;/p&gt;

&lt;h5 id=&quot;two-hash-locations-for-each-key&quot;&gt;Two Hash Locations for Each Key&lt;/h5&gt;

&lt;p&gt;使用2个hash函数，这样一个Level里面有两个候选的buckets，加上Bottom里面的，就是4个，不过Bottom Level里面的一个bucket是被Top Level里面的2个bucket共用的。&lt;/p&gt;

&lt;h5 id=&quot;sharing-based-two-level-structure&quot;&gt;Sharing-based Two-level Structure&lt;/h5&gt;

&lt;p&gt;Bottom Level里面的一个bucket是被top level里面的2个bucket共用的，这里的思想和Path Hashing里面的是一样的。不够这里的Bottom Level是上次rehash之前的Top Level。&lt;/p&gt;

&lt;h5 id=&quot;at-most-one-movement-for-each-successful-insertion&quot;&gt;At Most One Movement for Each Successful Insertion&lt;/h5&gt;

&lt;p&gt;这里选择可用的solt利用了cuckoo hash的思路，但是为了解决cuckoo hash级联的数据驱逐问题。这里最多允许最多移动一个项。选择的步骤如下:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;利用两个hash函数计算出的2个位置，检测使用优空的slot；&lt;/li&gt;
  &lt;li&gt;有，则选择；没有，则检查是否通过移动一个元素就能解决；&lt;/li&gt;
  &lt;li&gt;能，就移动，然后选择空出来的位置；不能，则用同样的方法查看Bottom Level；&lt;/li&gt;
  &lt;li&gt;都不能，进行resize操作；&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;选择位置的计算方式:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Lt 1 = hash1 (K)%N, Lt 2 = hash2 (K)%N (1) 

Lb 1 = hash1 (K)%(N/2), Lb 2 = hash2 (K)%(N/2) (2)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;.&lt;/p&gt;

&lt;blockquote&gt;

&lt;/blockquote&gt;

&lt;h3 id=&quot;0x02--cost-efficient-in-place-resizing&quot;&gt;0x02  Cost-efficient In-place Resizing&lt;/h3&gt;

&lt;p&gt;相对于读来说，NVM写的成本比较高，为了减少resize中的写操作，这里一个创新的地方就是每次resize就创建一个新的更加大的Top Level，之前的Top Level成为新的Bottom Level，只需要移动旧的Bottom Level里面的数据就可以了，这样只要移动1/3的数据。当resize是减小size时，反过来就可以了。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/level-hasing-resize.png&quot; alt=&quot;level-hasing-resize&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Resize这里还有2个优化:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;为了解决2个Level数据发布不均导致的影响负载因子的问题，这里使用了一种叫做bottom-to-top movement (B2T) 的方法。具体方法是在插入操作的时候，如果所有的buckets都是满的，那么就尝试将Bottom Level两个候选的位置里面的项尝试移动到Top Level，只有在这些都不能移动的时候才resize。&lt;/p&gt;

    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;By performing the B2T scheme, the items between top and bottom levels are redistributed, thus improving the maximum load factor. The red line in Figure 4 shows the load factors when the resizings occur via using the B2T scheme. 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;在resize之后，由于现在的大部分数据在Bottom Level，而查找的时候总是后查找Bottom Level，这样就导致了性能的降低。这里解决方法是dynamic search scheme，通过Top Level和Bottom Level里面的项的数量来决定先查哪一个。&lt;/p&gt;

    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Thus after resizing, the items in the bottom level are more than those in the top level and hence we first probe the bottom level, thus improving the search performance. 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;

    &lt;p&gt;.&lt;/p&gt;

    &lt;blockquote&gt;

    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;0x03-low-overhead-consistency-guarantee&quot;&gt;0x03 Low-overhead Consistency Guarantee&lt;/h3&gt;

&lt;p&gt;为了标示一个slot是否为空，这里使用了一个标志位.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/level-hashing-bucket.png&quot; alt=&quot;level-hashing-bucket&quot; /&gt;&lt;/p&gt;

&lt;p&gt;为了保证操作的一致性(关于NVM一致性的特点，可用查阅相关资料)，这里提出了一种叫做log-free consistency guarantee schemes和opportunistic log-free guarantee scheme:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;To reduce the overhead of guaranteeing consistency in level hashing, we propose log-free consistency guarantee schemes for deletion, insertion, and resizing operations, and an opportunistic log-free guarantee scheme for update operation, by leveraging the tokens to be performed in the atomic-write manner.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;.&lt;/p&gt;

&lt;h5 id=&quot;log-free-deletion&quot;&gt;Log-free Deletion&lt;/h5&gt;

&lt;p&gt;只需要改变solt的对应的标志位即可；&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; since the item becomes valid until the token is set to ‘1’. If a system failure occurs during writing the item, this item may be partially written but invalid since the current token is ‘0’ and this slot is still available. Hence, the hash table is in a consistent state when system failures occur.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;.&lt;/p&gt;

&lt;h5 id=&quot;log-free-insertion&quot;&gt;Log-free Insertion&lt;/h5&gt;

&lt;p&gt;没有项移动的情况下，先写入数据，然后改变标志位。然后通过MFENCE来保证数据更新到NVM上面了。在有数据项移动的情况下，先拷贝要移动的数据到可选的位置，然后将这个位置(被移动对象目前的位置)的标志位置为 1，然后将原来的位置置为0，然后执行插入操作。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; If a system failure occurs after changing the token of slot-alt before changing the token of slot-cur, the hash table contains two duplicate key-value items, which however does not impact on the data consistency. It is because when searching this key-value item, the returned value is always correct whichever one of the two items is queried.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;.&lt;/p&gt;

&lt;h5 id=&quot;log-free-resizing&quot;&gt;Log-free Resizing&lt;/h5&gt;

&lt;p&gt;也是基于标志位的一个操作:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;we first copy the key-value item of slotold into slotnew, and then modifies the token of slotnew from ‘0’ to ‘1’ and finally modifies the token of slotold from ‘1’ to ‘0’. The ordering of the three steps is ensured via MFENCEs. 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;这里要处理的一个问题就是在一些crash的情况下，可能导致已经rehash的项没有被标记为删除，这里只需要检查是否已经存在相同的项即可。&lt;/p&gt;

&lt;p&gt;.&lt;/p&gt;

&lt;h5 id=&quot;opportunistic-log-free-update&quot;&gt;Opportunistic Log-free Update&lt;/h5&gt;

&lt;p&gt;为了避免使用log保证一致性，这里使用了Opportunistic Log-free Update的方式，具体是:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;一个bucket里面是否存在可使用的空slot，就使用类似先插入，然后同时更改标志位即可，因为同一个bucket里面的标志位在一起，可以一起修改；&lt;/li&gt;
  &lt;li&gt;如果没有，就使用log。&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;When updating an existing key-value item, if the updated item has two copies in the hash table, we first delete one and then update the other.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;.&lt;/p&gt;

&lt;blockquote&gt;

&lt;/blockquote&gt;

&lt;h3 id=&quot;0x04-评估&quot;&gt;0x04 评估&lt;/h3&gt;

&lt;p&gt;详细数据查看论文.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/level-hasing-performance.png&quot; alt=&quot;level-hasing-performance&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;

&lt;/blockquote&gt;

&lt;h2 id=&quot;参考&quot;&gt;参考&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;Write-Optimized and High-Performance Hashing Index Scheme for Persistent Memory, OSDI 2018.&lt;/li&gt;
  &lt;li&gt;A Write-Friendly and Cache-Optimized Hashing Scheme for Non-Volatile Memory Systems，TPDS 2018.&lt;/li&gt;
&lt;/ol&gt;
</description>
                <link>https://www.nagekar.com/2018/10/Level-Hashing.html</link>
                <guid>https://www.nagekar.com/2018/10/Level Hashing</guid>
                <pubDate>Mon, 08 Oct 2018 00:00:00 +0800</pubDate>
        </item>

        <item>
                <title>计算机科学经典论文</title>
                <description>&lt;h2 id=&quot;计算机科学经典论文&quot;&gt;计算机科学经典论文&lt;/h2&gt;

&lt;h3 id=&quot;引言&quot;&gt;引言&lt;/h3&gt;

&lt;p&gt;2018-10-01，国庆节，祝祖国69岁生日快乐。&lt;/p&gt;

&lt;blockquote&gt;

&lt;/blockquote&gt;

&lt;p&gt;计算机科学发展上的经典之作(不是所有方向)。&lt;/p&gt;

&lt;p&gt;逐渐更新，这里是目录…&lt;/p&gt;

&lt;p&gt;还能跟踪一下最新的发展就更加好了。这里大概率会是上个世纪的文章。&lt;/p&gt;

&lt;blockquote&gt;

&lt;/blockquote&gt;

&lt;h3 id=&quot;architecture&quot;&gt;Architecture&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Yeh, T.-Y.; Patt, Y. N. (1991). “Two-Level Adaptive Training Branch Prediction”. Proceedings of the 24th annual international symposium on Microarchitecture. Albuquerque, New Mexico, Puerto Rico: ACM. pp. 51–61.&lt;/p&gt;

    &lt;p&gt;两级自适应分支预测器.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;os&quot;&gt;OS&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;The UNIX Time-Sharing System,The Bell System Technical Journal 57 no. 6, part 2 (July-August 1978)&lt;/p&gt;

    &lt;p&gt;Unix操作系统。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Exokernel: An Operating System Architecture for Application-Level Resource Management,  SIGOPS ’95.&lt;/p&gt;

    &lt;p&gt;Exokernel.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The Slab Allocator: An Object-Caching Kernel Memory Allocator, USENIX SUMMER TECHNICAL CONFERENCE , 1994.&lt;/p&gt;

    &lt;p&gt;Slab分配器.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Algorithms for Scalable Synchronization on Shared-Memory Multiprocessors,ACM Transactions on Computer Systems, Feb. 1991.&lt;/p&gt;

    &lt;p&gt;几种经典的同步方法的设计.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;database&quot;&gt;Database&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Access Path Selection In A Relational Database Management System. SIGMOD 1979.&lt;/p&gt;

    &lt;p&gt; Query Optimizer的基本思路&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ARIES: A Transaction Recovery Method Supporting Fine-Granularity Locking and Partial Rollbacks Using Write-Ahead Logging，ACM Transactions on Database Systems, 17(1), 1992, 94-162.&lt;/p&gt;

    &lt;p&gt;WAL的经典文章.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;An optimistic methods for concurrency control, ACM Transactions on Database Systems, 1981.&lt;/p&gt;

    &lt;p&gt;OCC.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;D.P.Reed. Naming and Synchronization in a Decentralized Computer System. Ph.D. dissertation, 1978.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;MVCC.&lt;/p&gt;

&lt;h3 id=&quot;distributed&quot;&gt;Distributed&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Time, clocks and the ordering of events in a distributed system, L. Lamport, Communications ACM 1978.&lt;/p&gt;

    &lt;p&gt;逻辑时间，顺序.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;network&quot;&gt;Network&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Van Jacobson, Michael J. Karels. Congestion Avoidance and Control.  Sigcomm ‘88. 1988.&lt;/p&gt;

    &lt;p&gt;TCP拥赛控制和避免.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;storage&quot;&gt;Storage&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;A Fast File System for UNIX,ACM Transactions on Computer Systems 1984.&lt;/p&gt;

    &lt;p&gt;Unix FFS.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The Design and Implementation of a Log-Structured File System,ACM Transactions on Computer Systems 1991.&lt;/p&gt;

    &lt;p&gt;Log-Structured文件系统.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Patterson, David; Gibson, Garth A.; Katz, Randy (1988). A Case for Redundant Arrays of Inexpensive Disks (RAID). SIGMOD Conferences.&lt;/p&gt;

    &lt;p&gt;RAID.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;algorithm&quot;&gt;Algorithm&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Bayer, Rudolf (1971), Binary B-Trees for Virtual Memory.&lt;/p&gt;

    &lt;p&gt;B-tree.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Linear scan register allocation, ACM Transactions on Programming Languages and Systems, 1999.&lt;/p&gt;

    &lt;p&gt;线性扫描寄存器分配算法.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;参考&quot;&gt;参考&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;http://www.redbook.io/ch3-techniques.html，Red Book 5th Chapter 3.&lt;/li&gt;
&lt;/ol&gt;

</description>
                <link>https://www.nagekar.com/2018/10/Classical-Papers.html</link>
                <guid>https://www.nagekar.com/2018/10/Classical Papers</guid>
                <pubDate>Mon, 01 Oct 2018 00:00:00 +0800</pubDate>
        </item>

        <item>
                <title>Optimizing the Block IO Subsystem for Fast Storage Devices</title>
                <description>&lt;h2 id=&quot;optimizing-the-block-io-subsystem-for-fast-storage-devices&quot;&gt;Optimizing the Block I/O Subsystem for Fast Storage Devices&lt;/h2&gt;

&lt;h3 id=&quot;0x00-引言&quot;&gt;0x00 引言&lt;/h3&gt;

&lt;p&gt;这篇Paper讨论的是如何优化Linux的Block I/O Subsystem以使用新的高速的硬件，这里面主要提出了6条改进方案，这些措施有些之间是相互不兼容的，也就是说不能同时使用。此外，这里讨论的是如何对内核做更改来优化，而不是在现在的内核上调整参数设置来优化性能:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;In this article, we explore six optimizations for the block I/O subsystem: polling I/O completion, eliminating context switches from the I/O path, merging discontiguous requests, reconfiguring an I/O scheduler for an SSD, resolving the read-ahead dilema, and avoiding a lock contention in a request queue. 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;另外，这篇Paper长达48页，所以这里这会总结改进方案的基本原理以及能获取到的好处，和存在的缺点，不具体讨论细节。&lt;/p&gt;

&lt;p&gt;Linux内核基本的Block IO处理示意图:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/optimizing-io-iopath.png&quot; alt=&quot;optimizing-io-iopath&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;0x02-目前存在的问题&quot;&gt;0x02 目前存在的问题&lt;/h3&gt;

&lt;p&gt;这些问题是针对高速存储硬件而言的，对于一些较低速的硬件如HHD，可能就不是问题。&lt;/p&gt;

&lt;h5 id=&quot;high-software-latency-高软件延时&quot;&gt;High Software Latency 高软件延时&lt;/h5&gt;

&lt;p&gt;来源：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;中断，目前一般使用中断的方式通知IO相关操作的消息；&lt;/li&gt;
  &lt;li&gt;Delayed Execution，延迟执行，对HHD中的常见优化；&lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;low-random-throughput低随机操作的带宽&quot;&gt;Low Random Throughput，低随机操作的带宽&lt;/h5&gt;

&lt;p&gt;来源：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Narrow Device I/O Interface，狭隘的设备IO接口，只留出的操作的余地很小；&lt;/li&gt;
  &lt;li&gt;Disk-Oriented Configuration，主要是面向HHD的一些设计，比如CFQ IO调度器，粗粒度的timer，预读确实高成本；&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;0x03-优化的基本的方法和性能相关值的度量&quot;&gt;0x03 优化的基本的方法和性能相关值的度量&lt;/h3&gt;

&lt;p&gt;对于优化基本的方向就是两个：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Reducing per-request latency，减少每个请求的延时；&lt;/li&gt;
  &lt;li&gt;Amortizing per-request latency，平摊延时；&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;性能相关的值：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Device bandwidth，设备带宽，这个是有设备本身决定的；&lt;/li&gt;
  &lt;li&gt;Application throughput，应用带宽，应用能使用的带宽；&lt;/li&gt;
  &lt;li&gt;Hardware latency，硬件延时，这个也是由设备本身决定的；&lt;/li&gt;
  &lt;li&gt;Software latency，软件延时，从发出IO请求到得到IO请求完成通知的时间。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;优化的6个基本方法:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;The optimizations are summarized in the following:
O1. using polling instead of interrupts;
O2. establishing a synchronous I/O path;
O3. dispatching discontiguous requests through an extended I/O interface;
O4. adjusting block I/O subsystem configurations to an SSD;
O5. avoiding harmful block prefetching; and
O6. using double buffering for I/O requests.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;下面就是讨论了一些具体的方法，这里是从一个方法出发解决现有的问题，但方法本身也带来了额外的问题，然后提出改进策略，直到达到一个较好的结果。&lt;/p&gt;

&lt;blockquote&gt;

&lt;/blockquote&gt;

&lt;h3 id=&quot;0x04-syncpath-designing-a-synchronous-io-path&quot;&gt;0x04 SyncPath: Designing a Synchronous I/O Path&lt;/h3&gt;

&lt;p&gt;SyncPath设计为block layer的一个子系统，对应优化方法中的O1 O2。分为这些步骤处理IO请求:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;预先为数据传输准备DMA缓冲区；&lt;/li&gt;
  &lt;li&gt;请求设备的互斥锁；&lt;/li&gt;
  &lt;li&gt;初始化DMA传输，然后轮询请求完全情况；&lt;/li&gt;
  &lt;li&gt;完成请求之后释放资源；&lt;/li&gt;
  &lt;li&gt;通知用户进程IO请求完成；&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;这里直接就绕开了现在处理方式。对于提高随机读写和顺序读以及随机的混合操作有正向的效果，对于顺序写由反向的效果。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;O1 and O2 directly lead to performance improvement under several workloads. SyncPath attains a 1.58 ∼ 2.47 times improvement over SCSI INTR under the random read, the random write, and the random mixed workload.
...
On the contrary, the sequential write throughput drops from 479MB/s to 309MB/s, reaching the random write throughput. As SyncPath cannot take advantage of an I/O scheduler to merge contiguous write requests, the sequential write workload is handled in the same way as the random write workload.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;.&lt;/p&gt;

&lt;blockquote&gt;

&lt;/blockquote&gt;

&lt;h3 id=&quot;0x05--extending-device-io-interface&quot;&gt;0x05  Extending Device I/O Interface&lt;/h3&gt;

&lt;p&gt;SyncPath中表现出了没有合并写请求对顺序写性能的影响。现在Scatter-gather DMA I/O是一种常见的优化方式，这里提出了一种新的device IO接口，Scatter-Scatter I/O (SSIO) Interface。用于将不连续的主机内存里面的数据写入到存储设备不连续的段里面，反之同理。这利用了SSD内部的并行特性，给合并IO请求提供了额外的机会。这种方式也要求对DMA做出一些改进。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; The DMA engine of the DRAM-based SSD is customized accordingly by using a set of descriptors for an I/O request. A request descriptor represents a single mapping of {host memory segment, storage address segment, data size}. The Block Control Table (BCT) maintains 1,024 request descriptors, implying that the block I/O subsystem can dispatch up to 1,024 I/O requests at a time through the SSIO interface.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;.&lt;/p&gt;

&lt;blockquote&gt;

&lt;/blockquote&gt;

&lt;h3 id=&quot;0x06-stm-synchronously-merging-discontiguous-io-requests&quot;&gt;0x06 STM: Synchronously Merging Discontiguous I/O Requests&lt;/h3&gt;

&lt;p&gt;SSIO利用的是SSD的性能特点，不同于现在的Spatial Merge在空间的局部性上做合并操作，SSIO是在在时间上的局部性做合并的操作。这里就定义了Temporal Merge，即在一个时间窗口内的操作被合并到一起。利用这个特点，这里就实现了Synchronous Temporal Merge。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Figure 7 depicts the difference between Spatial Merge and Temporal Merge. When 5 contiguous and 3 discontiguous I/O requests enter the block I/O subsystem, Spatial Merge would combine them into one large I/O request and three small I/O requests, while Temporal Merge would build one I/O request with 8 request descriptors.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/optimizeing-blk-tm.png&quot; alt=&quot;optimizeing-blk-tm&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Synchronous Temporal Merge基于上面的O1 O2和O3。当一些IO请求并发地到达的时候，其中的一个被挑选为Winner，其它的请求则跟随Winner的操作路径。步骤如下：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;(1) Choose a Winner among the CPU contexts by using an atomic operation.  

(2) (Losers) Add an I/O request to the Winner’s queue and sleep in a task queue. 

(3) (Winner) Perform Temporal Merge on the I/O requests in the queue. 

(4)  (Winner) Prepare DMA buffers, acquires an exclusive lock for a device, and dispatch the merged request through the SSIO interface. 

(5)  (Winner) Initiate DMA transfer and poll on the completion of the merged I/O request. 

(6)  (Winner) Release the resources assigned to all the requests in the merged I/O request. 

(7)  (Winner) Update flag variables to notify the Losers of the I/O completions. 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;利用时间上的局部性很好的提高了顺序写入的性能，解决了之前SyncPath存在的问题的同时了也利用好了SyncPath的优点。&lt;/p&gt;

&lt;p&gt;不过STM方式也存在缺点，一个是时间合并的在IO请求较少时难以发挥。第二个是随机读的性能明显低于随机写。&lt;/p&gt;

&lt;blockquote&gt;

&lt;/blockquote&gt;

&lt;h3 id=&quot;0x07-atm-asynchronously-merging-discontiguous-io-requests&quot;&gt;0x07 ATM: Asynchronously Merging Discontiguous I/O Requests&lt;/h3&gt;

&lt;p&gt;Asynchronous Temporal Merge可以看作是STM的一个改进版本，ATM基于上面的O1 O2和O4。不同于STM，它使用请求队列阻塞机制来积累IO请求。在将IO请求插入到IO调度队列之前，先将IO请求的kernel buffer映射到DMA buffer，这一步叫做queue bouncing。在收到一个unplugging(疏通)事件时，先Temporal Merge，然后一次性地将这些请求发送出去。在检测到IO操作完成的时候，使用每个CPU的软件中断，让之前的阻塞的CPU核心参与之后的工作。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/optimizing-io-atm.png&quot; alt=&quot;optimizing-io-atm&quot; /&gt;&lt;/p&gt;

&lt;p&gt;这样是不够的，还需要修改IO调度器的配置，默认的CFQ不能适应这里的情况，这里修改为NOOP之后才能发挥出来。这里涉及到2个参数的设置：unplug thresh 和 unplug delay。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ATM performs better than STM under write workloads; the improvements are 18% and 5% under the sequential write and the random write workload, respectively.
... 
On the other hand, the sequential and the random read throughput by ATM are lower than those by STM, which is consistently observed across other Iozone settings. The reason is that ATM was unable to merge read requests because of the critical section design in the Linux storage stack; 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;ATM表现出比STM更加好的写性能，然而这里的读性能低于STM，原因在与ATM很好地合并读请求。&lt;/p&gt;

&lt;blockquote&gt;

&lt;/blockquote&gt;

&lt;h3 id=&quot;0x08-htm-hybrid-use-of-synchronous-and-asynchronous-temporal-merge&quot;&gt;0x08 HTM: Hybrid Use of Synchronous and Asynchronous Temporal Merge&lt;/h3&gt;

&lt;p&gt;接下来的这种方式就是将STM和ATM组合，形成了HTM(Hybrid Temporal Merge)。STM和ATM分别对读和写友好，HTM这里就将latency-sensitive分配给STM处理， throughput-sensitive交给ATM处理。一般而言，将读请求定义为latency-sensitive，此外，将direct I/O请求也定义为latency-sensitive(不论是读还是写)。其它的就定义为throughput-sensitive。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;HTM achieves 100% of the device bandwidth under the sequential read and the sequential write workload, and 95% under the random write workload. Interestingly, HTM performs worse than both STM and ATM when read and write requests are mixed. Only 43% of the device bandwidth is exploited by HTM, which is comparable to the random read throughput. 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;HTM对顺序读写、随机写有着非常好的性能。但是对于随机读和混合读写，性能比较差。这里认为主要是ATM和STM之间的相互干扰造成的。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/optimizing-io-htm.png&quot; alt=&quot;optimizing-io-htm&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;0x09-vfs-htm-integrating-the-vfs-layer-with-htm&quot;&gt;0x09 VFS-HTM: Integrating the VFS Layer with HTM&lt;/h3&gt;

&lt;p&gt;在分析了上面随机读和混合读写的性能问题之后，发现问题出在Linux的VFS layer，主要原因在与不合适的预读区处理。将ra_pages(Linux 预读相关的数值,可参考相关资料)设置为0之后，产生的效果就是随机读取性能提高的，但是顺序读取性能下降了。&lt;/p&gt;

&lt;p&gt;为了解决这个问题，这里就将VFS layer和 block I/O子系统集成，用一些策略去除不必要的操作。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;It clones the I/O path in the VFS layer and directs a system call to execute the new I/O path, not requiring any modification to an OS. VFS-HTM disables the context lookup feature because block prefetching for a single-threaded sequential read workload is sufficient; even without block prefetching, Temporal Merge can build a large I/O request.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;这样就很好地解决了之前的问题。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/optimizing-io-vfs.png&quot; alt=&quot;optimizing-io-vfs&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;0x0a-sq-using-double-buffering-to-avoid-lock-contention&quot;&gt;0x0A SQ: Using Double Buffering to Avoid Lock Contention&lt;/h3&gt;

&lt;p&gt;这里要解决的就是对请求队列上的竞争问题，这里加入了另外的一个队列，叫做Shadow Queue (SQ)。功能和使用方法类似于Double Buffering(所以标题就是Using Double Buffering to Avoid Lock Contention)。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;The requests in the shadow queue are moved into a request queue only by a draining event. This enables the requests to remain in the request queue even when an unplugging event is triggered by the insertion of a read request. Consequently, the unplugging event ends up releasing the queuelock within 1∼2 microseconds.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;这样的操作带来的优点就是随机读和混合读写的性能有所提高，但是降低了顺序写的性能。原因与文件系统(这里使用的是ext3)的日志相关。在不支持日志的文件系统如ext2上就没有见到这种情况。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;In the case of ext3, the synchronous writes issued by a jour- naling thread prevent next requests from entering SQ, reducing the concurrency from which Temporal Merge benefits. Identifying such synchronous requests submitted by different file systems and minimizing the software latency of these is a remaining challenge in SQ.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;这个问题没有完全解决。&lt;/p&gt;

&lt;blockquote&gt;

&lt;/blockquote&gt;

&lt;h3 id=&quot;0x0b-评估&quot;&gt;0x0B 评估&lt;/h3&gt;

&lt;p&gt;总体性能:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/optimizing-io-performance.png&quot; alt=&quot;optimizing-io-performance&quot; /&gt;&lt;/p&gt;

&lt;p&gt;论文中还有一大堆的其它的内容。&lt;/p&gt;

&lt;h2 id=&quot;参考&quot;&gt;参考&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;Yu, Y. J., Shin, D. I., Shin, W., Song, N. Y., Choi, J. W., Kim, H. S., Eom, H., Yeom, H. Y. 2014. Optimizing the block I/O subsystem for fast storage devices. ACM Trans. Comput. Syst. 32, 2, Article 6 (June 2014), 48 pages.  DOI:http://dx.doi.org/10.1145/2619092.&lt;/li&gt;
&lt;/ol&gt;

</description>
                <link>https://www.nagekar.com/2018/09/Optimizing-the-Block-IO-Subsystem-for-Fast-Storage-Devices.html</link>
                <guid>https://www.nagekar.com/2018/09/Optimizing the Block IO Subsystem for Fast Storage Devices</guid>
                <pubDate>Tue, 25 Sep 2018 00:00:00 +0800</pubDate>
        </item>

        <item>
                <title>PolarFS</title>
                <description>&lt;h2 id=&quot;polarfs-an-ultra-low-latency-and-failure-resilient-distributed-file-system-for-shared-storage-cloud-database&quot;&gt;PolarFS: An Ultra-low Latency and Failure Resilient Distributed File System for Shared Storage Cloud Database&lt;/h2&gt;

&lt;h3 id=&quot;0x00-引言&quot;&gt;0x00 引言&lt;/h3&gt;

&lt;p&gt;PolarFS是阿里巴巴为它推出的PolarDB设计的文件系统，作为控制面和数据面分离中的一部分。PolarFS在使用新技术时还是挺激进的，在一个商业产品上面一下子就搞这么多东西，将 network stack, IO stack都做到了user-space，使用3D XPoint ，RDMA, NVMe SSD, and SPDK 等新的技术。&lt;/p&gt;

&lt;h3 id=&quot;0x01-基本架构&quot;&gt;0x01 基本架构&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/polarfs-arch.png&quot; alt=&quot;polarfs-arch&quot; /&gt;&lt;/p&gt;

&lt;p&gt;PolarFS主要分为2层，Storage Layer负责管理storage nodes的磁盘资源， 为每个数据库实例提供数据卷(volume)。File system layer 支持在这些volume上实现文件管理，同时还负责访问文件元数据时的互斥和同步。Polar的组件分为一下几个部分:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;libpfs，一个实现在用户空间的类似POSIX的文件系统接口；&lt;/li&gt;
  &lt;li&gt;Polar-Switch，运行在计算结点上，将IO请求发送给ChunkServer；&lt;/li&gt;
  &lt;li&gt;ChunkServers，允许在storage结点上，处理IO请求；&lt;/li&gt;
  &lt;li&gt;PolarCtrl，PolarCtrl 时控制面，有一组master组成，同时在每一个storage结点上部署了一个agent；&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;0x02-组件&quot;&gt;0x02 组件&lt;/h3&gt;

&lt;h4 id=&quot;file-system-layer&quot;&gt;File System Layer&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/polarfs-pfs-api.png&quot; alt=&quot;polarfs-pfs-api&quot; /&gt;&lt;/p&gt;

&lt;p&gt;这些接口以一个libpfs 库提供给使用者。&lt;/p&gt;

&lt;blockquote&gt;

&lt;/blockquote&gt;

&lt;h4 id=&quot;storage-layer&quot;&gt;Storage Layer&lt;/h4&gt;

&lt;p&gt;在PolarDB中一个数据库实例的大小为10GB – 100TB，这个被称为一个volmue。一个volmue被划分为chunk，发布在一组ChunkServers 上面，一个chunk只会保存在一个磁盘上面，一个chunk会被复制3份，保存在不同的ChunkServer上面。ChunkServer会使用迁移数据的形式解决一些热点的问题。&lt;/p&gt;

&lt;p&gt;Chunk的大小被设置为了10GB，这么尺寸的Chunk显著的减小了元数据的大小，有利于其它组件处理(比如PolarSwitch)。一个chunk会被继续分为block，一个block的大小为64KB。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;A 10 GB chunk contains 163,840 data blocks. The mapping table of chunk’s LBA (Logical Block Address, the linear address range from 0 to 10 GB) to blocks are stored locally in ChunkServer, together with the bitmap of free blocks on each disk. The mapping table of a single chunk occupies 640 KB memory, which is quite small and can be cached in memory of the ChunkServer.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;.&lt;/p&gt;

&lt;h5 id=&quot;polarswitch&quot;&gt;PolarSwitch&lt;/h5&gt;

&lt;p&gt;PolarSwitch作为一个守护进程，责任就是根据数据库的IO请求，将这些请求发送给合适的ChunkServer。对于可能跨越多个chunk的请求，需要将其拆分为sub-requests。PolarSwitch通过PalrCtrl获取chunk的一些信息。所有的数据副本中，只有leader才会处理IO请求。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Leadership changes in the consensus group are also synchronized and cached in PolarSwitch’s local cache. If response timeout happens, PolarSwitch would keep retrying with exponential backoff while detecting whether leader election happens, switch to new leader and retransmit immediately if it does.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;.&lt;/p&gt;

&lt;h5 id=&quot;chunkserver&quot;&gt;ChunkServer&lt;/h5&gt;

&lt;p&gt;ChunkServer 使用了NVMe SSD 来保存数据，负责chunk的读写等的功能，同时使用为了保证原子性和持久性，使用了WAL，这里PolarFS就直接使用了非易失性内存来加速WAL:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ChunkServer uses a piece of fixed size 3D XPoint SSD buffer as a write cache for WAL, logs are preferred to be placed in 3D XPoint buffer. If the buffer is full, ChunkServer will try to recycle dated logs. If there is still not enough space in 3D XPoint buffer, logs are written into NVMe SSD instead. Chunk blocks are always written into NVMe SSD.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;ChunkServers会将这些IO请求通过使用ParallelRaft 来复制到一个 consensus group。&lt;/p&gt;

&lt;blockquote&gt;

&lt;/blockquote&gt;

&lt;h5 id=&quot;polarctrl&quot;&gt;PolarCtrl&lt;/h5&gt;

&lt;p&gt;PolarCtrl为PolarFS的控制面，它负责结点管理, 卷管理,资源分配, 元数据同步,监控等等一大堆的功能。此外，还最近ChunkServer的负载情况，必要的时候迁移数据来尝试解决负载的问题。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;As a control plane, PolarCtrl is not on the critical I/O path, its service continuity can be provided using traditional high availability techniques. Even during the short interval between PolarCtrl’s crash and recovery, I/O flows in PolarFS would not likely be affected due to the cached meta- data on PolarSwith and the self-management of ChunkServer.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;.&lt;/p&gt;

&lt;blockquote&gt;

&lt;/blockquote&gt;

&lt;h3 id=&quot;0x03-io-execution-model&quot;&gt;0x03 I/O Execution Model&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/polarfs-io-execution.png&quot; alt=&quot;polarfs-io-execution&quot; /&gt;&lt;/p&gt;

&lt;p&gt;上面的图基本表示了PolarFS的写操作的流程。为了实现超低的延时和高的吞吐，这里直接使用了RDMA。RDMA有两类操作，Send/Recv 和 Read/Write，各自有不同的特点(可参考相关论文)，已经有不少的key-value系统使用RDMA，并探究了Send/Recv 和 Read/Write对这些系统的性能影响。这里PolarFS混合使用了Send/Recv 和 Read/Write，对于小的请求，直接使用Send/Recv，对于大的数据的请求，先使用Send/Recv同步一些元数据，然后使用Read/Write来传输数据。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;PolarFS uses a hybrid of Send/Recv and Read/Write verbs. Small payloads are transferred by Send/Recv verbs directly. For a large chunk of data or a batch of data, nodes negotiate about the destination memory address on the remote node using Send/Recv verbs, and then complete the actual data transmission over Read/Write verbs. PolarFS elimi- nates context switches by polling the CQ in user space instead of relying on interrupts.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;写操作步骤分析:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;POLARDB将一个写操作的IO请求发送给PolarSwitch和libpfs的环形缓冲区；&lt;/li&gt;
  &lt;li&gt;PolarSwitch发现这些亲戚之后，将这些请求发送给对应的chunk’s leader 结点；&lt;/li&gt;
  &lt;li&gt;RDMA NIC在接受到一个IO请求的时候，将这些请求放入预先准备的一个buffer中，然后想请求队列里面添加一项；一个IO轮询线程发现了这些请求队列里面的项之后，立即开始处理这个请求；&lt;/li&gt;
  &lt;li&gt;请求被写入到通过使用SPDK写入到log里面，然后通过RDMA将发送给副本。这两部都是异步操作，真正的数据传输时同时进行的；&lt;/li&gt;
  &lt;li&gt;副本接受到这些请求之后，也重复类似leader的动作，最后也将这些请求放到了一个请求队列里面；&lt;/li&gt;
  &lt;li&gt;副本上面的IO轮询发现了请求之后，就执行写操作；&lt;/li&gt;
  &lt;li&gt;副本写操作完成之后，通过一个回调通知leader；&lt;/li&gt;
  &lt;li&gt;leader在收到了超过一半的回复之后，leader执行写操作；&lt;/li&gt;
  &lt;li&gt;leader写操作完成之后，通过RDMA通知PolarSwitch写操作完成；&lt;/li&gt;
  &lt;li&gt;PolarSwitch记录这个写操作已经完成，同时通知client(这里就是数据库).&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;读操作就简单多了，leader结点处理就行。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Read I/O requests are (more simply) processed by leader alone. In ChunkServer there is a submodule named IoScheduler which is responsible to arbitrate the order of disk I/O operations issued by concurrent I/O requests to execute on the ChunkServer. IoScheduler guarantees that a read operation can always retrieve the latest committed data.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;.&lt;/p&gt;

&lt;blockquote&gt;

&lt;/blockquote&gt;

&lt;h3 id=&quot;0x04-consistency-model&quot;&gt;0x04 Consistency Model&lt;/h3&gt;

&lt;p&gt;Paper这一个部分将的是关于ParallelRaft的一些问题，这里就不具体讨论这个了，只关心FS本身。&lt;/p&gt;

&lt;blockquote&gt;

&lt;/blockquote&gt;

&lt;h3 id=&quot;0x05-fs-layer-implementation&quot;&gt;0x05 FS Layer Implementation&lt;/h3&gt;

&lt;p&gt;这里讨论的就是如何实现类似文件系统结构的功能：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/polarfs-fs-layer.png&quot; alt=&quot;polarfs-fs-layer&quot; /&gt;&lt;/p&gt;

&lt;p&gt;具体如何实现的参见论文。&lt;/p&gt;

&lt;blockquote&gt;

&lt;/blockquote&gt;

&lt;h3 id=&quot;0x06-评估&quot;&gt;0x06 评估&lt;/h3&gt;

&lt;p&gt;PolarFS虽然说时一个分布式的文件系统，不过由于使用了RDMA，估计还是只能在一个数据中心内部的分布，与类似GFS的跨地域分布不同。不够他们面向的对象也不一样。&lt;/p&gt;

&lt;p&gt;文章还提了利用RDMA加速Paxos之类算法的研究，比如APUS，估计是想反正现在都用上RDMA，不如利用它再来加速一下子？？瞎猜。&lt;/p&gt;

&lt;blockquote&gt;

&lt;/blockquote&gt;

&lt;p&gt;延时:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/polarfs-latency.png&quot; alt=&quot;polarfs-latency&quot; /&gt;&lt;/p&gt;

&lt;p&gt;吞吐:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/polarfs-throughput.png&quot; alt=&quot;polarfs-throughput&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;参考&quot;&gt;参考&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;Wei Cao, Zhenjun Liu, Peng Wang, Sen Chen, Caifeng Zhu, Song Zheng, Yuhui Wang, Guoqing Ma. PolarFS: An Ultra-low La- tency and Failure Resilient Distributed File System for Shared Storage Cloud Database. PVLDB, 11 (12): 1849 - 1862, 2018. DOI: https://doi.org/10.14778/3229863.3229872&lt;/li&gt;
&lt;/ol&gt;

</description>
                <link>https://www.nagekar.com/2018/09/PolarFS.html</link>
                <guid>https://www.nagekar.com/2018/09/PolarFS</guid>
                <pubDate>Sat, 22 Sep 2018 00:00:00 +0800</pubDate>
        </item>

        <item>
                <title>A Receiver-Driven Low-Latency Transport Protocol</title>
                <description>&lt;h2 id=&quot;homa--a-receiver-driven-low-latency-transport-protocol-using-network-priorities&quot;&gt;Homa – A Receiver-Driven Low-Latency Transport Protocol Using Network Priorities&lt;/h2&gt;

&lt;h2 id=&quot;0x00-引言&quot;&gt;0x00 引言&lt;/h2&gt;

&lt;p&gt;最近几年为数据中心设计的新的传输协议不少，这篇是SIGCOMM上最新的一篇(截止写这篇论文时)，总而言之，这篇论文做到了:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;In simulations, Homa’s latency is roughly equal to pFabric and significantly better than pHost, PIAS, and NDP for almost all message sizes and workloads. Homa can also sustain higher network loads than pFabric, pHost, or PIAS.

-----

Our implementation of Homa achieves 99th percentile round trip latencies less than 15 μs for small messages at 80% network load with 10 Gbps link speeds, and it does this even in the presence of competing large messages. Across a wide range of message sizes and work- loads, Homa achieves 99th percentile latencies at 80% network load that are within a factor of 2–3.5x of the minimum possible latency on an unloaded network. 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Homa倾向于为short messages设计。&lt;/p&gt;

&lt;h3 id=&quot;0x01-key-ideas&quot;&gt;0x01 KEY IDEAS&lt;/h3&gt;

&lt;p&gt;Homa的设计有4个key design principles ：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;(i) transmitting short messages blindly;

(ii) using in-network priorities;

(iii) allocating priorities dynamically at receivers in conjunction with receiver-driven rate control;

(iv) controlled overcommitment of receiver downlinks.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Homa这么设计出于一下的考虑：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;1. There is no time to schedule every packet. 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;特别在意延时的情况下，schedule带来的延时都不可以接受。所以有了principle 1。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;2. Buffering is a necessary evil.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;没有一个protocol在没有导致buffering的同时实现low latency，讽刺的是，buffering又会带来latency。buffer，latency，throughput，欢喜冤家，emmmmmmm这里是不是可以写一本书了。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;3. In-network priorities are a must. 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;由于上一条，为了减少延时，不同的packet区分处理能获得一些效果。这个方法在很多类似的protocol中都有使用。这样就有了principle 2.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;4. Making best use of limited priorities requires receiver control.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;简而言之就是recevier控制更加好。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;5. Receivers must allocate priorities dynamically.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Homa 使用Receivers动态分配优先级的方式解决了之前类似协议的一些问题(pHost )，比如large的messag使用高优先级带来的问题，只使用一种优先级可能导致的delay。这样就有了principl 3。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;6. Receivers must overcommit their downlink in a controlled manner.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;为了解决一些情况下链路利用率低的问题，比如一个sender项多个recevier发送数据(注意Homa使用的是receiver控制的传输方式)。为了解决这个问题，一个receiver可以过量使用downlink，比如同时给几个sender发送可以向receiver发送数据的grants。这样可能造成packet queuing ，但是对于提高利用率来说是必要的。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;7. Senders need SRPT(shortest remaining processing time first) also. 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;排队也可能在sender端出现，Sender知道SRPT能跟好的解决这些问题。&lt;/p&gt;

&lt;h3 id=&quot;0x02-基本设计&quot;&gt;0x02 基本设计&lt;/h3&gt;

&lt;p&gt;先来一张论文中的图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/homa-arch.png&quot; alt=&quot;homa-arch&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Homa有以下特点：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Homa contains several unusual features: 
it is receiver-driven; 
it is message-oriented, rather than stream-oriented; 
it is connectionless; 
it uses no explicit acknowledgments; 
and it implements at-least-once semantics, rather than the more traditional at-most-once semantics.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;rpcs-not-connections&quot;&gt;RPCs, not connections&lt;/h4&gt;

&lt;p&gt;Homa是无连接的，一个来讲client的request message 对应一个来自server的 response message。由一个全局唯一的RPCid表示(id由客户端生成)。有以下的packet类型：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/homa-packet-types.png&quot; alt=&quot;homa-packet-types&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;basic-sender-behavior&quot;&gt;Basic sender behavior&lt;/h4&gt;

&lt;p&gt;Homa将Message分为两部分，一部分是unscheduled 的部分，这部分是立即会被发送的，可能有一个or多个packet。第二个部分是unscheduled 的部分，这部分只有在收到receiver的GRANT 包之后才能发送。每一个DATA 包都有一个优先级。这个优先级由receiver决定。&lt;/p&gt;

&lt;p&gt;Sender也使用了SRPT，当多个message的DATA包同时准备好发送的时候，决定先发送的是有最少剩余数据的message的包。sender 不根据优先级决定那些包先发送(由此可以看出，优先级只对路由器产生作用)。此外，控制类型的包，如GRANT and RESEND包的优先级总是高于DATA包。&lt;/p&gt;

&lt;h4 id=&quot;packet-priorities&quot;&gt;Packet priorities&lt;/h4&gt;

&lt;p&gt;从论文中的描述看，优先级的部分是这个transport protocol的核心的一个部分：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;The most novel feature in Homa, and the key to its performance, is its use of priorities. Each receiver determines the priorities for all of its incoming DATA packets in order to approximate the SRPT policy. It uses different mechanisms for unscheduled and scheduled packets. 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;对于unscheduled 的包，receiver提前根据最近的流量模式决定优先级，然后会将顺带地这些信息发送给sender(by piggybacking it on other packets )，sender会报送最近的每一个receiver的相关信息。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/homa-unscheduled.png&quot; alt=&quot;homa-unscheduled&quot; /&gt;&lt;/p&gt;

&lt;p&gt;对于scheduled 的包，receiver在GRANT包中给其分配优先级，sender使用这个优先级发送DATA包。这样的好处就是可以实时地适应目前的状况。这些主要是根据SRPT 。&lt;/p&gt;

&lt;h4 id=&quot;lost-packets&quot;&gt;Lost packets&lt;/h4&gt;

&lt;p&gt;Homa认为包丢失是概率很小的事件。主要关注2种类型的包丢失：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;corruption in the network, and buffer overflow
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Homa中，由receiver发现包丢失(这个与TCP完全不相同，我们可以发送，Homa这个协议将很多东西都放到了receiver这边，这种设计目前看来适应datacenter这样的环境还是很不错的)。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; Receivers use a simple timeout-based mechanism to detect lost packets. If a long time period (a few milliseconds) elapses without additional packets arriving for a message, the receiver sends a RESEND packet that identifies the first range of missing bytes; the sender will then retransmit those bytes.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;at-least-once-semantics&quot;&gt;At-least-once semantics&lt;/h4&gt;

&lt;p&gt;Homa实现的语义是至少一次，我们知道至少一次的基本套路就是一直重试到收到确认为止。Homa又是为RPC设计的，那Homa是如何处理被多次执行的呢？答案很简单粗暴：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Homa assumes that higher level software will either tolerate redundant executions of RPCs or filter them out.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;说白了就是不管，有Application处理。&lt;/p&gt;

&lt;h4 id=&quot;其它一些东西&quot;&gt;其它一些东西&lt;/h4&gt;

&lt;p&gt;另外paper中关于 Flow control，Overcommitment ，Incast 的部分可以参看原论文。&lt;/p&gt;

&lt;h3 id=&quot;0x03-limitations&quot;&gt;0x03 LIMITATIONS&lt;/h3&gt;

&lt;p&gt;Paper还很罕见的讨论了这个协议的缺点(暴露自己缺点的paper很少见啊)，我们从之前的内容中也可以发现，Homa协议是建立在很多的假设上的，这也让我对其实际的可用性造成了很大的怀疑:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  Homa is designed for use in datacenter networks and capital- izes on the properties of those networks; it is unlikely to work well in wide-area networks.
  Homa assumes that congestion occurs primarily at host down- links, not in the core of the network. Homa assumes per-packet spraying to ensure load balancing across core links, combined with sufficient overall capacity. .... We hypothesize that congestion in the core of datacenter networks will be uncommon because it will not be cost-effective. ... 
  ...
  Homa also assumes a single implementation of the protocol for each host-TOR link, such as in an operating system kernel running on bare hardware, so that Homa is aware of all incoming and outgoing traffic.
  ...
  Homa assumes that the most severe forms of incast are pre- dictable because they are self-inflicted by outgoing RPCs;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;一堆的假设，看不下去了。。。。。&lt;/p&gt;

&lt;h2 id=&quot;参考&quot;&gt;参考&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;Homa: A Receiver-Driven Low-Latency Transport Protocol Using Network Priorities, SIGCOMM 2018;&lt;/li&gt;
&lt;/ol&gt;
</description>
                <link>https://www.nagekar.com/2018/09/Homa-Transport-Protocol.html</link>
                <guid>https://www.nagekar.com/2018/09/Homa Transport Protocol</guid>
                <pubDate>Thu, 20 Sep 2018 00:00:00 +0800</pubDate>
        </item>

        <item>
                <title>Datacenter TCP, Deadline Driven Delivery and Deadline-aware Datacenter TCP</title>
                <description>&lt;h2 id=&quot;datacenter-tcp-deadline-driven-delivery-and-deadline-aware-datacenter-tcp&quot;&gt;Datacenter TCP, Deadline Driven Delivery and Deadline-aware Datacenter TCP&lt;/h2&gt;

&lt;h3 id=&quot;0x00-引言&quot;&gt;0x00 引言&lt;/h3&gt;

&lt;p&gt;这篇总结包含了3篇Paper的内容，一篇是SIGCOMM 2010上的DCTCP，一篇是SIGCOMM 2011上的Deadline Driven Delivery，还有一篇是SIGCOMM 2012上面的D2TCP。前者将的是如何利用&lt;strong&gt;Explicit Congestion Notification&lt;/strong&gt; (&lt;strong&gt;ECN&lt;/strong&gt;)解决数据中心网络中TCP的一些问题，第二个是如何加入deadline的优化，后者是前2者的优化。&lt;/p&gt;

&lt;p&gt;这里只是简单地介绍。&lt;/p&gt;

&lt;h3 id=&quot;0x01-ecn&quot;&gt;0x01 ECN&lt;/h3&gt;

&lt;p&gt;ECN就是显示的拥塞通知。对于IPv4，它使用了DiffServ字段最右边的两个bits来标示(在一些早一点的书上，可以发现说这里是预留给以后的功能的，目前没有使用，当然现在是已经使用了)，在IPv6上Traffic Class字段的最后两个bits。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/dctcp-header.png&quot; alt=&quot;dctcp-header&quot; /&gt;&lt;/p&gt;

&lt;p&gt;​    这里只给出了IPv4的header，图片来源于维基百科。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;00&lt;/code&gt; – 不支持ECN；&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;10&lt;/code&gt; – 支持ECN；&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;01&lt;/code&gt; – 支持ECN，和上面相同；&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;11&lt;/code&gt; –遇到了阻塞；&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;ECN还有更多的细节，可参考相关资料。&lt;/p&gt;

&lt;p&gt;.&lt;/p&gt;

&lt;h3 id=&quot;0x02-问题&quot;&gt;0x02 问题&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;incast 问题，在Partition/Aggregate模式中比较常见，服务器同时回复请求端导致某个地方的包突然大量增加，从而导致丢包；&lt;/li&gt;
  &lt;li&gt;排队问题，长时间的流和短时间的流同时使用一个交换机端口时，导致排队，也导致短时间的数据被drop，及时没有被drop页导致了延时的增加；&lt;/li&gt;
  &lt;li&gt;buffer的问题，不同的流使用不同的交换机短空，长时间的流占用了共享的buffer。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/dctcp-problems.png&quot; alt=&quot;dctcp-problems&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;0x03-dctcp&quot;&gt;0x03 DCTCP&lt;/h3&gt;

&lt;p&gt;DCTCP利用了ECN。在交换机上，当一个端口的包超过一定的阈值之后，给包加上ECN标志。包的接受这在接受到这些包之后，将这些信息回复给发送者。发送者根据收到的包里面的ECN的情况来调整发送行为。这样就可以在发生拥塞之前就调整行为。总结一下就是3个部分:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Simple Marking at the Switch；&lt;/li&gt;
  &lt;li&gt;ECN-Echo at the Receiver；&lt;/li&gt;
  &lt;li&gt;ControllerattheSender；&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;发送方记录收到的回复包里面比例，使用这样一个函数更新：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;α ← (1 − g) × α + g × F
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;F为最新窗口里面被标记的笔记，g时一个0到1之间的比例值，这个和RTT的时间估计类似。&lt;/p&gt;

&lt;p&gt;然后使用:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;cwnd ← cwnd × (1 − α/2)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;来更新cwnd。&lt;/p&gt;

&lt;p&gt;.&lt;/p&gt;

&lt;p&gt;效果的部分数据:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/dctcp-results.png&quot; alt=&quot;dctcp-results&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;0x04-deadline-driven-delivery&quot;&gt;0x04 Deadline Driven Delivery&lt;/h3&gt;

&lt;p&gt;D3的出现时为了解决DCTCP中的deadline的问题，使用的时带宽分配的方式。每一个RTT内，发送方都计算需要在deadline之前发送完数据的带宽，然后把这个信息放进包里面。交换机在收到了这样的信息之后，使用贪婪的方式分配带宽：&lt;/p&gt;

&lt;p&gt;对于有deadline的流，就在平均共享的带宽上加上发送方需要的带宽的值，没有，则就选择平均分配的带宽:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;• For a deadline flow with desired rate r, a = (r+fs), where fs is the fair share of the spare capacity after satisfying deadline flow requests.
• For a non-deadline flow, a = fs.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;这里的具体操作还有更多的细节:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;The rate allocation description above assumes the router has the rate requests for all flows at the same point in time. In reality, the router needs to make allocation decisions in an online, dynamic setting, i.e., rate requests are spread over time, and flows start and finish. To achieve this, the rate allocation operates in a slotted fashion (from the perspective of the endhosts). 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;结果的部分数据：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/dctcp-d3-results.png&quot; alt=&quot;dctcp-d3-results&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;0x05-d2tcp&quot;&gt;0x05 D2TCP&lt;/h3&gt;

&lt;p&gt;DCTCP也是为了解决DCTCP中不可值的deadline时间改进的，同时解决D3种存在的问题。它处理考虑到拥塞的情况性外，还考虑了包的deadline信息，到达了以下的效果：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;• reduces the fraction of missed deadlines compared to DCTCP and D3 by 75% and 50%, respectively;
• achieves nearly as high bandwidth as TCP for background flows without degrading OLDI performance;
• meets3deadlines that are 35-55% tighter than those achieved by D for a reasonable 5% of missed deadlines, giving OLDIs more time for actual computation; and
• coexists with TCP flows without degrading their performance.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;D2TCP的Congestion Avoidance算法，首先同样时DCTCP中的一个公式:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;α = (1 − g) × α + g × f
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;然后定义一个参数d，代表了deadline的紧迫程度，这里的d越大代表越紧迫，然后计算一个参数p(就是penalty的意思)：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;p = a ^ d
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;This function was originally proposed for color correction in graphics, and was dubbed gamma-correction because the original paper uses γ as the exponent. Note that being a fraction, 𝛼 ≤ 1 and therefore, 𝑝 ≤ 1. 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;然后就计算新的窗口大小:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;w = w * (1 - p/2) if p &amp;gt; 0,
  = w + 1, if p = 0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;这里可以看出来，如果没有被标记的包，a = 0，这样p就是0，行为和正常TCP的行为一样，如果a = 1，那么计算处理啊的w就是正常情况下的一半。具体的d如何处理可参加论文。&lt;/p&gt;

&lt;p&gt;Famma-correction函数的示意图:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/dctcp-correction.png&quot; alt=&quot;dctcp-correction&quot; /&gt;&lt;/p&gt;

&lt;p&gt;具体分析这里省略了。结果的部分数据:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/d2tcp-results.png&quot; alt=&quot;d2tcp-results&quot; /&gt;&lt;/p&gt;

&lt;p&gt;.&lt;/p&gt;

&lt;h2 id=&quot;参考&quot;&gt;参考&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;Data Center TCP (DCTCP), SIGCOMM 2010.&lt;/li&gt;
  &lt;li&gt;Deadline-Aware Datacenter TCP (D2TCP), SIGCOMM 2012.&lt;/li&gt;
  &lt;li&gt;Better Never than Late: Meeting Deadlines in Datacenter Networks, SIGCOMM 2011.&lt;/li&gt;
  &lt;li&gt;https://en.wikipedia.org/wiki/Explicit_Congestion_Notification, Explicit Congestion Notification (ECN).&lt;/li&gt;
&lt;/ol&gt;
</description>
                <link>https://www.nagekar.com/2018/09/DCTCP,-D3-and-D2TCP.html</link>
                <guid>https://www.nagekar.com/2018/09/DCTCP, D3 and D2TCP</guid>
                <pubDate>Mon, 17 Sep 2018 00:00:00 +0800</pubDate>
        </item>

        <item>
                <title>Linux Multi-Queue Block Layer</title>
                <description>&lt;h2 id=&quot;linux-block-io-introducing-multi-queue-ssd-access-on-multi-core-systems&quot;&gt;Linux Block IO: Introducing Multi-queue SSD Access on Multi-core Systems&lt;/h2&gt;

&lt;h3 id=&quot;0x00-引言&quot;&gt;0x00 引言&lt;/h3&gt;

&lt;p&gt;这篇Paper讲的是Linux Multi-Queue Block Layer。Linux Multi-Queue Block Layer主要是为了解决现在的Linux Block Layer不能很好地使用新的高速的存储硬件(比如4k读写能得到100W的超高速的NVMe的SSD)。Multi-Queue Block Layer在Linux 3.x的后期合并到了Linux内核主线，在Linux 4.x变化比较大。&lt;/p&gt;

&lt;p&gt;硬件性能的快速变化，这里还只是2012年的数据，实际上现在的SSD的性能比这些数据高出很多。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/mq-blk-ssd.png&quot; alt=&quot;mq-blk-ssd&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;0x02-目前的问题&quot;&gt;0x02 目前的问题&lt;/h3&gt;

&lt;p&gt;Linux目前的Block Layer都是为HHD优化设计的。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/mq-blk-single.png&quot; alt=&quot;mq-blk-single&quot; /&gt;&lt;/p&gt;

&lt;p&gt;IO操作在Block Layer中会经过复杂的操作才会被执行，在高速的存储硬件目前暴露出了以下的缺点：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Request Queue Locking，现在的请求队列只的操作中，会频繁地使用锁，而这里的很多锁都是全局的，也就是说所有经过这里的请求都有可能形成竞争。在HHD这样性能比较低的硬件上，带来的竞争不明显。但是在SSD上，这里的竞争、同步操作显著了性能。&lt;/li&gt;
  &lt;li&gt;Hardware Interrupts，IOPS达到了几十万甚至更加高的时候，很多的时间就会被消耗在中断处理上了。&lt;/li&gt;
  &lt;li&gt;Remote Memory Accesses，目前的设计会导致CPU核心访问的会是Remote Memory，这个造成了不小的性能损失，特别是在NUMA架构的机器上。对缓存也不友好。&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;

&lt;/blockquote&gt;

&lt;h3 id=&quot;0x03-基本设计&quot;&gt;0x03 基本设计&lt;/h3&gt;

&lt;p&gt;顾名思义，Mutil-Queue就是由对个队列，不过不是简单的拆分而言，还要考虑到block layer上层和block layer下层各自的特性。这里使用两级队列的结构，上层的软件队列和下层的硬队列，两个队列实现不同的功能，对应不同的优化策略。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/mq-blk-multi.png&quot; alt=&quot;mq-blk-multi&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;software-staging-queues&quot;&gt;Software Staging Queues&lt;/h4&gt;

&lt;p&gt;多个Software Staging Queues为了减少请求之间的竞争，数量根据CPU数量 or CPU核心的数据确定，这个数量的确定是在lock竞争、缓存友好性和内存等资源消耗之间的一个权衡。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;So, on a NUMA system with 4 sockets and 6 cores per socket, the staging area may contain as few as 4 and as many as 24 queues. The variable nature of the request queues decreases the proliferation of locks if contention on a single queue is not a bottleneck. With many CPU architectures offering a large shared L3 cache per socket (typically a NUMA node as well), having just a single queue per proces- sor socket offers a good trade-off between duplicated data structures which are cache unfriendly and lock contention.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;hardware-dispatch-queues&quot;&gt;Hardware Dispatch Queues&lt;/h4&gt;

&lt;p&gt;Hardware Dispatch Queues利用的是存储设备自己的并行特点，具体的数量设置根据设备的特点决定，可在1 - 2048个之间选择。这里不负责顺序保障，这部分的工作有上面的层级保障。对于NVMe的SSD，一般有对个提交队列以及多个完成队列，这里的设计就显得更加重要。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Because IO ordering is not supported within the block layer any software queue may feed any hardware queue without needing to maintain a global ordering. This allows hardware to implement one or more queues that map onto NUMA nodes or CPU’s directly and provide a fast IO path from application to hardware that never has to access remote memory on any other node.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Software 和 Hardware之间的队列不一定是一一对应的关系。Software Staging Queues可以利用之前block的一些优化策略。&lt;/p&gt;

&lt;h4 id=&quot;io-scheduling--tagged-io-and-io-accounting&quot;&gt;IO-Scheduling &amp;amp; Tagged IO and IO Accounting&lt;/h4&gt;

&lt;p&gt;现在的IO-Scheduling想尽办法地将随机操作转化为顺序操作，所以要做很多的额外的工作。这里的实现方式不同，在Hardware Dispatch Queues上面的IO请求，使用的是简单的FIFO的策略。&lt;/p&gt;

&lt;p&gt;与此同时，为了在HHD之类的硬件上or保持兼容性等的原因。请求全局的重排序也是可以在Mutil-Queue中实现的，实现的位置在Software Staging Queues。&lt;/p&gt;

&lt;p&gt;对于现在默认的保证公平的IO调度器(就是CFQ IO调度器)，是在IO性能不足下的一种资源分配策略，而如果是高速的存储硬件，IO性能是充足的，也就不需要复杂的调度策略。这里Paper中也没有把问题说死：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;If fairness is essential, it is possible to design a scheduler that exploits the characteristics of SSDs at coarser granularity to achieve lower performance overhead. Whether the scheduler should reside in the block layer or on the SSD controller is an open issue. If the SSD is responsible for fair IO scheduling, it can leverage internal device parallelism, and lower latency, at the cost of additional interface complexity between disk and OS.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;.&lt;/p&gt;

&lt;p&gt;为了与前面的两级队列想适应，在其它方面也走出了一些优化。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;tag-based completions，一个tag 是一个整数，标示出了一个IO请求在驱动submission queue中的位置。当完成这个标记后(一种指示操作?)，就意味着对应的IO操作以及完成。避免了一些搜索。&lt;/li&gt;
  &lt;li&gt;更加细粒度的统计功能，同时考虑了software queues 和 dispatch queues。也方便了blktrace之类的工具对IO的追踪操作。&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;multiqueue-impact-on-device-manufacturers&quot;&gt;Multiqueue Impact on Device Manufacturers&lt;/h4&gt;

&lt;p&gt;对于制造商来说就是驱动的优化了，主要是dispatch queue，submission queue ，IO tag handling 的处理。&lt;/p&gt;

&lt;blockquote&gt;

&lt;/blockquote&gt;

&lt;h3 id=&quot;0x04-评估&quot;&gt;0x04 评估&lt;/h3&gt;

&lt;p&gt;具体参看论文:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;With SQ, the maximum latency reaches 250 milliseconds in the 4 sockets system and 750 milliseconds on the 8 sockets system. Interestingly, with SQ on a 8 sockets sys- tems, 20% of the IO requests take more than 1 millisecond to complete. This is a very significant source of variability for IO performance. In contrast, with MQ, the number of IOs which take more than 1ms to complete only reaches 0.15% for an 8 socket system, while it is below 0.01% for the other systems. Note Raw exhibits minimal, but stable, variation across all systems with around 0.02% of the IOs that take more than 1ms to complete.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;.&lt;/p&gt;

&lt;blockquote&gt;

&lt;/blockquote&gt;

&lt;h2 id=&quot;参考&quot;&gt;参考&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;Linux Block IO: Introducing Multi-queue SSD Access on Multi-core Systems, SYSTOR ’13.&lt;/li&gt;
&lt;/ol&gt;
</description>
                <link>https://www.nagekar.com/2018/09/Linux-Multi-Queue-Block-Layer.html</link>
                <guid>https://www.nagekar.com/2018/09/Linux Multi-Queue Block Layer</guid>
                <pubDate>Sat, 15 Sep 2018 00:00:00 +0800</pubDate>
        </item>

        <item>
                <title>BFQ IO Scheduler of Linux</title>
                <description>&lt;h2 id=&quot;bfq-io-scheduler-of-linux&quot;&gt;BFQ I/O Scheduler of Linux&lt;/h2&gt;

&lt;h3 id=&quot;0x00-引言&quot;&gt;0x00 引言&lt;/h3&gt;

&lt;p&gt;Linux目前使用的IO Scheduler有CFQ，NOOP和Deadline。在最近几年，Linux的Block Layer做了很多的有哈，最大的一个改变就是添加了multi-queue block layer。相适应的，IO Scheduler也出现了新的变化，在4.12版本中新加入了Budget Fair Queueing (BFQ) Storage-I/O Scheduler和Kyber multiqueue I/O scheduler。前者有不少的资料，这里先记录一下BFQ。Kyber multi-queue I/O scheduler没有文档资料，不过这个应该是一个比较简单的，代码只有约1000行，先研究研究再说。&lt;/p&gt;

&lt;p&gt;BFQ包含了很多细节的优化，这里以后慢慢完善。&lt;/p&gt;

&lt;h3 id=&quot;0x01-bfq基本思路&quot;&gt;0x01 BFQ基本思路&lt;/h3&gt;

&lt;p&gt;BFQ保持每一个进程一个IO请求队列，不同于CFQ用RR的方式轮询这些队列的方法，BFQ给这些队列一个I/O budget，这个I/O budget代表了下次能写入到磁盘的扇区(这里是HHD上面的说法)的数量。这个I/O budget的计算时一个很复杂的过程，这里只会简单的说明，不会很具体的讨论。这个I/O budget主要和进程的行为有关。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/bfq-arch.png&quot; alt=&quot;bfq-arch&quot; /&gt;&lt;/p&gt;

&lt;p&gt;一个基本的过程如下：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;挑选一下一个服务的进程，这里使用内部的B-WF2Q+，一个内部的fair-queueing调度器；&lt;/li&gt;
  &lt;li&gt;请求分发，等待调用相关函数，处理请求，这时候的处理逻辑是：
    &lt;ol&gt;
      &lt;li&gt;一个内部的C-LOOK调度器选择这个应用的一个请求，从它的请求队列中那处理，然后存储设备处理请求。这个C-LOOK既能适应HHD也能适应SSD。&lt;/li&gt;
      &lt;li&gt;应用的budget会逐渐递减；&lt;/li&gt;
      &lt;li&gt;如何这个应用的请求处理完成，or 没有了budget之后，执行下面的第3步；&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;这个应用会被退出服务，赋予一个新的budget，这里会有算法计算新的这个budget；&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;.&lt;/p&gt;

&lt;p&gt;在BGQ中，每个进程都会赋予一个固定的权重，从总体上来说，这个进程使用的磁盘的吞吐量只会和这个权重相关，和上面的budget其实没有关系。也就是说，这个磁盘的带宽会被竞争使用这个磁盘的进程共同分配。&lt;/p&gt;

&lt;p&gt;这个看上去反直觉，因为上面提到了budget，这个数值大的能帮助进程获取更加多的资源。但是这里如果是budget更加大，B-WF2Q+会将其的服务推迟得更晚，更小的budget能更及时地得到服务。进程的这个budget的计算时和这个进程的权重时没有关系了，上面提到了，这个budget更多由进程的行为决定的。&lt;/p&gt;

&lt;p&gt;如果对CFS的进程调度器也熟悉的话，这里的很多思想也是类似。BFQ、CFQ和CFS的一个目标都是公平。此外，区分进程的特点，比如区分交互进程和非交互进程以提供更加好的服务。&lt;/p&gt;

&lt;h4 id=&quot;budget-assignment&quot;&gt;Budget assignment&lt;/h4&gt;

&lt;p&gt;最为一个常识，要想获得最高的磁盘性能，访问最好都是顺序的，很显然这里如果是顺序访问磁盘的进程，最好就能赋予一个更加大的budget。BFQ会通过一系列的方法来计算磁盘的性能特点和一个进程访问的特点。通过 feedback-loop算法，BFQ尝试去计算下一次的budget，让这个budget尽可能的接近进程的需求，当然这里值的计算要考虑到目前的系统状况和磁盘的特点。&lt;/p&gt;

&lt;p&gt;BGQ会给一个进程最大的使用时间，这样可以避免随机访问的进程长时间的占有磁盘而降低了吞吐量(这里可以猜想，这里是因为随机访问的应用实际写入的数据比较小，磁盘随机访问的性能远低于顺序访问的性能，而导致budget消耗速度比较慢，从而导致了一直占有磁盘)。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;To further limit the extent at which random applications may decrease the throughput, on a budget timeout BFQ also (over)charges the just deactivated application an entire budget even if the application has used only part of it. This reduces the frequency at which applications incurring budget timeouts access the disk.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;.&lt;/p&gt;

&lt;h3 id=&quot;一些细节和优化&quot;&gt;一些细节和优化&lt;/h3&gt;

&lt;p&gt;TODO&lt;/p&gt;

&lt;h2 id=&quot;参考&quot;&gt;参考&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;https://lwn.net/Articles/720675/, Two new block I/O schedulers for 4.12.&lt;/li&gt;
  &lt;li&gt;https://lwn.net/Articles/601799/, The BFQ I/O scheduler.&lt;/li&gt;
  &lt;li&gt;Improving Application Responsiveness with the BFQ Disk I/O Scheduler, SYSTOR ’12, https://core.ac.uk/download/pdf/54000085.pdf&lt;/li&gt;
  &lt;li&gt;http://algo.ing.unimo.it/people/paolo/disk_sched/mst-2015.pdf, Evolution of the BFQ Storage-I/O Scheduler&lt;/li&gt;
&lt;/ol&gt;
</description>
                <link>https://www.nagekar.com/2018/09/BFQ-IO-Scheduler-of-Linux.html</link>
                <guid>https://www.nagekar.com/2018/09/BFQ IO Scheduler of Linux</guid>
                <pubDate>Thu, 13 Sep 2018 00:00:00 +0800</pubDate>
        </item>

        <item>
                <title>New Syscalls of Linux 4.x</title>
                <description>&lt;h2 id=&quot;new-syscalls-of-linux-4x&quot;&gt;New Syscalls of Linux 4.x&lt;/h2&gt;

&lt;h3 id=&quot;引言&quot;&gt;引言&lt;/h3&gt;

&lt;p&gt;Linux 4.19 就要发布了，按照套路，4.19应该是Linux 4.x的最后一个版本。Linux 4.0发布在2015年4月(那时候还是大二的小懵懂)。这几年中Linux改变了那些东西呢。这篇是总结了一下新增的syscalls的，其实syscall与新特性大部分时候都没啥关系，这里只是为了好玩。&lt;/p&gt;

&lt;h3 id=&quot;syscalls&quot;&gt;Syscalls&lt;/h3&gt;

&lt;h4 id=&quot;copy_file_range&quot;&gt;copy_file_range&lt;/h4&gt;

&lt;div class=&quot;language-c highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kt&quot;&gt;ssize_t&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;copy_file_range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fd_in&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loff_t&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;off_in&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                        &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fd_out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loff_t&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;off_out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                        &lt;span class=&quot;kt&quot;&gt;size_t&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;unsigned&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;flags&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Linux 4.5中添加的，主要是为了减少不必要的数据拷贝。比如从一个文件的数据拷贝到另外一个文件是，可以不经过user space。&lt;/p&gt;

&lt;h4 id=&quot;mlock2&quot;&gt;mlock2&lt;/h4&gt;

&lt;div class=&quot;language-c highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mlock&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;addr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;size_t&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mlock2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;addr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;size_t&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;flags&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;munlock&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;addr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;size_t&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Linux 4.4 添加的，mlock的强化版本，主要就是添加了一个flags的参数。主要功能就是将一个process的page不要被sweep到磁盘上面。&lt;/p&gt;

&lt;h4 id=&quot;pkey_alloc--pkey_free--pkey_mprotect&quot;&gt;pkey_alloc,  pkey_free,  pkey_mprotect&lt;/h4&gt;

&lt;div class=&quot;language-c highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pkey_alloc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;unsigned&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;long&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;flags&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;unsigned&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;long&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;access_rights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pkey_free&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pkey&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pkey_mprotect&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;addr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;size_t&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;prot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pkey&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;这个主要是对page权限的改进。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Memory Protection Keys provide a mechanism for changing protections without requiring modification of the page tables on every permission change.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;preadv2-pwritev2&quot;&gt;preadv2, pwritev2&lt;/h4&gt;

&lt;div class=&quot;language-c highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kt&quot;&gt;ssize_t&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;preadv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;struct&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;iovec&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iov&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;iovcnt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                      &lt;span class=&quot;kt&quot;&gt;off_t&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;offset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;

&lt;span class=&quot;kt&quot;&gt;ssize_t&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pwritev&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;struct&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;iovec&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iov&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;iovcnt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                       &lt;span class=&quot;kt&quot;&gt;off_t&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;offset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;

&lt;span class=&quot;kt&quot;&gt;ssize_t&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;preadv2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;struct&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;iovec&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iov&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;iovcnt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                       &lt;span class=&quot;kt&quot;&gt;off_t&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;offset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;flags&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;

&lt;span class=&quot;kt&quot;&gt;ssize_t&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pwritev2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;struct&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;iovec&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iov&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;iovcnt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                        &lt;span class=&quot;kt&quot;&gt;off_t&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;offset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;flags&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;preadv, pwritev的添加了一个flags参数的版本。&lt;/p&gt;

&lt;h4 id=&quot;statx&quot;&gt;statx&lt;/h4&gt;

&lt;div class=&quot;language-c highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;statx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dirfd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;char&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pathname&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;flags&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                 &lt;span class=&quot;kt&quot;&gt;unsigned&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;struct&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;statx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;statxbuf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;stat的强化版本。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;To access a file's status, no permissions are required on the file itself, but in the case of statx() with a pathname, execute (search) permission is required on all of the directories in pathname that lead to the file.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;userfaultfd&quot;&gt;userfaultfd&lt;/h4&gt;

&lt;div class=&quot;language-c highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;userfaultfd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;flags&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;上面的几个都很无聊，这个比较有意思一点，在Linux 4.3 添加。通常情况下，page fault是有kernel处理的，这个syscall通过文件的形式给user space处理page fault提供了一些功能。[2]中有一些说明。&lt;/p&gt;

&lt;blockquote&gt;

&lt;/blockquote&gt;

&lt;h3 id=&quot;new-syscalls-of-linux-3x&quot;&gt;New Syscalls of Linux 3.x&lt;/h3&gt;

&lt;p&gt;顺便看一下3.x的吧，emmmm。&lt;/p&gt;

&lt;h4 id=&quot;bpf&quot;&gt;bpf&lt;/h4&gt;

&lt;div class=&quot;language-c highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bpf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cmd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;union&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bpf_attr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;attr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;unsigned&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Berkeley Packet Filters相关的一个syscall，挺有用的。&lt;/p&gt;

&lt;h4 id=&quot;execveat&quot;&gt;execveat&lt;/h4&gt;

&lt;div class=&quot;language-c highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;execveat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dirfd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;char&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pathname&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
	&lt;span class=&quot;kt&quot;&gt;char&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;argv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[],&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;char&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;envp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[],&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;flags&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;execve(2)的强化版本。&lt;/p&gt;

&lt;h4 id=&quot;finit_module&quot;&gt;finit_module&lt;/h4&gt;

&lt;div class=&quot;language-c highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;finit_module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;char&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;param_values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;flags&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Load一个ELF image到kernel space，类似init_module。&lt;/p&gt;

&lt;h4 id=&quot;getrandom&quot;&gt;getrandom&lt;/h4&gt;

&lt;div class=&quot;language-c highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kt&quot;&gt;ssize_t&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;getrandom&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;buf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;size_t&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;buflen&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;unsigned&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;flags&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;借助内核生成随机数，比直接读区/dev/random更加好用。&lt;/p&gt;

&lt;h4 id=&quot;kcmp&quot;&gt;kcmp&lt;/h4&gt;

&lt;div class=&quot;language-c highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kcmp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pid_t&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pid1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pid_t&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pid2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                &lt;span class=&quot;kt&quot;&gt;unsigned&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;long&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;idx1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;unsigned&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;long&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;idx2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;检查2个进程的共享资源的信息。&lt;/p&gt;

&lt;h4 id=&quot;kexec_file_load&quot;&gt;kexec_file_load&lt;/h4&gt;

&lt;div class=&quot;language-c highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kt&quot;&gt;long&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kexec_file_load&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kernel_fd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;initrd_fd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                           &lt;span class=&quot;kt&quot;&gt;unsigned&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;long&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cmdline_len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;char&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cmdline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                           &lt;span class=&quot;kt&quot;&gt;unsigned&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;long&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;flags&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;kexec_file_load用来用于在reboot之后使用新的kernel。&lt;/p&gt;

&lt;h4 id=&quot;membarrier&quot;&gt;membarrier&lt;/h4&gt;

&lt;div class=&quot;language-c highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;membarrier&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cmd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;flags&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;提供一个类似内存屏障的功能.&lt;/p&gt;

&lt;h4 id=&quot;memfd_create&quot;&gt;memfd_create&lt;/h4&gt;

&lt;div class=&quot;language-c highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;memfd_create&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;char&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;unsigned&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;flags&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;将内存当作文章。&lt;/p&gt;

&lt;h4 id=&quot;process_vm_readv-process_vm_writev&quot;&gt;process_vm_readv, process_vm_writev&lt;/h4&gt;

&lt;div class=&quot;language-c highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kt&quot;&gt;ssize_t&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;process_vm_readv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pid_t&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                &lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;struct&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;iovec&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;local_iov&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                &lt;span class=&quot;kt&quot;&gt;unsigned&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;long&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;liovcnt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                &lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;struct&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;iovec&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;remote_iov&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                &lt;span class=&quot;kt&quot;&gt;unsigned&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;long&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;riovcnt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                &lt;span class=&quot;kt&quot;&gt;unsigned&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;long&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;flags&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;

&lt;span class=&quot;kt&quot;&gt;ssize_t&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;process_vm_writev&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pid_t&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                 &lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;struct&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;iovec&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;local_iov&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                 &lt;span class=&quot;kt&quot;&gt;unsigned&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;long&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;liovcnt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                 &lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;struct&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;iovec&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;remote_iov&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                 &lt;span class=&quot;kt&quot;&gt;unsigned&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;long&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;riovcnt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                 &lt;span class=&quot;kt&quot;&gt;unsigned&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;long&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;flags&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;对vm space做一些操作。&lt;/p&gt;

&lt;h4 id=&quot;renameat2&quot;&gt;renameat2&lt;/h4&gt;

&lt;div class=&quot;language-c highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;renameat2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;olddirfd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;char&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;oldpath&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                     &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;newdirfd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;char&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;newpath&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;unsigned&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;flags&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;renameat添加了flags参数.&lt;/p&gt;

&lt;h4 id=&quot;sched_setattr-sched_getattr&quot;&gt;sched_setattr, sched_getattr&lt;/h4&gt;

&lt;div class=&quot;language-c highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sched_setattr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pid_t&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;struct&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sched_attr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;attr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                         &lt;span class=&quot;kt&quot;&gt;unsigned&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;flags&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;

&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sched_getattr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pid_t&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;struct&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sched_attr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;attr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                         &lt;span class=&quot;kt&quot;&gt;unsigned&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;unsigned&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;flags&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;scheduling policy相关的syscalls。&lt;/p&gt;

&lt;h4 id=&quot;seccomp&quot;&gt;seccomp&lt;/h4&gt;

&lt;div class=&quot;language-c highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;seccomp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;unsigned&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;operation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;unsigned&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;flags&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Secure Computing (seccomp)，终于看到一个有趣的一点的了，这个syscall在一些地方很有用，最大的一个用处就是，终于看到一个有趣的一点的了，这个syscall在一些地方很有用，最大的一个用处就是可以对调用syscall做一些限制。&lt;/p&gt;

&lt;h4 id=&quot;sendmmsg&quot;&gt;sendmmsg&lt;/h4&gt;

&lt;div class=&quot;language-c highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sendmmsg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sockfd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;struct&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mmsghdr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;msgvec&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;unsigned&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vlen&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                    &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;flags&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;这个也是一个比较有用的syscall，主要就是将多个syscall合在一起，平摊(amortize)syscall的开支。&lt;/p&gt;

&lt;h4 id=&quot;setns&quot;&gt;setns&lt;/h4&gt;

&lt;div class=&quot;language-c highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;setns&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nstype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Linux namespace 直接相关的syscall，Docker之类的都要使用的东西。Linux 3.0的时候就添加了。&lt;/p&gt;

&lt;h2 id=&quot;参考&quot;&gt;参考&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;http://man7.org/linux/man-pages/man2/syscall.2.html&lt;/li&gt;
  &lt;li&gt;http://xiaogr.com/?p=96, Look Into Userfaultfd&lt;/li&gt;
&lt;/ol&gt;
</description>
                <link>https://www.nagekar.com/2018/09/New-Syscalls-of-Linux-4.x.html</link>
                <guid>https://www.nagekar.com/2018/09/New Syscalls of Linux 4.x</guid>
                <pubDate>Tue, 11 Sep 2018 00:00:00 +0800</pubDate>
        </item>

        <item>
                <title>Linux BtrFS</title>
                <description>&lt;h3 id=&quot;btrfs-the-linux-b-tree-filesystem&quot;&gt;BTRFS: The Linux B-Tree Filesystem&lt;/h3&gt;

&lt;h3 id=&quot;0x00-引言&quot;&gt;0x00 引言&lt;/h3&gt;

&lt;p&gt;BtrFS号称是Linux的下一代文件系统，不过感觉现在进展不怎么样，目前性能也渣。虽然有一大堆的新功能和高级特性，不过bug一大堆。从2007年开始开发到现在都11年了，还是这个鬼样子，有种要扶不上墙的感觉。这篇Paper就介绍了BtrFS的基本技术:&lt;/p&gt;

&lt;h3 id=&quot;0x01-btrfs特点&quot;&gt;0x01 BtrFS特点&lt;/h3&gt;

&lt;p&gt;诸多的新功能:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;(1) CRCs maintained for all metadata and data;
(2) efficient writeable snapshots, clones as first class citizens; 
(3) multidevice support;
(4) online resize and defragmentation;
(5) compression;
(6) efficient storage for small files;
(7) SSD optimizations and TRIM support.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;.&lt;/p&gt;

&lt;p&gt;挖坑，先去吃烧烤，之后补上&lt;/p&gt;

&lt;p&gt;.&lt;/p&gt;

&lt;h2 id=&quot;参考&quot;&gt;参考&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;Rodeh, O., Bacik, J., and Mason, C. 2013. BTRFS: The Linux B-tree filesystem. ACM Trans. Storage 9, 3, Article 9 (August 2013), 32 pages.  DOI:http://dx.doi.org/10.1145/2501620.2501623&lt;/li&gt;
&lt;/ol&gt;
</description>
                <link>https://www.nagekar.com/2018/09/Linux-BtrFS.html</link>
                <guid>https://www.nagekar.com/2018/09/Linux BtrFS</guid>
                <pubDate>Sun, 09 Sep 2018 00:00:00 +0800</pubDate>
        </item>

        <item>
                <title>F2FS</title>
                <description>&lt;h2 id=&quot;f2fs-a-new-file-system-for-flash-storage&quot;&gt;F2FS: A New File System for Flash Storage&lt;/h2&gt;

&lt;h3 id=&quot;0x00-引言&quot;&gt;0x00 引言&lt;/h3&gt;

&lt;p&gt;SSD有种它自己的特点。Linux的Ext FS，XFS FS等主要还是为HHD设计，优化的出发点也是HHD的工作方式。而SSD的工作方式和HHD有着本质的不同. F2FS是Linux上的一个为SSD设计的FS，另外F2FS是一个Log Structured的FS(所以先看一下[2,3])，现在在很多的智能手机上已经使用了(苹果也号称它的新的APFS是为SSD优化设计的，不过找不到具体技术细节的东西)。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;The detrimental effects of random writes could be reduced by the log-structured file system (LFS) approach and/or the copy-on-write strategy. For exam- ple, one might anticipate file systems like BTRFS and NILFS2 would perform well on NAND flash SSDs; unfortunately, they do not consider the charac- teristics of flash storage devices and are inevitably suboptimal in terms of performance and device lifetime. 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;.&lt;/p&gt;

&lt;blockquote&gt;

&lt;/blockquote&gt;

&lt;h3 id=&quot;0x01-基本思路&quot;&gt;0x01 基本思路&lt;/h3&gt;

&lt;h5 id=&quot;on-disk-layout&quot;&gt;On-Disk Layout&lt;/h5&gt;

&lt;p&gt;F2FS将SSD分为固定程度的segment，连续的segment组成section，一些section 组成zone。&lt;/p&gt;

&lt;p&gt;F2FS分为6个区域:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Superblock: 初始化之后就不变的，文件系统的元数据；SB&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Checkpoint：保存了文件系统状态，和LFS中的一样，也是2个轮流使用的；CP&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Segment Information Table：segmemt相关的信息；  SIT&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Node Address Table ：Node地址表；NAT&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Segment Summary Area : 保存Main Area中block的信息； SSA&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Main Area: 保存数据的地方。MA&lt;/p&gt;

    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; A node block con- tains inode or indices of data blocks, while a data block contains either directory or user file data.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;结构图，这个图很清晰地表示了F2FS的结构：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/f2fs-layout.png&quot; alt=&quot;f2fs-layout&quot; /&gt;&lt;/p&gt;

&lt;p&gt;看一看一个读文件的过程加深理解，假设读取“/dir/file” :&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;读取包含root的inode的block，这个位置信息在NAT中；&lt;/li&gt;
  &lt;li&gt;从root inode的块中，找到dir的目录项，得到dir的inode number；&lt;/li&gt;
  &lt;li&gt;通过NAT，根据dir的inode number找到物理位置；&lt;/li&gt;
  &lt;li&gt;读取dir对应的block；&lt;/li&gt;
  &lt;li&gt;在dir的block中找file的inode number，重复3，找到inode；&lt;/li&gt;
  &lt;li&gt;根据inode的信息读取file的数据，数据保存在MA中；&lt;/li&gt;
&lt;/ol&gt;

&lt;h5 id=&quot;file-structure&quot;&gt;File Structure&lt;/h5&gt;

&lt;p&gt;​    与LFS中的inode map不同，F2FS使用 “node”结构，每一个node有一个唯一的node id。使用这个id，NAT可以找到node block。node block可以表示inode，direct和direct node。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; An inode block contains a file’s metadata, such as file name, inode number, file size, atime and dtime. A direct node block contains block addresses of data and an indirect node block has node IDs locating another node blocks.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/f2fs-file-structure.png&quot; alt=&quot;f2fs-file-structure&quot; /&gt;&lt;/p&gt;

&lt;p&gt;这里基本结构和常见的没有很多区别。也使用了一些常见的优化方法，比如小文件内联。&lt;/p&gt;

&lt;p&gt;这里要注意到与LFS不同的地方，LFS是尽可能的避免随机写，F2FS这里是存在一些随机写的(看上面的结构图),比如更新NAT。由于SSD随机写的性能远高于HHD，这个是合理的。&lt;/p&gt;

&lt;h5 id=&quot;directory-structure&quot;&gt;Directory Structure&lt;/h5&gt;

&lt;p&gt;没有什么特别的。可以说就是一个映射关系表而已。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;In F2FS, a 4KB directory entry (“dentry”) block is composed of a bitmap and two arrays of slots and names in pairs. The bitmap tells whether each slot is valid or not. A slot carries a hash value, inode number, length of a file name and file type (e.g., normal file, directory and symbolic link). A directory file constructs multi-level hash tables to manage a large number of dentries efficiently.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;.&lt;/p&gt;

&lt;blockquote&gt;

&lt;/blockquote&gt;

&lt;h3 id=&quot;0x02-logging&quot;&gt;0x02 Logging&lt;/h3&gt;

&lt;h5 id=&quot;multi-head-logging&quot;&gt;Multi-head Logging&lt;/h5&gt;

&lt;p&gt;Logging当然是Log Structured的核心内容啦。与LFS使用”一个“log不同，F2FS使用了”多个”log。根据数据的“冷热”程度分为hot, warm and cold。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/f2fs-logging-level.png&quot; alt=&quot;f2fs-logging-level&quot; /&gt;&lt;/p&gt;

&lt;p&gt;F2FS使用了一些方式来判断冷热程度。如上表所示。&lt;/p&gt;

&lt;h5 id=&quot;cleaning&quot;&gt;Cleaning&lt;/h5&gt;

&lt;p&gt;Cleaning类似LFS中GC的过程，不够正对SSD做了一些优化。首先，以section粒度来进行cleaning。基本的思路和常见的比如LFS的相似:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Victim selection。找到需要回收的section；&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Valid block identification and migration，移动存活的block；&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Post-cleaning process，只有在一个checkpoint之后，这个被回收的才真正被标记为空，可以被重新使用。&lt;/p&gt;

    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;After all valid blocks are migrated, a victim section is registered as a candidate to become a new free section (called a “pre-free” section in F2FS). After a checkpoint is made, the section finally becomes a free section, to be reallocated. We do this because if a pre-free section is reused before checkpointing, the file system may lose the data referenced by a previous checkpoint when unexpected power outage occurs.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h5 id=&quot;adaptive-logging&quot;&gt;Adaptive Logging&lt;/h5&gt;

&lt;p&gt;在 normal logging和threaded logging 2中logging策略中. 与LFS不同，F2FS根据系统状态选择不同的策略，这里考虑带了SSD相对较高的随机写性能：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;if there are more than k clean sections, where k is a pre-defined threshold, normal logging is ini- tiated. Otherwise, threaded logging is activated. k is set to 5% of total sections by default and can be configured. ...
There is a chance that threaded logging incurs undesirable random writes when there are scattered holes. Nevertheless, such random writes typically show better spatial locality than those in update-in-place file systems, since all holes in a dirty segment are filled first before F2FS searches for more in other dirty segments.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;.&lt;/p&gt;

&lt;blockquote&gt;

&lt;/blockquote&gt;

&lt;h3 id=&quot;0x03-checkpointing-and-recovery&quot;&gt;0x03 Checkpointing and Recovery&lt;/h3&gt;

&lt;p&gt;这里与CP区域的关系比较大，在CP区域保存一下信息：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Header and footer，用途类似LFS中的时间戳；&lt;/li&gt;
  &lt;li&gt;NAT and SIT bitmaps，标记目前的NAT，SIT blocks;&lt;/li&gt;
  &lt;li&gt;NAT and SIT journals，保存最近更新的NAT SIT项，避免频繁更新NAT SIT；&lt;/li&gt;
  &lt;li&gt;Summary blocks of active segments，还没有写入SSA区域的SSA blocks；&lt;/li&gt;
  &lt;li&gt;Orphan blocks，如果一个inode在关闭之前被删除，它就被记录为孤儿inode(这个与linux中的文件操作相关，比如两个process同时操作一个文件，一个进程在另外一个关闭之前将这个文件删除了，那么只有到这个进程关闭之前才会将这个文件删除).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;恢复:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;After a sudden power-off, F2FS rolls back to the latest consistent checkpoint. In order to keep at least one stable checkpoint pack while creating a new pack, F2FS maintains two checkpoint packs. If a checkpoint pack has identical contents in the header and footer, F2FS considers it valid. Otherwise, it is dropped.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;此外，F2FS也使用了常见的Roll-Forward Recovery 方式&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;F2FS implements an efficient roll-forward recovery mechanism to enhance fsync performance. The key idea is to write data blocks and their direct node blocks only, excluding other node or F2FS metadata blocks. In order to find the data blocks selectively after rolling back to the stable checkpoint, F2FS remains a special flag in- side direct node blocks.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;.&lt;/p&gt;

&lt;blockquote&gt;

&lt;/blockquote&gt;

&lt;h2 id=&quot;参考&quot;&gt;参考&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;F2FS: A New File System for Flash Storage, FAST’15.&lt;/li&gt;
  &lt;li&gt;“Operating Systems: Three Easy Pieces“ (Chapter: Log-structured File Systems) by Remzi Arpaci-Dusseau and Andrea Arpaci-Dusseau. Arpaci-Dusseau Books, 2014.&lt;/li&gt;
  &lt;li&gt;The Design and Implementation of a Log-Structured File System, TOCS 1992.&lt;/li&gt;
&lt;/ol&gt;
</description>
                <link>https://www.nagekar.com/2018/09/F2FS.html</link>
                <guid>https://www.nagekar.com/2018/09/F2FS</guid>
                <pubDate>Thu, 06 Sep 2018 00:00:00 +0800</pubDate>
        </item>

        <item>
                <title>Flash-based SSDs</title>
                <description>&lt;h2 id=&quot;flash-based-ssds&quot;&gt;Flash-based SSDs&lt;/h2&gt;

&lt;h3 id=&quot;0x00-引言&quot;&gt;0x00 引言&lt;/h3&gt;

&lt;p&gt;SSD在个人电脑是逐渐要普及了。在一个很多公司，数据库的机器也是被特殊对待，一般会配备性能杠杠的SSD。这篇主要参考了[1]，这本书是可以免费获取的。&lt;/p&gt;

&lt;h3 id=&quot;0x01-基本结构和操作&quot;&gt;0x01 基本结构和操作&lt;/h3&gt;

&lt;p&gt;SSD中分为2级的结构，blocks常见的大小是128KB 256KB，另外一个是Page，最常见的大小是4KB。SSD中更加基本的结构是transistor ，一个transistor 可以存储1 2 3 4bits(目前最多一般只有4个)的信息。一个transistor 里面bit越多，性能越差，寿命越短(相对而言，在同样的技术条件下)。&lt;/p&gt;

&lt;h5 id=&quot;基本操作&quot;&gt;基本操作&lt;/h5&gt;

&lt;p&gt;Flash的基本操作有3个:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Read(page):  读取一个page的数据，这个是SSD比HHD的优点之一，随机读的性能远高于HHD；&lt;/li&gt;
  &lt;li&gt;Erase (a block): 很不幸的是，这个是Flash-based SSD的最大最麻烦的一个问题，也极大地影响了SSD和其控制器的设计。在一个page被program之前，SSD必须擦除page所在的整个块。这个是由于SSD存储的原理决定的。&lt;/li&gt;
  &lt;li&gt;Program (a page): 将数据写入到一个已经擦除的page中。&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;

&lt;/blockquote&gt;

&lt;h3 id=&quot;0x02-ftl&quot;&gt;0x02 FTL&lt;/h3&gt;

&lt;p&gt;很不幸的是，SSD的寿命也不如HHD，主要的原因是block能被擦出的次数有限:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;The typical lifetime of a block is currently not well known. Manufac- turers rate MLC-based blocks as having a 10,000 P/E (Program/Erase) cycle lifetime; that is, each block can be erased and programmed 10,000 times before failing. SLC-based chips, because they store only a single bit per transistor, are rated with a longer lifetime, usually 100,000 P/E cycles. However, recent research has shown that lifetimes are much longer than expected.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;所以，频繁地写入一个地方会导致SSD很快的损失寿命，这个就是SSD主控要解决的问题之一。SSD修改一个page里面的数据时，并不能直接修改，只能独出原来的数据，修改之后写入新的块。这样的话，物理上的块就不能被直接拿来用，直接使用逻辑上的page。这个也是FTL要实现的功能。&lt;/p&gt;

&lt;blockquote&gt;

&lt;/blockquote&gt;

&lt;h3 id=&quot;0x03-a-log-structured-ftl&quot;&gt;0x03 A Log-Structured FTL&lt;/h3&gt;

&lt;p&gt;​    一个常用的方法就是Log-Structured的 FTL，思路和Log-Structured File System相似(都是这里要解决的问题不同)。基本操作都是写入下一个空闲的page，然后更新mapping table。这里就可以想象，SSD的主控就是一个功能专用的计算机。&lt;/p&gt;

&lt;p&gt;Mapping Table是保存在内存里面的(SSD的内存)，那么这里就遇到了一个和很多存储系统相同的问题：掉电了怎么办？解决办法当然也和很村存储系统一样，使用logging加上checkpoint机制。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/flash-ssd-maping-table.png&quot; alt=&quot;flash-ssd-maping-table&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;mapping-table-size&quot;&gt;Mapping Table Size&lt;/h4&gt;

&lt;p&gt;另外一个mapping table需要解决的问题就是这个table的尺寸问题，对于现在的SSD，TB级别的很常见了(咸鱼这台电脑就已经512GB，三星一些1TB级别的性能爆表，价格也比前几年下降了好多)，甚至出现了10TB级别的SSD。对于1TB的mapping table，它的尺寸就是1GB(a single 4-byte entry per 4-KB page results in 1 GB of memory needed the device)。&lt;/p&gt;

&lt;p&gt;有没有想到OS的多级页表，这里也可以使用类似的方法。Page上面有更大的block。现在很多SSD使用了Hybrid Mapping 的机制。在这种方法中，FTL保持几个blocks是已经被擦除的，写的时候直接写这些blocks，这些blocks被称为log blocks。与此同时，FTL必须能以page为单位进行操作，所以它保存了一个page基本的mapping table，但只是对于这些log blocks的，这个table叫做log table。另外一个更大的block级别的table叫做data table。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;The key to the hybrid mapping strategy is keeping the number of log blocks small. To keep the number of log blocks small, the FTL has to periodically examine log blocks (which have a pointer per page) and switch them into blocks that can be pointed to by only a single block pointer. This switch is accomplished by one of three main techniques, based on the contents of the block
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;.&lt;/p&gt;

&lt;blockquote&gt;

&lt;/blockquote&gt;

&lt;h3 id=&quot;0x04-garbage-collection&quot;&gt;0x04 Garbage Collection&lt;/h3&gt;

&lt;p&gt;​    Log-Structured是FTL需要解决的一个问题。对于SSD来说，回收垃圾，然后重新安排存活的page，将空闲的空间放在一块有利于提高性能。这里的基本思路也是和  Log-Structured File System相似，都是读取存活的数据，然后写到另外一个地方，同时将垃圾回收。&lt;/p&gt;

&lt;p&gt;为此，SSD一般预留了一些额外的空间，或者出现了一些240GB的容量之类的。SSD的垃圾回收也是很影响性能的。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;To reduce GC costs, some SSDs overprovision the device; by adding extra flash capacity, cleaning can be delayed and pushed to the background, perhaps done at a time when the device is less busy. 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;.&lt;/p&gt;

&lt;blockquote&gt;

&lt;/blockquote&gt;

&lt;h3 id=&quot;0x05-wear-leveling&quot;&gt;0x05 Wear Leveling&lt;/h3&gt;

&lt;p&gt;​     前面提到，SSD的block擦除的次数是有限了，一个block坏了就很麻烦。最后的方式就是能将写入分摊到各个block上，这里又要考虑到很多的东西。想详细理解可以参考相关论文。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;To remedy this problem, the FTL must periodically read all the live data out of such blocks and re-write it elsewhere, thus making the block available for writing again. This process of wear leveling increases the write amplification of the SSD, and thus decreases performance as extra I/O is required to ensure that all blocks wear at roughly the same rate.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;.&lt;/p&gt;

&lt;blockquote&gt;

&lt;/blockquote&gt;

&lt;h3 id=&quot;0x06-另外一个问题&quot;&gt;0x06 另外一个问题&lt;/h3&gt;

&lt;p&gt;这里可以看到SSD内部也是Log-Structured的，对于现在OS的FS来说，大部分都是日志式的文件系统，也适用log来保证cras时的完整性，对于现在的存储系统来说(比如key-value的Level DB, RockDB or常见的数据如MySQL, PostgreQL)，也使用log来保证数据持久化。&lt;/p&gt;

&lt;p&gt;这样一层层的下来，就会做了很多额外的工作。这里就出现了Open Channel SSD，可以在软件层面控制FTL的一些功能，关于这个可以看看[2].&lt;/p&gt;

&lt;blockquote&gt;

&lt;/blockquote&gt;

&lt;h2 id=&quot;参考&quot;&gt;参考&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;“Operating Systems: Three Easy Pieces“ (Chapter: Flash-based SSDs) by Remzi Arpaci-Dusseau and Andrea Arpaci-Dusseau. Arpaci-Dusseau Books, 2014.&lt;/li&gt;
  &lt;li&gt;LightNVM: The Linux Open-Channel SSD Subsystem, FAST 2017&lt;/li&gt;
&lt;/ol&gt;
</description>
                <link>https://www.nagekar.com/2018/09/Flash-based-SSDs.html</link>
                <guid>https://www.nagekar.com/2018/09/Flash-based SSDs</guid>
                <pubDate>Wed, 05 Sep 2018 00:00:00 +0800</pubDate>
        </item>

        <item>
                <title>Log-Structured File System</title>
                <description>&lt;h2 id=&quot;log-structured-file-system&quot;&gt;Log-Structured File System&lt;/h2&gt;

&lt;h3 id=&quot;0x00-引言&quot;&gt;0x00 引言&lt;/h3&gt;

&lt;p&gt;Log是Computer Science中一个非常重要的思想，与存储相关的地方都非常常见。Log-Structured File System(这里是LFS)是不同与常见的Unix FFS的一类文件系统，这篇论文发表于1992年(还是比我年龄要大呀)，比发表于1984年的Unix FFS晚了8年时间。计算机系统随着时间也发生了很多的变化，新的方法也会随之诞生。&lt;/p&gt;

&lt;p&gt;同前面的Unix FFS的paper一样，Log-Structured File System的论文也大概在大三的时候就看过了，因为某些原因，又重新回顾了一遍。&lt;/p&gt;

&lt;p&gt;Log-Structured 是从文件系统，内存分配起到NVM空间管理，到Log-Structured 的数据结构，SSD的内部等等等，一大堆。可以来一个集合。&lt;/p&gt;

&lt;blockquote&gt;

&lt;/blockquote&gt;

&lt;h3 id=&quot;0x01-动机&quot;&gt;0x01 动机&lt;/h3&gt;

&lt;p&gt;动机基于以下的观察:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;System memories are growing;&lt;/li&gt;
  &lt;li&gt;There is a large gap between random I/O performance and se- quential I/O performance;&lt;/li&gt;
  &lt;li&gt;Existing file systems perform poorly on many common workloads;&lt;/li&gt;
  &lt;li&gt;File systems are not RAID-aware;&lt;/li&gt;
&lt;/ol&gt;

&lt;blockquote&gt;

&lt;/blockquote&gt;

&lt;p&gt;所以这里系统设计的核心就是[1]：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;How can a file system transform all writes into sequential writes? For reads, this task is impossible, as the desired block to be read may be any- where on disk. For writes, however, the file system always has a choice, and it is exactly this choice we hope to exploit.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;0x02-writing-to-disk-sequentially&quot;&gt;0x02 Writing To Disk Sequentially&lt;/h3&gt;

&lt;p&gt;​    既然想写操作都变成顺序的，那么这里就顺序写好了。不管是什么类型的写操作，都顺序的写入磁盘，看看会产生哪些问题，如何在想办法解决这些问题，最后我就就得到了一个Log-Structured File System。&lt;/p&gt;

&lt;h5 id=&quot;问题&quot;&gt;问题&lt;/h5&gt;

&lt;p&gt;如果我们更新一个文件的时候，文件的元数据也要走相应的修改。在Unix FFS之类的文件系统中，inode之类的元数据的位置是固定的，所以在更新了文件的数据之后，会直接找到对应的inode进行就地更新操作。但是对于LFS来说，这样的话写操作就不是顺序的了，会有对inode的随机写。&lt;/p&gt;

&lt;p&gt;这里的解决办法就是把inode也更新，顺序写到这里更新的操作之中，舍弃原来的旧的inode数据。&lt;/p&gt;

&lt;p&gt;看到起就想这样[1]:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;+--------+----------+----------------------------------+
|        |          |  blk[0]:A0|                      |
|        |          |           |                      |
|        |          |    I      |                      |
|        |          |           |                      |
+--------+----------------------+----------------------+
         A0         |
         ^          |
         |          |
         +----------+
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;blockquote&gt;

&lt;/blockquote&gt;

&lt;h5 id=&quot;问题-1&quot;&gt;问题&lt;/h5&gt;

&lt;p&gt;上面的方案看上去是讲所有的写操作都顺序化了。但是来考虑一种case，就是当写入了一点数据(一个block)的时候，等待了一小段时间，写入另外的数据。这个时候会发现由于磁盘在旋转，原来写入的tail还要等待一段时间才能到达写入的位置，这样导致的问题就是写入的效率降低了。&lt;/p&gt;

&lt;p&gt;​     解决方案就是write buffering，这个在文件系统中是很常用的方法了。LFS讲一次写入更新的chunk叫做segment。在前面的动机中提到，现在的系统内存越来越大，所以合理利用内存能有效地提升性能。多个在一起之后layout看上去就是这样：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/lfs-semgent.png&quot; alt=&quot;lfs-semgent&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;

&lt;/blockquote&gt;

&lt;h3 id=&quot;0x02-the-inode-map&quot;&gt;0x02 The Inode Map&lt;/h3&gt;

&lt;p&gt;对于写来说，上面的方法看上去把写操作都顺序化了，当时当想要读的时候就发现了问题: 如果找到inode以继续后面的操作？&lt;/p&gt;

&lt;p&gt;使用一个间接层来解决这个问题，这里叫做Inode map，这个map使得我们可以用一个inode number就获取到这个inode最新版本的位置。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;People often say that the solution to all problems in Computer Science is simply a level of indirection. This is clearly not true; it is just the solution to most problems (yes, this is still too strong of a comment, but you get the point). 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;这样的一个问题就是这个map必须是固定的吗？固定的话是不是又会导致非顺序写的问题。这里LFS使用的方法如inode的方法一样。&lt;/p&gt;

&lt;p&gt;问题还没有完，这样的话是不是进入了无穷的循环了。imap这么寻址？无论如何，FS中必须有一些东西是固定，要不然没法操作。LFS使用的方法是check- point region (CR)，CR保存了最新imap片段的指针。CR被周期性地更新，降低对系统性能的影响。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/lfs-cr.png&quot; alt=&quot;lfs-cr&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;

&lt;/blockquote&gt;

&lt;h3 id=&quot;0x03-读操作&quot;&gt;0x03 读操作&lt;/h3&gt;

&lt;p&gt;到了这里来看看是如何读操作的:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;第一步是读取CR；&lt;/li&gt;
  &lt;li&gt;然后LFS读取inode map到内存中(根据CR);&lt;/li&gt;
  &lt;li&gt;根据一个inode读取文件到时候，先根据inode number在inode map中找到inode的位置信息；&lt;/li&gt;
  &lt;li&gt;从磁盘中读取inode；&lt;/li&gt;
  &lt;li&gt;利用inode的信息读取数据块。&lt;/li&gt;
&lt;/ol&gt;

&lt;blockquote&gt;

&lt;/blockquote&gt;

&lt;h3 id=&quot;0x04-目录&quot;&gt;0x04 目录&lt;/h3&gt;

&lt;p&gt;到上面部分为止，基本的操作依据有雏形了。但这里还缺少了一个部分，文件系统是有目录结构的，如何处理目录。在Unix FFS中可以看到，创建一个文件也需要修改目录的数据。&lt;/p&gt;

&lt;p&gt;解决方案也是讲目录信息每次都更新写入，就想下面一样：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/lfs-dir.png&quot; alt=&quot;lfs-dir&quot; /&gt;&lt;/p&gt;

&lt;p&gt;​    到了这里来更新一下0x03中读操作的过程:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;第一步是读取CR；&lt;/li&gt;
  &lt;li&gt;然后LFS读取inode map到内存中(根据CR);&lt;/li&gt;
  &lt;li&gt;根据一个inode读取文件到时候，先根据inode number在inode map中找到inode的位置信息；这里先读取目录的数据。&lt;/li&gt;
  &lt;li&gt;从磁盘中读取inode；&lt;/li&gt;
  &lt;li&gt;利用inode的信息读取数据块。&lt;/li&gt;
  &lt;li&gt;在目录的数据中找到文件的inode number信息；&lt;/li&gt;
  &lt;li&gt;重复3,4,5步骤，读取到最终的文件数据；&lt;/li&gt;
&lt;/ol&gt;

&lt;blockquote&gt;

&lt;/blockquote&gt;

&lt;h5 id=&quot;问题-2&quot;&gt;问题&lt;/h5&gt;

&lt;p&gt;这个问题是recursive update problem，当更新的时候，由于层层目录的关系，会导致目录都被更新。&lt;/p&gt;

&lt;p&gt;其实LFS在之前的内容中就已经把这个问题解决了，解决方法依然是inode map。inode的问题可能改变了，都是LFS的目录中不保存inode的位置信息，而是保存inode number。而一个文件更新之后inode位置可能变化了，但是inode number是不会变化的。&lt;/p&gt;

&lt;blockquote&gt;

&lt;/blockquote&gt;

&lt;h3 id=&quot;0x05-garbage-collection&quot;&gt;0x05 Garbage Collection&lt;/h3&gt;

&lt;p&gt;当文件系统被更新的时候，写入的位置不断后移，之前的数据会不断的删除。LFS过期的数据会一直保存在原地，直到被垃圾回收的任务回收。&lt;/p&gt;

&lt;p&gt;前面提到了LFS中segment 的概念。LFS的GC也是根据segment 来进行的。LFS会周期性的读取segment 的信息当一个segment 里面存活的数据满足一定的条件，这里面存活的数据会被拷贝到新的地方，被删除的数据占用的空间也随之被回收。&lt;/p&gt;

&lt;p&gt;GC在Log Stuctured FS中是一个很重要的内容，可以参看原论文[2]和之后的一些优化的论文,比如[3]。&lt;/p&gt;

&lt;blockquote&gt;

&lt;/blockquote&gt;

&lt;h3 id=&quot;0x06-crash-recovery-and-the-log&quot;&gt;0x06 Crash Recovery And The Log&lt;/h3&gt;

&lt;p&gt;文件系统提高持久化的数据保存。我们前面看到，LFS大量的数据是cache在内存中的，这样就需要处理系统crash的问题(也不仅仅是这个原因)。LFS需要解决以下的问题：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;CR原子更新的问题；&lt;/li&gt;
  &lt;li&gt;CR周期更新过程中CR数据丢失的问题。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;解决第一个问题的方法是使用2个CR，每次更新的时候是更新另外一个。先修改CR，然后在修改指针的方式。此外，CR在前后两端保存了时间戳，当是完整更新的时候这两个时间戳是系统的，不相同则认为没有更新成功。&lt;/p&gt;

&lt;p&gt;解决第二个问题使用了类似数据中的方法。从最后的检查点开始，查找有效的更新。如果存在，则相应更新操作。&lt;/p&gt;

&lt;blockquote&gt;

&lt;/blockquote&gt;

&lt;h3 id=&quot;0x07-summary&quot;&gt;0x07 Summary&lt;/h3&gt;

&lt;p&gt;经典文章，值得一读。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;some modern commercial file systems, including NetApp’s WAFL, Sun’s ZFS, and Linux btrfs, and even modern flash-based SSDs, adopt a similar copy-on-write approach to writing to disk, and thus the intellectual legacy of LFS lives on in these modern file systems. In particular, WAFL got around cleaning problems by turning them into a feature; by providing old versions of the file system via snapshots, users could access old files whenever they deleted current ones accidentally.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;参考&quot;&gt;参考&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;“Operating Systems: Three Easy Pieces“ (Chapter: Log-structured File Systems) by Remzi Arpaci-Dusseau and Andrea Arpaci-Dusseau. Arpaci-Dusseau Books, 2014.&lt;/li&gt;
  &lt;li&gt;The Design and Implementation of a Log-Structured File System, TOCS 1992.&lt;/li&gt;
  &lt;li&gt;Improving the Performance of Log-structured File Systems with Adaptive Methods, SOSP 1997.&lt;/li&gt;
&lt;/ol&gt;
</description>
                <link>https://www.nagekar.com/2018/09/Log-structured-File-Systems.html</link>
                <guid>https://www.nagekar.com/2018/09/Log-structured File Systems</guid>
                <pubDate>Tue, 04 Sep 2018 00:00:00 +0800</pubDate>
        </item>

        <item>
                <title>A Fast File System for Unix</title>
                <description>&lt;h2 id=&quot;a-fast-file-system-for-unix&quot;&gt;A Fast File System for Unix&lt;/h2&gt;

&lt;blockquote&gt;

&lt;/blockquote&gt;

&lt;h3 id=&quot;0x00-引言&quot;&gt;0x00 引言&lt;/h3&gt;

&lt;p&gt;Unix FFS是针对HHD的经典文件系统设计，文章发表于1984了，距今已有34年了(比我年龄大上好多)。这篇论文在大三的时候就看过了。FFS的论文里面的思想影响到了现在很多文件系统的设计。&lt;/p&gt;

&lt;blockquote&gt;

&lt;/blockquote&gt;

&lt;h3 id=&quot;0x01-原有解决方案存在的问题&quot;&gt;0x01 原有解决方案存在的问题&lt;/h3&gt;

&lt;p&gt;​    经典的Unix文件系统布局:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Super Block
  +------+---------+---------------------------+
  |      |         |                           |
  |      |         |         Data              |
  |      | inodes  |                           |
  +------+---------+---------------------------+
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;原有系统的问题就是糟糕的性能表现，大概只能发挥出磁盘2%的带宽，低到可怕。性能低的一个最主要的原因就是远文件系统的设计是把此篇当作是一个随机访问的内存，只不过这个不支持byte-addressable而已，只能按块来访问。系统运行一段时间后碎片导致了文件块发布在此篇的各个地方，操作文件造成了大量的seek操作，严重降低了性能。&lt;/p&gt;

&lt;blockquote&gt;

&lt;/blockquote&gt;

&lt;p&gt;所以FFS的核心就是[2]:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;How can we organize file system data structures so as to improve per- formance? What types of allocation policies do we need on top of those data structures? How do we make the file system “disk aware”?
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;blockquote&gt;

&lt;/blockquote&gt;

&lt;h3 id=&quot;0x02-基本思路&quot;&gt;0x02 基本思路&lt;/h3&gt;

&lt;p&gt;问了保持兼容性，针对文件系统的API没有变化，变化的只是内部的实现。&lt;/p&gt;

&lt;blockquote&gt;

&lt;/blockquote&gt;

&lt;h4 id=&quot;关键思路-01-the-cylinder-group&quot;&gt;关键思路 01: The Cylinder Group&lt;/h4&gt;

&lt;p&gt;FFS第一个思路就是将磁盘的空间分组，主要是安照块在磁盘上的位置，它们有着到磁片中心相同的距离。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/ffs-cylinder.png&quot; alt=&quot;ffs-cylinder&quot; /&gt;&lt;/p&gt;

&lt;p&gt;​    [2]中的这一幅图很好的表达了这种思想。&lt;/p&gt;

&lt;p&gt;​     值得注意的是现在的磁盘已经获取不到相关的物理信息了，所以现在的一些FS使用的方法是将磁盘分为block group，每一个group是在磁盘上是连续的。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;+--------------------+-------------------+----------------------+
|                    |                   |                      |
|      Group 0       |     Group 1       |    Group 2           |
|                    |                   |                      |
+--------------------+-------------------+----------------------+
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;磁盘的结构也做了一些变化:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Super Block
    +-------+---------+------------+--------------------------------+
    |       |         |            |                                |
    |       |         |    inodes  |         data                   |
    |       |         |            |                                |
    +-------+---------+------------+--------------------------------+
            inode bitmap
                data bitmap
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Super Block包含了文件系统的基本元数据信息，inode bitmap， data bitmap用于指示后面的indoes，data哪些部分被使用了。后面的两个部分用途和之前系统的一样(这里bitmap，inodes是预先就分配好的，在1984年的时候磁盘比较小，分配死的没有什么问题，后来的磁盘越来越多，现在的很多文件系统针对这个方面有了很多的优化)。这里的要按照cylinder group来方便分配。&lt;/p&gt;

&lt;blockquote&gt;

&lt;/blockquote&gt;

&lt;h5 id=&quot;基本操作&quot;&gt;基本操作&lt;/h5&gt;

&lt;p&gt;让我们来想想在这个FS中创建一个文件并写入少量数据，它会进行哪些操作呢？&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;分配一个新的inode；&lt;/li&gt;
  &lt;li&gt;在inode bitmap中表示这个inode被分配了；&lt;/li&gt;
  &lt;li&gt;分配一个数据块(假如一个就可以了)，这样的话需要写data部分；&lt;/li&gt;
  &lt;li&gt;在data bitmap中标示对应的数据块被分配；&lt;/li&gt;
  &lt;li&gt;这还没有完，文件系统是存在目录结构的，所以对应的目录的元数据也要修改；&lt;/li&gt;
&lt;/ol&gt;

&lt;blockquote&gt;

&lt;/blockquote&gt;

&lt;h4 id=&quot;关键思路-02-把相关的东西放一起&quot;&gt;关键思路 02: 把相关的东西放一起&lt;/h4&gt;

&lt;p&gt;使用了几种方式把相关的东西放在一起(要考虑到HHD的特点)&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;对于目录: 查找有最少数量的目录和最多空闲inodes的cylinder，将目录的inode和数据放在这里。&lt;/li&gt;
  &lt;li&gt;对于文件: 保证inode和数据块在一个group内，将同一个目录下面的文件放在一个group内。&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;The FFS policy heuristics are not based on extensive studies of filesystem traffic or anything particularly nuanced; rather, they are based on good old-fashioned common sense (isn’t that what CS stands for after all?)1. Files in a directory are often accessed together: imagine compiling a bunch of files and then linking them into a single executable. Be- cause such namespace-based locality exists, FFS will often improve performance, making sure that seeks between related files are nice and short.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;blockquote&gt;

&lt;/blockquote&gt;

&lt;h4 id=&quot;关键思路-03-处理大文件&quot;&gt;关键思路 03: 处理大文件&lt;/h4&gt;

&lt;p&gt;由于大文件会占据一大块的空间，访问后面的文件时会造成比较昂贵的seek操作。FFS会将文件的前面几个block放到文件所在的group，而将后面的数据分成比较大的chunk，保存在其它的group中。虽然这里可能造成一些额外的seek操作，但是由于文件比较大，会被数据的传输时间”掩盖”.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Specifically, if the chunk size is large enough, the file system will spend most of its time transferring data from disk and just a (relatively) little time seeking between chunks of the block. This process of reducing an overhead by doing more work per overhead paid is called amortization and is a common technique in computer systems.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;.&lt;/p&gt;

&lt;blockquote&gt;

&lt;/blockquote&gt;

&lt;h4 id=&quot;关键思路-04-block-和-sub-blocks&quot;&gt;关键思路 04: Block 和 sub-blocks&lt;/h4&gt;

&lt;p&gt;​    FFS将block的大小增大为4k，减小了overhead，提高了性能。更加大的block也使得前面的bitmap结构更加小。带来的缺点就是可能更加多的内部碎片，因为一般情况下小文件总是占了大部分。FFS这里使用的解决方法是将一个block又可以在内部分为4个sub-blocks。好处就是文件都很小的时候能更充分利用空间，减少碎片，缺点就是处理这个使得系统的复杂程度增加了不少，有时候还会造成额外的数据移动。&lt;/p&gt;

&lt;blockquote&gt;

&lt;/blockquote&gt;

&lt;h3 id=&quot;0x03-summary&quot;&gt;0x03 Summary&lt;/h3&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Certainly all modern systems account for the main lesson of FFS: treat the disk like it’s a disk.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;.&lt;/p&gt;

&lt;blockquote&gt;

&lt;/blockquote&gt;

&lt;h2 id=&quot;参考&quot;&gt;参考&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;A Fast File System for Unix, TOCS 1984.&lt;/li&gt;
  &lt;li&gt;“Operating Systems: Three Easy Pieces“ (Chapter: Locality and The Fast File System) by Remzi Arpaci-Dusseau and Andrea Arpaci-Dusseau. Arpaci-Dusseau Books, 2014.&lt;/li&gt;
&lt;/ol&gt;

</description>
                <link>https://www.nagekar.com/2018/09/A-Fast-File-System-for-Unix.html</link>
                <guid>https://www.nagekar.com/2018/09/A Fast File System for Unix</guid>
                <pubDate>Mon, 03 Sep 2018 00:00:00 +0800</pubDate>
        </item>

        <item>
                <title>The Multi-Level Feedback Queue</title>
                <description>&lt;h2 id=&quot;the-multi-level-feedback-queue&quot;&gt;The Multi-Level Feedback Queue&lt;/h2&gt;

&lt;h3 id=&quot;0x00-引言&quot;&gt;0x00 引言&lt;/h3&gt;

&lt;p&gt;MLFQ应该OS课程都会将到的一个Scheduling算法吧。《Operating Systems: Three Easy Pieces》中这一章将的真的是非常赞了(强势安利)。就来复习一下几年前学过的内容。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;The Multi-level Feedback Queue (MLFQ) scheduler was first described by Corbato et al. in 1962 in a system known as the Compatible Time-Sharing System (CTSS), and this work, along with later work on Multics, led the ACM to award Corbato its highest honor, the Turing Award. The scheduler has subsequently been refined throughout the years to the implementations you will encounter in some modern systems.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;0x01-how-to-change-priority&quot;&gt;0x01 How To Change Priority&lt;/h3&gt;

&lt;p&gt;算法的核心就是到达一下的目标：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;How can we design a scheduler that both minimizes response time for interactive jobs while also minimizing turnaround time without a priori knowledge of job length?
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;书中这篇讲述的方式和《Paxos Made Simple》的一样，都是先提出一个问题or目标，给出基本的方案。然后分析基本的方案存在的问题，之后解决这些问题，得到一个达到目标的解决方案。&lt;/p&gt;

&lt;p&gt;Scheduler 一般都存在一些基本的规则，先来看看这里的两个基本的规则:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;• Rule 1: If Priority(A) &amp;gt; Priority(B), A runs (B doesn’t).

• Rule 2: If Priority(A) = Priority(B), A &amp;amp; B run in RR.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;要改变Priority的一个原因就是根据任务运行的状况观测太是出于哪一种类型的人物，对于交互式的任务(or IO密集型的)，一般的做法就是提高它的Priority，来达到更好的反应时间。交互式任务的一个特点就是等待(想想一个人键盘的输入，从按下空格键到按下回车键对于计算机来说都是一段很长的时间了，对于计算机来说，它等你按回车键等了很久)。所以这里一个基本的思路就是，对于每次运行丢用不完自己时间片的任务，就保持它的Priority，对于能用完自己时间片的任务，更加可能是非交互式的任务，所以就降低它的Priority：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;• Rule 3: When a job enters the system, it is placed at the highest priority (the topmost queue).

• Rule 4a: If a job uses up an entire time slice while running, its pri- ority is reduced (i.e., it moves down one queue).

• Rule 4b: If a job gives up the CPU before the time slice is up, it stays at the same priority level.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h5 id=&quot;存在的问题&quot;&gt;存在的问题&lt;/h5&gt;

&lt;ol&gt;
  &lt;li&gt;饥饿，交互式的任务一多，其它类型的任务就很难得到运行的机会；&lt;/li&gt;
  &lt;li&gt;不安全，应用可以hack这个sheduler，通过在这个时间片的末尾发出IO请求，它就能一直保持住它的优先级；&lt;/li&gt;
  &lt;li&gt;任务的行为不是一成不变的，一个任务已开始可能是IO密集型的，之后会变成CPU密集型的，反之亦然。然而在这里一旦开始被认为是CPU密集型的，它就没机会变为IO密集型的了(sheduler的看法)&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;the-priority-boost&quot;&gt;The Priority Boost&lt;/h3&gt;

&lt;p&gt;为了解决之前存在的问题，引入了规则5:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;• Rule 5: After some time period S, move all the jobs in the system to the topmost queue.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;很显然的上面的第1，3个问题就被解决了。&lt;/p&gt;

&lt;h3 id=&quot;better-accounting&quot;&gt;Better Accounting&lt;/h3&gt;

&lt;p&gt;​    上面提到的第2个问题还是没有解决。之所以存在这个问题，是因为上面的rule 4a,4b。那就将rule修改一下:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Rule 4: Once a job uses up its time allotment at a given level (regardless of how many times it has given up the CPU), its priority is reduced (i.e., it moves down one queue).
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;现在，只要它用完了在一个allotment上面的时间片，它就会被削减priority。对于交互型的任务，它就越可能留在更高的优先级上，对于非交互型的任务，它就会越快地滑向低优先级。&lt;/p&gt;

&lt;h3 id=&quot;summary&quot;&gt;Summary&lt;/h3&gt;

&lt;p&gt;当然实际上的相同会比这个复杂很多，不过这里基本思路都有了。实际使用的系统中，有Solaris和FreeBSD使用了这个类型的Scheduler。之前Linux 使用的O(1)也是类型的。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;MLFQ is interesting for the following reason: instead of demanding a priori knowledge of the nature of a job, it observes the execution of a job and prioritizes it accordingly. In this way, it manages to achieve the best of both worlds: it can deliver excellent overall performance (similar to SJF/STCF) for short-running interactive jobs, and is fair and makes progress for long-running CPU-intensive workloads. For this reason, many systems, including BSD UNIX derivatives, Solaris, and Windows NT and subsequent Windows operating systems use a form of MLFQ as their base scheduler.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;总结一下：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;• Rule 1: 如果A的优先级 &amp;gt; B的优先级, A运行.

• Rule 2: 如果A的优先级 == B的优先级, A B以RR的方式运行.

• Rule 3: 任务最开始加入的时候，都将它运行在最高的优先级.

• Rule 4: 在一个队列上面运行完时间片之后，将削减优先级，降低到更加低的队列.

• Rule 5: 周期性地将所有任务提升到最后优先级.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;.&lt;/p&gt;

&lt;blockquote&gt;

&lt;/blockquote&gt;

&lt;h2 id=&quot;参考&quot;&gt;参考&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;“Operating Systems: Three Easy Pieces“ (Chapter: The Multi-Level Feedback Queue) by Remzi Arpaci-Dusseau and Andrea Arpaci-Dusseau. Arpaci-Dusseau Books, 2014.&lt;/li&gt;
&lt;/ol&gt;

</description>
                <link>https://www.nagekar.com/2018/09/The-Multi-Level-Feedback-Queue.html</link>
                <guid>https://www.nagekar.com/2018/09/The Multi-Level Feedback Queue</guid>
                <pubDate>Sun, 02 Sep 2018 00:00:00 +0800</pubDate>
        </item>

        <item>
                <title>ISA Wars</title>
                <description>&lt;h2 id=&quot;isa-wars-understanding-the-relevance-of-isa-being-risc-or-cisc-to-performance-power-and-energy-on-modern-architectures指令集战争理解现代risc或cisc处理器架构与性能能耗和能效的相关性&quot;&gt;ISA Wars: Understanding the Relevance of ISA being RISC or CISC to Performance, Power, and Energy on Modern Architectures(指令集战争：理解现代RISC或CISC处理器架构与性能、能耗和能效的相关性)&lt;/h2&gt;

&lt;h2 id=&quot;0x00-引言&quot;&gt;0x00 引言&lt;/h2&gt;

&lt;p&gt;RISC和CISC的比较来源已久，早期的比较大多都集中在性能方面。一般情况下，会认为CISC处理器的性能更加好，而能耗和能效更加低，RISC反之，当然这种说法不一定准确。而现在，CPU架构发展已经相对成熟，单核性能每年增长的幅度很小，CPU多核化，智能手机等移动设备的CPU越来越受关注。那么在CPU发展变化之后，RISC和CISC的指令集类型对处理器的各个关键指标又有什么样的影响呢？这里我们来看一看TPCS 2015上的一篇文章[1]。这篇paper主要做的就是:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; We present an exhaustive and rigorous analysis using workloads that span smart-phone, desktop, and server applications. In our study, we are primarily interested in whether and, if so, how the ISA being RISC or CISC impacts performance and power.
 通过运行智能手机、桌面和服务器应用程序等的不同的应用程序，我们描述了一个详尽的、严格的分析。在我们的研究中，感兴趣的主要是RISC或CISC指令集是否对性能、能耗造成了影响，如果是，是如何影响的。
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;0x01-所研究的平台&quot;&gt;0x01 所研究的平台&lt;/h2&gt;

&lt;p&gt;​    这篇paper主要讨论了x86、ARM和MIPS架构。讨论的处理器信息如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/ISA-Wars-table-2png.png&quot; alt=&quot;ISA-Wars-table-2png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;注：&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Width: 能同时发射几条指令;
Issue: 指令发射(这里表示的是顺序发射还是乱序发射)；
OoO: 乱序（out-of-order)；
In Order: 顺序；
L1D: 一级数据缓存；
L1I: 一级指令缓存；
AVX，SSE：x86架构的SIMD(单指令多数据)指令集拓展;
NEON: ARM架构的SIMD指令集拓展;
关于这类处理器效果的基本知识，可以参考文献[2,3].
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;.&lt;/p&gt;

&lt;h2 id=&quot;0x02-key-finds&quot;&gt;0x02 Key Finds&lt;/h2&gt;

&lt;p&gt;关于论文具体是如何做出评价，这里就不具体讨论了，有兴趣的可参考原paper，这里只总结一下论文中的key finds（图表也不一一给出了，比较多，复杂。还是可以参考原论文）。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;h4 id=&quot;key-finding-1-from-execution-time-comparison从执行时间方面&quot;&gt;Key Finding 1: (from Execution Time Comparison，从执行时间方面)&lt;/h4&gt;

    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  Large performance gaps exist across the seven platforms studied, as expected, since frequency ranges from 600MHz to 3.4GHz and microarchitectures are very different.
  因为CPU频率和微架构存在巨大的差异，CPU之间也存在巨大性能差异。
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;h4 id=&quot;key-finding-2-from-cycle-count-comparison从运行周期方面&quot;&gt;Key Finding 2: (from Cycle-Count Comparison，从运行周期方面)&lt;/h4&gt;

    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  Performance gaps, when normalized to cycle counts, are predominantly less than 3× when comparing in-order cores to each other and out-of-order cores to each other.
  性能之间的差异，当规范化到执行周期之后，顺序执行的CPU之间、乱序执行的CPU之间的性能差距不超过3x。
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;h4 id=&quot;key-finding-3-from-instruction-count-comparison从执行的指令数量方面&quot;&gt;Key Finding 3: (from Instruction Count Comparison,从执行的指令数量方面)&lt;/h4&gt;

    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  Despite similar instruction counts across ISAs, CPI can be less on x86 implementations across all suites (as shown in Table VI). This finding disproves prior belief that CISC implementations should have a higher CPI than RISC implementations (due to the complex instructions in CISC). Microarchitecture is the dominant factor that affects performance, not the ISA.
  尽管不同的指令集的CPU执行的指令数相似，但是x86架构的CPI在各个测试中都更小。这个发现证明了之前关于CISC架构的处理器比RISC架构的处理器会有更高的CPI的观点是错误的(因为CISC架构的处理器指令更加复杂)。微架构是影响性能的主要因素，而不是指令集。
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;h4 id=&quot;key-finding-4-from-instruction-format-and-mix从指令格式和各指令组合&quot;&gt;Key Finding 4: (from Instruction Format and Mix,从指令格式和各指令组合)&lt;/h4&gt;

    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; Combining the instruction count and mix findings, we conclude that ISA effects are indistinguishable between x86 and ARM implementations. Due to infrastructure limitations, we do not have enough data to make the same claim for the MIPS platform. However, we suspect that instruction count differences on MIPS platform are due to system software issues and not due to the ISA.
  组合指令数和指令组合的结果，我们总结出指令集的作用在x86和ARM上无法区分。因为基础设施的限制，我们没有足够的数据对MIPS架构做出同样的总结。然而，我们推测MIPS平台上的指令数的不同是由于系统软件的问题而不是因为指令集。
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;h4 id=&quot;key-finding-5-from-microarchitecture从微架构方面&quot;&gt;Key Finding 5: (from microarchitecture,从微架构方面)&lt;/h4&gt;

    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  The microarchitecture has the dominant impact on performance. The ARM, x86, and MIPS architectures have similar instruction counts. The microarchitecture, not the ISA, is responsible for performance differences.
  微架构对性能起支配性的作用。ARM，x86和MIPS有相似的指令数。微架构是造成性能的原因而不是指令集。
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;h4 id=&quot;key-finding-6-from-isa-influence-on-microarchitecture-从指令集对微架构的影响&quot;&gt;Key Finding 6: (from ISA Influence on Microarchitecture, 从指令集对微架构的影响)&lt;/h4&gt;

    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  As shown in Table VIII, there are significant differences in microarchitectures. Drawing on instruction mix and instruction count analysis, we feel that the only case where the ISA forces larger structures is on the ROB size, physical rename file size, and scheduler size since there are almost the same number of x86 micro-ops in flight compared to ARM and MIPS instructions. The difference is small enough that we argue it is not necessary to quantify further. Beyond the translation to micro-ops, pipelined implementation of an x86 ISA introduces no additional overheads over an ARM or MIPS ISA for these performance levels.
  如表8所示，微架构之间存在明显的不同。依据指令组合和指令数分析，我们认为指令集迫使更大的结构只在ROB尺寸，物理重命名文件大小和调度器大小，因为运行中x86微操作的数量和ARM、MIPS几乎相同。我们认为没有必要进一步量化，因为差别很小。除了转化到微操作之外，x86指令集流水线的实现在这些性能级别上不会在ARM或MIPS的实现上引入额外的开销。
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;

    &lt;p&gt;&lt;img src=&quot;/assets/img/ISA-Wars-table-8.png&quot; alt=&quot;ISA-Wars-table-8&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;h4 id=&quot;key-finding-7-from-average-power从平均能耗方面&quot;&gt;Key Finding 7: (from Average Power,从平均能耗方面)&lt;/h4&gt;

    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; Power consumption does not have a direct correlation to the ISA being RISC or CISC.
 能源消耗和指令集在RISC或CISC没有之间的关联。
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;h4 id=&quot;key-finding-8-from-average-technology-independent-power从技术无关的拼接能耗方面&quot;&gt;Key Finding 8: (from Average Technology Independent Power,从技术无关的拼接能耗方面)&lt;/h4&gt;

    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  the choice of power- or performance-optimized core designs impacts core power use more than does ISA.
  能耗或性能优化的核心设计选择造成的能耗影响大于指令集的影响。
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;h4 id=&quot;key-finding-9-from--average-energy-从平均能效方面&quot;&gt;Key Finding 9: (from  Average Energy, 从平均能效方面)&lt;/h4&gt;

    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; Since power and performance are both primarily design choices, energy use is also primarily impacted by design choice. ISA’s impact on energy is insignificant.
 因为能耗和性能都是主要的CPU设计选择，能源用途也主要被CPU设计上的选择影响。指令集的对能效的影响是微不足道的。
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;h4 id=&quot;key-finding-10-from-power-performance-tradeoffs-从能耗性能权衡方面&quot;&gt;Key Finding 10: (from Power-Performance Tradeoffs, 从能耗、性能权衡方面)&lt;/h4&gt;

    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  Regardless of ISA or energy-efficiency, high-performance processors require more power than lower-performance processors. They follow well-established cubic power/performance tradeoffs regardless of ISA.
  不考虑指令集或能效的情况下，高性能处理器比低性能的处理器有更高的能耗。不考虑指令集情况下，他们服从公认的三次方的功耗/性能比。
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;h4 id=&quot;key-finding-11-from-energy-performance-tradeoffs-从能效性能权衡方面&quot;&gt;Key Finding 11: (from Energy-Performance Tradeoffs, 从能效、性能权衡方面)&lt;/h4&gt;

    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;It is the microarchitecture and design methodologies that really matter.
微架构和设计上的选择非常重要。
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;0x03-总结&quot;&gt;0x03 总结&lt;/h2&gt;

&lt;p&gt;​    从本文的一些总结可以看出：处理器的性能、能耗和能效对于处理器的指令集是CISC或者RISC没有多大的联系。这篇paper通过使用一些新的方法，测量现代的一些处理器的性能、功耗和能效的特点(这些处理器的年代在2010年-2013年左右，大约5-8年前)，更加严谨地得出来这样一个观点。关于paper使用的具体方法、工具和测量数据，有兴趣的可以阅读原文。&lt;/p&gt;

&lt;h2 id=&quot;0x04-参考文献&quot;&gt;0x04 参考文献&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;Emily Blem, Jaikrishnan Menon, Thiruvengadam Vijayaraghavan, and Karthikeyan Sankaralingam. 2015. ISA wars: Understanding the relevance of ISA being RISC or CISC to performance, power, and energy on modern architectures. ACM Trans. Comput. Syst. 33, 1, Article 3 (March 2015), 34 pages.  DOI: http://dx.doi.org/10.1145/2699682&lt;/li&gt;
&lt;/ol&gt;

</description>
                <link>https://www.nagekar.com/2018/09/ISA-Wars.html</link>
                <guid>https://www.nagekar.com/2018/09/ISA Wars</guid>
                <pubDate>Sat, 01 Sep 2018 00:00:00 +0800</pubDate>
        </item>

        <item>
                <title>A Write-Friendly and Cache-Optimized Hashing Scheme for NVM Systems</title>
                <description>&lt;h2 id=&quot;a-write-friendly-and-cache-optimized-hashing-scheme-for-non-volatile-memory-systems&quot;&gt;A Write-Friendly and Cache-Optimized Hashing Scheme for Non-Volatile Memory Systems&lt;/h2&gt;

&lt;h3 id=&quot;0x00-引言&quot;&gt;0x00 引言&lt;/h3&gt;

&lt;p&gt;非易失性内存(NVM)是现在的一个研究热点。对于现在存在的一些hash table的算法都不能很好的适应NVM的环境，其主要原因就是NVM写的成本相对来说比较高，而常规的设计会造成很多额外的写操作。这篇paper的主要目的就是减少额外的写操作，同时为cache优化。&lt;/p&gt;

&lt;h3 id=&quot;0x01-基本思路&quot;&gt;0x01 基本思路&lt;/h3&gt;

&lt;p&gt;这篇paper使用的方法叫做Path hashing ，方法理解起来很简单，文章中的一段话加上一幅图即可：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  Storage cells in the path hashing are logically organized as an inverted complete binary tree. The last level of the inverted binary tree, i.e., all leaf nodes, is addressable by the hash functions. All nodes in the remaining levels are non-addressable and considered as the shared standby positions of the leaf nodes to deal with hash collisions. When hash collisions occur in a leaf node, the empty standby positions of the leaf node are used to store the conflicting items. Thus insertion and deletion requests in path hashing only need to probe the leaf node and its standby positions for finding an empty position or the target item, resulting in no extra writes.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;基本思路就是出路原来位置槽的部分外，还有额外的共享的standby cells，这些cells使用了解决hash冲突的，同时又不造成额外的写操作。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/path-hashing-arch.png&quot; alt=&quot;path-hashing-arch&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;position-sharing&quot;&gt;Position Sharing&lt;/h4&gt;

&lt;p&gt;倒置的树形结构，除了最高的一层外，更高的层的standby cell是有多个slot共享的，越低共享的slot就越多。可想而知，越向下数量越少就节约了空间。如果都使用一样的节点数量，达到下面层的数据项是越来越少的，这样就会浪费空间。&lt;/p&gt;

&lt;h4 id=&quot;double-path-hashing&quot;&gt;Double-Path Hashing&lt;/h4&gt;

&lt;p&gt;一盒hash函数的情况下，每一个key可以放置的位置是L+1个(L是高度)，一个基本的思路使用2个hash函数，这样的话可选择的位置就增加了接近2倍。但是实际上这样的方法并不是很好，两个hash有可能有不少的重叠的地方。更加简单更加好的方法就是一个hash函数计算在前半部分的slot，一个计算在后半部分的slot。&lt;/p&gt;

&lt;h4 id=&quot;path-shortening&quot;&gt;Path Shortening&lt;/h4&gt;

&lt;p&gt;去除没有数据的level，消除了不必要的查询。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/path-hasing-phy.png&quot; alt=&quot;path-hasing-phy&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;0x02-算法&quot;&gt;0x02 算法&lt;/h4&gt;

&lt;p&gt;Path hashing的基本算法和普通的hash table没有区别，算法也简单直观，直接就直接给出paper中的描述就可以了:&lt;/p&gt;

&lt;p&gt;插入，可以插入的位置都要考虑到。我们这里也发现了path hasing的一个缺点，没有rehash的逻辑。(个人认为这里有一个比较好的解决方法，不知道行不行得通).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/pash-hasing-insert.png&quot; alt=&quot;pash-hasing-insert&quot; /&gt;&lt;/p&gt;

&lt;p&gt;其它的query，delete操作都比较简单，就不重复了。&lt;/p&gt;

&lt;h3 id=&quot;0x03-缓存优化&quot;&gt;0x03 缓存优化&lt;/h3&gt;

&lt;p&gt;简而言之，就是将一些”level”“压”到一起：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/path-hashing-cache.png&quot; alt=&quot;path-hashing-cache&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;

&lt;/blockquote&gt;

&lt;blockquote&gt;

&lt;/blockquote&gt;

&lt;p&gt;简单有好玩的新思路，good idea。&lt;/p&gt;

&lt;blockquote&gt;

&lt;/blockquote&gt;

&lt;blockquote&gt;

&lt;/blockquote&gt;

&lt;h2 id=&quot;参考&quot;&gt;参考&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;A Write-Friendly and Cache-Optimized Hashing Scheme for Non-Volatile Memory Systems，TPDS 2018.&lt;/li&gt;
&lt;/ol&gt;
</description>
                <link>https://www.nagekar.com/2018/08/Path-Hashing.html</link>
                <guid>https://www.nagekar.com/2018/08/Path Hashing</guid>
                <pubDate>Sat, 25 Aug 2018 00:00:00 +0800</pubDate>
        </item>

        <item>
                <title>Fine-grained TCP Retransmissions</title>
                <description>&lt;h2 id=&quot;safe-and-effective-fine-grained-tcp-retransmissions-for-datacenter-communication&quot;&gt;Safe and Effective Fine-grained TCP Retransmissions for Datacenter Communication&lt;/h2&gt;

&lt;h3 id=&quot;引言&quot;&gt;引言&lt;/h3&gt;

&lt;p&gt;TCP中对于超时重传的时间处理上，为了避免虚假的超时重传，设置来一个最小的超时时间。在Linux上，这个是时间一般是200ms。在广域网上，这个最低的值设置是有意义的。但是在延时很低的数据中心内部，设个时间限制就可能导致一些问题，特别是incast的情况(就是多台机器与一台机器同时通信)。&lt;/p&gt;

&lt;p&gt;这篇Paper就讨论了减小这个RTO带来的影响，发现可以通过将这个RTO的最小值设置的很小也能保证安全和效率，同时解决存在的问题。&lt;/p&gt;

&lt;h3 id=&quot;问题分析&quot;&gt;问题分析&lt;/h3&gt;

&lt;p&gt;现在常见的RTO计算方式是:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;RTO = SRTT + (4 × RTTVAR)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;考虑到指数退让，这里就是:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;timeout = RTO × 2 ^ backoff
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;.&lt;/p&gt;

&lt;p&gt;模拟中这个RTO的最小值对性能的影响，可以看出来RTO的最小值对性能有着明显的影响(模拟情况):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/fine-rto-simulation.png&quot; alt=&quot;fine-rto-simulation&quot; /&gt;&lt;/p&gt;

&lt;p&gt;​	Paper中的在实际情况中的测试，也显示出减小RTOmin对性能的影响。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Despite these differences, the real world results show the need to reduce the RTO to at least 1ms to avoid throughput degradation at scales of up to 16 servers.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;此外，除了减小RTO能带来明显的好处外，给timeout一个随机的元素也能带来良好的效果:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;timeout = (RTO + (rand(0.5) × RTO)) × 2 ^ backoff
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;模拟结果：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/rto-fine-rand.png&quot; alt=&quot;rto-fine-rand&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;实现&quot;&gt;实现&lt;/h3&gt;

&lt;p&gt;实现上主要修改的地方就是利用linux的高精度时间子系统，解决之前TCP stack中时间测量过于粗糙的问题。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;With the TCP timestamp option enabled, RTT estimates are calculated based on the difference between the timestamp option in an earlier packet and the corresponding ACK. We convert the time from nanoseconds to microseconds and store the value in the TCP timestamp option.2 This change can be accomplished entirely on the sender—receivers already echo back the value in the TCP timestamp option.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;.&lt;/p&gt;

&lt;blockquote&gt;

&lt;/blockquote&gt;

&lt;h3 id=&quot;安全否&quot;&gt;安全否?&lt;/h3&gt;

&lt;p&gt;其实上面减小RTOmin能带来的效果是很显然的，主要是安全问题？是否会导致大量的假的超时，从而导致重传反而降低了性能呢？Paper的观点当然是很安全啦。&lt;/p&gt;

&lt;p&gt;RTO本来就有很大的随机性的元素，选择什么样的RTOmin，是一个取舍的问题。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;As a result, wide-area “packet delays [are] not mathematically [or] operationally steady”, which confirms the Allman and Paxson observation that RTO estimation involves a fundamental tradeoff between rapid retransmission and spurious retransmissions.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;.&lt;/p&gt;

&lt;p&gt;Delayed ACK确实可能对器有一些影响，但是，这个情况可以被TCP的新功能和能产生影响的有限的条件缓和。说了好大的一堆，就是说没啥影响。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;In practice, these two potential consequences are mitigated by newer TCP features and by the limited circumstances in which they occur, as we explore in the next two sections. We find that eliminating the RTOmin has little impact on bulk data transfer performance for wide- area flows, and that in the datacenter, delayed ACK causes only a small, though noticeable drop in throughput when the RTOmin is set below the delayed ACK threshold.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;.&lt;/p&gt;

&lt;p&gt;在广域网上测试的数据 ：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/rto-wide-erea.png&quot; alt=&quot;rto-wide-erea&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;参考&quot;&gt;参考&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;Safe and Effective Fine-grained TCP Retransmissions for Datacenter Communication, SIGCOMM 2009.&lt;/li&gt;
&lt;/ol&gt;
</description>
                <link>https://www.nagekar.com/2018/08/Fine-grained-TCP-Retransmissions-for-Datacenter.html</link>
                <guid>https://www.nagekar.com/2018/08/Fine-grained TCP Retransmissions for Datacenter</guid>
                <pubDate>Sat, 18 Aug 2018 00:00:00 +0800</pubDate>
        </item>

        <item>
                <title>Read-Log-Update</title>
                <description>&lt;h2 id=&quot;read-log-update--a-lightweight-synchronization-mechanism-for-concurrent-programming&quot;&gt;Read-Log-Update – A Lightweight Synchronization Mechanism for Concurrent Programming&lt;/h2&gt;

&lt;h2 id=&quot;0x00-引言&quot;&gt;0x00 引言&lt;/h2&gt;

&lt;p&gt;RLU可以看作是RCU的升级版本。RCU在Linux Kernel 中已经得到广泛的应用了。但是其也存在一些缺点，最大的一个缺点就是很难用，其次就是只适应有很少的write操作的情形，最后就是等待读者完成操作的延时问题(具体看看RCU有关的东西？)。RLU则可以解决RCU存在的一些问题:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;RLU provides support for multiple object updates in a single operation by combining the quiescence mechanism of RCU with a global clock and per thread object-level logs.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;.&lt;/p&gt;

&lt;blockquote&gt;

&lt;/blockquote&gt;

&lt;h3 id=&quot;0x01-基本思路&quot;&gt;0x01 基本思路&lt;/h3&gt;

&lt;p&gt;通过使用综合了结合global clock的RCU的quiescence机制和 thread直接独立的object logs，RLU一个操作支持多个对象更新。RLU有以下的基本思路，这一部分先假设写都是顺序进行的:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;所有操作在开始都要读取global clock(g-clock)到自己线程局部的l-lock中，使用这个lock来对共享的对象解引用；&lt;/li&gt;
  &lt;li&gt;对于更新操作，一个线程先将对象拷贝到自己的log中，在自己的log中操作这个对象，这里要获取object(s)的lock；这里读操作不需要申请任何lock(s)，可以直接访问对象；&lt;/li&gt;
  &lt;li&gt;对于write操作，为了提交这次的更新，这个线程要先递增g-lock，这样就将各个线程的操作分为了两个部分：
    &lt;ol&gt;
      &lt;li&gt;在这个递增之前发起的；&lt;/li&gt;
      &lt;li&gt;在这个递增之后发起的；&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;对于前者，这些操作它们会读取旧的object(s)，对于后者，这些操作会读取更新线程的log中的副本；&lt;/li&gt;
  &lt;li&gt;更新的线程等到在旧object(s)上的读操作都完成之后，更新的线程然后提交更新，释放lock(s)，完成操作。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;下面以一个图来一步步解释：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/rlu-principle.png&quot; alt=&quot;rlu-principle&quot; /&gt;&lt;/p&gt;

&lt;h5 id=&quot;第1部分&quot;&gt;第1部分&lt;/h5&gt;

&lt;p&gt;T1和T2都先执行读操作。先读取g-clock到自己本地的l-clock之中；然后T1读取O1, T2读取 O1 O2，它什么都是直接读取，不需要lock操作；&lt;/p&gt;

&lt;h5 id=&quot;第2部分&quot;&gt;第2部分&lt;/h5&gt;

&lt;p&gt;T2现在想要更新O2 O3，上面说到，更新都是在自己本地的log里面进行的，在此之前，T2要先申请O2 O3上的 RLU shared locks 。&lt;/p&gt;

&lt;p&gt;这个时候T1想要读取O2，然后它就会发现被T2 locked了，T1就去查看T2的w-clock，发现无穷大，T1就知道T2还没有提交更新的对象，T1就知道了自己的操作不在这个更新之后才发生的，这样它就应该直接去读取原来的object。&lt;/p&gt;

&lt;h5 id=&quot;第3部分&quot;&gt;第3部分&lt;/h5&gt;

&lt;p&gt;T2完成了自己的更新操作，准备提交这次更新。它先递增clock，写到自己的w-clock，然后就是将递增之后的写到g-clock。不过此时T2要等到之前的操作都完成，这里是T1的操作。等到T1完成，T2就可以将对象安全写回，然后释放lock(s)。对于T3，由于它是属于之后的操作，就可以不用管了。&lt;/p&gt;

&lt;p&gt;对于T3的读取，如果T2次时没有将更新写回，它就比较T2的w-clock，发现T3的 l-clock &amp;gt;= T2的w-clock，他就可以去T2的log里面读取数据。&lt;/p&gt;

&lt;h3 id=&quot;0x02-synchronizing-write-operations&quot;&gt;0x02 Synchronizing Write Operations&lt;/h3&gt;

&lt;p&gt;如果让RLU支持并发写操作呢？这里使用的直接就是细粒度的lock。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Another approach is to use fine-grained locks. In RLU, each object that a writer modifies is logged and locked by the RLU mechanism. Programmers can therefore use this locking process to coordinate write operations. For example, in a linked-list implementation, instead of grabbing a global lock for each writer, a programmer can use RLU to traverse the linked list, and then use the RLU mechanism to lock the target node and its predecessor. If the locking fails, then the operation restarts, otherwise the programmer can proceed and modify the target node (e.g., insertion or removal) and release the locks.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;来看看伪代码，enjoy it:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/rlu-pseudo-code.png&quot; alt=&quot;rlu-pseudo-code&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;0x03-一个优化&quot;&gt;0x03 一个优化&lt;/h3&gt;

&lt;p&gt;上面的伪代码中，可以发现writer每次提交都要call RLU_SYNCHRONIZE函数(第47行)。目的就是等待其它读者完成旧object(s)上面的操作，这里的一个优化就是尽可能地将这个操作推迟:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;On commit, instead of incrementing the global clock and executing RLU synchronize, the RLU writer simply saves the current write-log and generates a new log for the next writer. In this way, RLU writers execute without blocking on RLU synchronize calls, while aggregating write-logs and locks of objects being modified. 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;推迟操作，这里可以复习一下The Scalable Commutativity Rule讲的东西了[2].&lt;/p&gt;

&lt;h2 id=&quot;参考&quot;&gt;参考&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;Read-Log-Update: A Lightweight Synchronization Mechanism for Concurrent Programming, SOSP 2015.&lt;/li&gt;
  &lt;li&gt;Austin T. Clements, M. Frans Kaashoek, Nickolai Zeldovich, Robert T. Morris, and Eddie Kohler. 2015. The scalable commutativity rule: Designing scalable software for multicore processors. ACM Trans. Comput. Syst. 32, 4, Article 10 (January 2015), 47 pages.  DOI: http://dx.doi.org/10.1145/2699681.&lt;/li&gt;
&lt;/ol&gt;
</description>
                <link>https://www.nagekar.com/2018/08/Read-Log-Update.html</link>
                <guid>https://www.nagekar.com/2018/08/Read-Log-Update</guid>
                <pubDate>Wed, 01 Aug 2018 00:00:00 +0800</pubDate>
        </item>


</channel>
</rss>
